{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy neural network example\n",
    "\n",
    "We train a neural network giving of u in <0,1> a point (x,y) with coordinates \n",
    "$x = cos(2\\pi u)$\n",
    "$y = sin(2\\pi u)$\n",
    "that is, a point on a unit circle. The network will work as a random \"transformer\", transforming a random u in <0,1> to a random point on a unit circle. \n",
    "\n",
    "# Tasks:\n",
    "### 1. Study the code.\n",
    "There are several things that can be new:\n",
    "- numpy arrays: notice how we can operate on arrays - square them, take a sine or cosine, reshape them\n",
    "- pandas dataframes: notice how we can pack several arrays into a data table - a pandas DataFrame.\n",
    "- matplotlib plots: notice how simply we plot things.\n",
    "\n",
    "### 2. Study the neural network.\n",
    "- Training data. To train a neural network, we have to have a proper sample of training data. In this case, it is very simple to generate them. See what happens when you use a smaller or larger training sample.\n",
    "- Hyperparameters. We have some freedom in selecting how the network looks (sizes of hidden layers) and how we train it. Look into scikit-learn documentation (google \"scikit learn MLPRegressor\") and experiment with various combinations of hyperparameters. Optimize training data size and hyperparameters to achieve best precision.\n",
    "Rules for hidden layers: \n",
    "(i) In theory, a neural network with a single layer can represent any mapping between inputs and outputs (if sufficiently large). \n",
    "(ii) If the problem is strongly non-linear, then a two-layer network can be easier to train. \n",
    "(iii) More than two hidden layers are usually unnecessary.\n",
    "\n",
    "### 3. Investigate the problem.\n",
    "We need a surprisingly large network and a lot of training data to make the example work.\n",
    "On the other hand, the training is fairly fast. \n",
    "\n",
    "A. Create a network that will produce density proportional to $u(1-u)$ on the unit circle (rather than uniform)\n",
    "\n",
    "B. Create a network that will produce uniform density on an ellipse.\n",
    "\n",
    "C. Our network has a single input, u. Try to add some orthogonal polynomials (Legendre or Chebyshev) as additional inputs and examine how such networks train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Includes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import uniform\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_circle_data(n_samples = 100):\n",
    "    \"\"\"Creates pandas dataframe with columns u, x, y, such that u ~ U[0,1], x = cos(2*pi*u), y = sin(2*pi*u).\"\"\"\n",
    "    u = uniform.rvs(0,1,n_samples) \n",
    "    x = np.cos(2*np.pi*u)\n",
    "    y = np.sin(2*np.pi*u)\n",
    "    return pd.DataFrame({'u':u, 'x':x, 'y':y})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train a network regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(data, test_fraction = 0.1):\n",
    "    \"\"\"Returns a trained network object\"\"\"\n",
    "    input_cols = ['u']\n",
    "    output_cols = ['x','y']\n",
    "    n_rows = len(data)\n",
    "    n_test = int(n_rows * test_fraction)\n",
    "    n_train = n_rows - n_test\n",
    "    X_train = data[input_cols].values[:n_train]\n",
    "    X_test = data[input_cols].values[n_train:]\n",
    "    Y_train = data[output_cols].values[:n_train]\n",
    "    Y_test = data[output_cols].values[n_train:]\n",
    "    fitter = MLPRegressor(\n",
    "                hidden_layer_sizes = (60,60),\n",
    "                activation = 'relu',\n",
    "                tol = 1.0e-7,\n",
    "                alpha = 1.0e-5,\n",
    "                verbose = True\n",
    "            )\n",
    "    fitter.fit(X_train, Y_train)\n",
    "    print('Fit done. Score: {0}'.format(fitter.score(X_test, Y_test)))\n",
    "    Y_pred = fitter.predict(X_test)\n",
    "    rmse = np.sqrt(np.sum((Y_pred - Y_test)**2, axis = 0)/n_test)\n",
    "    print('RMS error: x {0}, y {1}'.format(rmse[0], rmse[1]))\n",
    "    return fitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tune_network(data, test_fraction = 0.1):\n",
    "    \"\"\"Returns a trained network object\"\"\"\n",
    "    input_cols = ['u']\n",
    "    output_cols = ['x','y']\n",
    "    n_rows = len(data)\n",
    "    n_test = int(n_rows * test_fraction)\n",
    "    n_train = n_rows - n_test\n",
    "    X_train = data[input_cols].values[:n_train]\n",
    "    X_test = data[input_cols].values[n_train:]\n",
    "    Y_train = data[output_cols].values[:n_train]\n",
    "    Y_test = data[output_cols].values[n_train:]\n",
    "    fitter0 = MLPRegressor(\n",
    "                hidden_layer_sizes = (60,60),\n",
    "                activation = 'relu',\n",
    "                tol = 1.0e-7,\n",
    "                alpha = 1.0e-5,\n",
    "                verbose = True\n",
    "            )\n",
    "    # Optimize for alpha, layer size\n",
    "    fitter = GridSearchCV(\n",
    "        fitter0, \n",
    "        param_grid = {\n",
    "            'alpha':[1.0e-5, 1.0e-4,1.0e-3,1.0e-2,1.0e-1],\n",
    "            'hidden_layer_sizes':[(a, a) for a in [40,60,80]]\n",
    "        }, \n",
    "        verbose = 1\n",
    "    )\n",
    "    fitter.fit(X_train, Y_train)\n",
    "    print('Fit done. Score: {0}'.format(fitter.score(X_test, Y_test)))\n",
    "    Y_pred = fitter.predict(X_test)\n",
    "    rmse = np.sqrt(np.sum((Y_pred - Y_test)**2, axis = 0)/n_test)\n",
    "    print('RMS error: x {0}, y {1}'.format(rmse[0], rmse[1]))\n",
    "    return fitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run and plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n",
      "Iteration 1, loss = 0.19854670\n",
      "Iteration 2, loss = 0.11733309\n",
      "Iteration 3, loss = 0.07537168\n",
      "Iteration 4, loss = 0.04775515\n",
      "Iteration 5, loss = 0.03616853\n",
      "Iteration 6, loss = 0.03113488\n",
      "Iteration 7, loss = 0.02746806\n",
      "Iteration 8, loss = 0.02394053\n",
      "Iteration 9, loss = 0.02007266\n",
      "Iteration 10, loss = 0.01566999\n",
      "Iteration 11, loss = 0.01141947\n",
      "Iteration 12, loss = 0.00784207\n",
      "Iteration 13, loss = 0.00526845\n",
      "Iteration 14, loss = 0.00353257\n",
      "Iteration 15, loss = 0.00243600\n",
      "Iteration 16, loss = 0.00172514\n",
      "Iteration 17, loss = 0.00126981\n",
      "Iteration 18, loss = 0.00093448\n",
      "Iteration 19, loss = 0.00071379\n",
      "Iteration 20, loss = 0.00054212\n",
      "Iteration 21, loss = 0.00042889\n",
      "Iteration 22, loss = 0.00035218\n",
      "Iteration 23, loss = 0.00028630\n",
      "Iteration 24, loss = 0.00024104\n",
      "Iteration 25, loss = 0.00020905\n",
      "Iteration 26, loss = 0.00017635\n",
      "Iteration 27, loss = 0.00015499\n",
      "Iteration 28, loss = 0.00013564\n",
      "Iteration 29, loss = 0.00011921\n",
      "Iteration 30, loss = 0.00010782\n",
      "Iteration 31, loss = 0.00009719\n",
      "Iteration 32, loss = 0.00008779\n",
      "Iteration 33, loss = 0.00008003\n",
      "Iteration 34, loss = 0.00007345\n",
      "Iteration 35, loss = 0.00006694\n",
      "Iteration 36, loss = 0.00006167\n",
      "Iteration 37, loss = 0.00005993\n",
      "Iteration 38, loss = 0.00005456\n",
      "Iteration 39, loss = 0.00005041\n",
      "Iteration 40, loss = 0.00004777\n",
      "Iteration 41, loss = 0.00004582\n",
      "Iteration 42, loss = 0.00004300\n",
      "Iteration 43, loss = 0.00003965\n",
      "Iteration 44, loss = 0.00003964\n",
      "Iteration 45, loss = 0.00003669\n",
      "Iteration 46, loss = 0.00003438\n",
      "Iteration 47, loss = 0.00003330\n",
      "Iteration 48, loss = 0.00003243\n",
      "Iteration 49, loss = 0.00003037\n",
      "Iteration 50, loss = 0.00002910\n",
      "Iteration 51, loss = 0.00002853\n",
      "Iteration 52, loss = 0.00002775\n",
      "Iteration 53, loss = 0.00002684\n",
      "Iteration 54, loss = 0.00002673\n",
      "Iteration 55, loss = 0.00002505\n",
      "Iteration 56, loss = 0.00002374\n",
      "Iteration 57, loss = 0.00002345\n",
      "Iteration 58, loss = 0.00002281\n",
      "Iteration 59, loss = 0.00002237\n",
      "Iteration 60, loss = 0.00002249\n",
      "Iteration 61, loss = 0.00002122\n",
      "Iteration 62, loss = 0.00002154\n",
      "Iteration 63, loss = 0.00002033\n",
      "Iteration 64, loss = 0.00002149\n",
      "Iteration 65, loss = 0.00002087\n",
      "Iteration 66, loss = 0.00001951\n",
      "Iteration 67, loss = 0.00002065\n",
      "Iteration 68, loss = 0.00001924\n",
      "Iteration 69, loss = 0.00001893\n",
      "Iteration 70, loss = 0.00001942\n",
      "Iteration 71, loss = 0.00001756\n",
      "Iteration 72, loss = 0.00001778\n",
      "Iteration 73, loss = 0.00001746\n",
      "Iteration 74, loss = 0.00001764\n",
      "Iteration 75, loss = 0.00001756\n",
      "Iteration 76, loss = 0.00001760\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.22297859\n",
      "Iteration 2, loss = 0.15378734\n",
      "Iteration 3, loss = 0.11707220\n",
      "Iteration 4, loss = 0.08366287\n",
      "Iteration 5, loss = 0.05392012\n",
      "Iteration 6, loss = 0.03442970\n",
      "Iteration 7, loss = 0.02406411\n",
      "Iteration 8, loss = 0.01742869\n",
      "Iteration 9, loss = 0.01224288\n",
      "Iteration 10, loss = 0.00845914\n",
      "Iteration 11, loss = 0.00575161\n",
      "Iteration 12, loss = 0.00410923\n",
      "Iteration 13, loss = 0.00311915\n",
      "Iteration 14, loss = 0.00250218\n",
      "Iteration 15, loss = 0.00202971\n",
      "Iteration 16, loss = 0.00168913\n",
      "Iteration 17, loss = 0.00142572\n",
      "Iteration 18, loss = 0.00121220\n",
      "Iteration 19, loss = 0.00104350\n",
      "Iteration 20, loss = 0.00090241\n",
      "Iteration 21, loss = 0.00079375\n",
      "Iteration 22, loss = 0.00069428\n",
      "Iteration 23, loss = 0.00061599\n",
      "Iteration 24, loss = 0.00055035\n",
      "Iteration 25, loss = 0.00049533\n",
      "Iteration 26, loss = 0.00044820\n",
      "Iteration 27, loss = 0.00040879\n",
      "Iteration 28, loss = 0.00036806\n",
      "Iteration 29, loss = 0.00033880\n",
      "Iteration 30, loss = 0.00031184\n",
      "Iteration 31, loss = 0.00028852\n",
      "Iteration 32, loss = 0.00026691\n",
      "Iteration 33, loss = 0.00024862\n",
      "Iteration 34, loss = 0.00023164\n",
      "Iteration 35, loss = 0.00021622\n",
      "Iteration 36, loss = 0.00020340\n",
      "Iteration 37, loss = 0.00019042\n",
      "Iteration 38, loss = 0.00017713\n",
      "Iteration 39, loss = 0.00016596\n",
      "Iteration 40, loss = 0.00015667\n",
      "Iteration 41, loss = 0.00014810\n",
      "Iteration 42, loss = 0.00013838\n",
      "Iteration 43, loss = 0.00013009\n",
      "Iteration 44, loss = 0.00012314\n",
      "Iteration 45, loss = 0.00011886\n",
      "Iteration 46, loss = 0.00011530\n",
      "Iteration 47, loss = 0.00010750\n",
      "Iteration 48, loss = 0.00010211\n",
      "Iteration 49, loss = 0.00010000\n",
      "Iteration 50, loss = 0.00009363\n",
      "Iteration 51, loss = 0.00008907\n",
      "Iteration 52, loss = 0.00008596\n",
      "Iteration 53, loss = 0.00008162\n",
      "Iteration 54, loss = 0.00008004\n",
      "Iteration 55, loss = 0.00007720\n",
      "Iteration 56, loss = 0.00007313\n",
      "Iteration 57, loss = 0.00007074\n",
      "Iteration 58, loss = 0.00006920\n",
      "Iteration 59, loss = 0.00006538\n",
      "Iteration 60, loss = 0.00006520\n",
      "Iteration 61, loss = 0.00006340\n",
      "Iteration 62, loss = 0.00006227\n",
      "Iteration 63, loss = 0.00005912\n",
      "Iteration 64, loss = 0.00006112\n",
      "Iteration 65, loss = 0.00005732\n",
      "Iteration 66, loss = 0.00005529\n",
      "Iteration 67, loss = 0.00005177\n",
      "Iteration 68, loss = 0.00005248\n",
      "Iteration 69, loss = 0.00005081\n",
      "Iteration 70, loss = 0.00005003\n",
      "Iteration 71, loss = 0.00004784\n",
      "Iteration 72, loss = 0.00004785\n",
      "Iteration 73, loss = 0.00004441\n",
      "Iteration 74, loss = 0.00004482\n",
      "Iteration 75, loss = 0.00004412\n",
      "Iteration 76, loss = 0.00004411\n",
      "Iteration 77, loss = 0.00004128\n",
      "Iteration 78, loss = 0.00004025\n",
      "Iteration 79, loss = 0.00003885\n",
      "Iteration 80, loss = 0.00004566\n",
      "Iteration 81, loss = 0.00003751\n",
      "Iteration 82, loss = 0.00003783\n",
      "Iteration 83, loss = 0.00003620\n",
      "Iteration 84, loss = 0.00003679\n",
      "Iteration 85, loss = 0.00003660\n",
      "Iteration 86, loss = 0.00003468\n",
      "Iteration 87, loss = 0.00003718\n",
      "Iteration 88, loss = 0.00003276\n",
      "Iteration 89, loss = 0.00003341\n",
      "Iteration 90, loss = 0.00003914\n",
      "Iteration 91, loss = 0.00003706\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.19625033\n",
      "Iteration 2, loss = 0.12982973\n",
      "Iteration 3, loss = 0.08553218\n",
      "Iteration 4, loss = 0.04969171\n",
      "Iteration 5, loss = 0.03358602\n",
      "Iteration 6, loss = 0.02658302\n",
      "Iteration 7, loss = 0.02113901\n",
      "Iteration 8, loss = 0.01581437\n",
      "Iteration 9, loss = 0.01123972\n",
      "Iteration 10, loss = 0.00766361\n",
      "Iteration 11, loss = 0.00508945\n",
      "Iteration 12, loss = 0.00339510\n",
      "Iteration 13, loss = 0.00238895\n",
      "Iteration 14, loss = 0.00176101\n",
      "Iteration 15, loss = 0.00135473\n",
      "Iteration 16, loss = 0.00107249\n",
      "Iteration 17, loss = 0.00088133\n",
      "Iteration 18, loss = 0.00074217\n",
      "Iteration 19, loss = 0.00064030\n",
      "Iteration 20, loss = 0.00055811\n",
      "Iteration 21, loss = 0.00049375\n",
      "Iteration 22, loss = 0.00043855\n",
      "Iteration 23, loss = 0.00039604\n",
      "Iteration 24, loss = 0.00036057\n",
      "Iteration 25, loss = 0.00032359\n",
      "Iteration 26, loss = 0.00029557\n",
      "Iteration 27, loss = 0.00027075\n",
      "Iteration 28, loss = 0.00024860\n",
      "Iteration 29, loss = 0.00022970\n",
      "Iteration 30, loss = 0.00021388\n",
      "Iteration 31, loss = 0.00019791\n",
      "Iteration 32, loss = 0.00018383\n",
      "Iteration 33, loss = 0.00017087\n",
      "Iteration 34, loss = 0.00016253\n",
      "Iteration 35, loss = 0.00014944\n",
      "Iteration 36, loss = 0.00014320\n",
      "Iteration 37, loss = 0.00013332\n",
      "Iteration 38, loss = 0.00012606\n",
      "Iteration 39, loss = 0.00011800\n",
      "Iteration 40, loss = 0.00010617\n",
      "Iteration 41, loss = 0.00009010\n",
      "Iteration 42, loss = 0.00008601\n",
      "Iteration 43, loss = 0.00008221\n",
      "Iteration 44, loss = 0.00007810\n",
      "Iteration 45, loss = 0.00007729\n",
      "Iteration 46, loss = 0.00007318\n",
      "Iteration 47, loss = 0.00007171\n",
      "Iteration 48, loss = 0.00006746\n",
      "Iteration 49, loss = 0.00006478\n",
      "Iteration 50, loss = 0.00006365\n",
      "Iteration 51, loss = 0.00006267\n",
      "Iteration 52, loss = 0.00006285\n",
      "Iteration 53, loss = 0.00005790\n",
      "Iteration 54, loss = 0.00005560\n",
      "Iteration 55, loss = 0.00005627\n",
      "Iteration 56, loss = 0.00005306\n",
      "Iteration 57, loss = 0.00005109\n",
      "Iteration 58, loss = 0.00004860\n",
      "Iteration 59, loss = 0.00004930\n",
      "Iteration 60, loss = 0.00004755\n",
      "Iteration 61, loss = 0.00004538\n",
      "Iteration 62, loss = 0.00004827\n",
      "Iteration 63, loss = 0.00004716\n",
      "Iteration 64, loss = 0.00004740\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.18439616\n",
      "Iteration 2, loss = 0.12152656\n",
      "Iteration 3, loss = 0.06962484\n",
      "Iteration 4, loss = 0.03391584\n",
      "Iteration 5, loss = 0.01961463\n",
      "Iteration 6, loss = 0.01143342\n",
      "Iteration 7, loss = 0.00638049\n",
      "Iteration 8, loss = 0.00364839\n",
      "Iteration 9, loss = 0.00219156\n",
      "Iteration 10, loss = 0.00140845\n",
      "Iteration 11, loss = 0.00097608\n",
      "Iteration 12, loss = 0.00069385\n",
      "Iteration 13, loss = 0.00051602\n",
      "Iteration 14, loss = 0.00039386\n",
      "Iteration 15, loss = 0.00030768\n",
      "Iteration 16, loss = 0.00023926\n",
      "Iteration 17, loss = 0.00017907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 18, loss = 0.00013118\n",
      "Iteration 19, loss = 0.00009789\n",
      "Iteration 20, loss = 0.00007706\n",
      "Iteration 21, loss = 0.00006360\n",
      "Iteration 22, loss = 0.00005366\n",
      "Iteration 23, loss = 0.00004617\n",
      "Iteration 24, loss = 0.00003881\n",
      "Iteration 25, loss = 0.00003436\n",
      "Iteration 26, loss = 0.00003176\n",
      "Iteration 27, loss = 0.00002899\n",
      "Iteration 28, loss = 0.00002651\n",
      "Iteration 29, loss = 0.00002484\n",
      "Iteration 30, loss = 0.00002329\n",
      "Iteration 31, loss = 0.00002233\n",
      "Iteration 32, loss = 0.00002135\n",
      "Iteration 33, loss = 0.00002160\n",
      "Iteration 34, loss = 0.00002033\n",
      "Iteration 35, loss = 0.00002039\n",
      "Iteration 36, loss = 0.00001934\n",
      "Iteration 37, loss = 0.00001852\n",
      "Iteration 38, loss = 0.00001942\n",
      "Iteration 39, loss = 0.00001883\n",
      "Iteration 40, loss = 0.00001752\n",
      "Iteration 41, loss = 0.00001733\n",
      "Iteration 42, loss = 0.00001771\n",
      "Iteration 43, loss = 0.00001720\n",
      "Iteration 44, loss = 0.00001834\n",
      "Iteration 45, loss = 0.00001699\n",
      "Iteration 46, loss = 0.00001751\n",
      "Iteration 47, loss = 0.00001632\n",
      "Iteration 48, loss = 0.00001852\n",
      "Iteration 49, loss = 0.00001733\n",
      "Iteration 50, loss = 0.00001847\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.19210956\n",
      "Iteration 2, loss = 0.12318908\n",
      "Iteration 3, loss = 0.06976886\n",
      "Iteration 4, loss = 0.03534220\n",
      "Iteration 5, loss = 0.02294473\n",
      "Iteration 6, loss = 0.01597650\n",
      "Iteration 7, loss = 0.01072648\n",
      "Iteration 8, loss = 0.00695031\n",
      "Iteration 9, loss = 0.00453665\n",
      "Iteration 10, loss = 0.00308734\n",
      "Iteration 11, loss = 0.00214414\n",
      "Iteration 12, loss = 0.00155824\n",
      "Iteration 13, loss = 0.00117970\n",
      "Iteration 14, loss = 0.00092429\n",
      "Iteration 15, loss = 0.00074387\n",
      "Iteration 16, loss = 0.00061711\n",
      "Iteration 17, loss = 0.00052641\n",
      "Iteration 18, loss = 0.00045192\n",
      "Iteration 19, loss = 0.00039417\n",
      "Iteration 20, loss = 0.00034788\n",
      "Iteration 21, loss = 0.00030437\n",
      "Iteration 22, loss = 0.00026779\n",
      "Iteration 23, loss = 0.00023735\n",
      "Iteration 24, loss = 0.00021163\n",
      "Iteration 25, loss = 0.00019438\n",
      "Iteration 26, loss = 0.00017678\n",
      "Iteration 27, loss = 0.00016375\n",
      "Iteration 28, loss = 0.00014980\n",
      "Iteration 29, loss = 0.00013974\n",
      "Iteration 30, loss = 0.00012257\n",
      "Iteration 31, loss = 0.00010471\n",
      "Iteration 32, loss = 0.00009432\n",
      "Iteration 33, loss = 0.00009032\n",
      "Iteration 34, loss = 0.00008449\n",
      "Iteration 35, loss = 0.00007911\n",
      "Iteration 36, loss = 0.00007433\n",
      "Iteration 37, loss = 0.00007061\n",
      "Iteration 38, loss = 0.00006895\n",
      "Iteration 39, loss = 0.00006400\n",
      "Iteration 40, loss = 0.00006128\n",
      "Iteration 41, loss = 0.00005749\n",
      "Iteration 42, loss = 0.00005496\n",
      "Iteration 43, loss = 0.00005355\n",
      "Iteration 44, loss = 0.00005145\n",
      "Iteration 45, loss = 0.00004892\n",
      "Iteration 46, loss = 0.00004899\n",
      "Iteration 47, loss = 0.00004686\n",
      "Iteration 48, loss = 0.00004433\n",
      "Iteration 49, loss = 0.00004396\n",
      "Iteration 50, loss = 0.00004199\n",
      "Iteration 51, loss = 0.00004079\n",
      "Iteration 52, loss = 0.00003848\n",
      "Iteration 53, loss = 0.00003913\n",
      "Iteration 54, loss = 0.00003761\n",
      "Iteration 55, loss = 0.00003697\n",
      "Iteration 56, loss = 0.00003289\n",
      "Iteration 57, loss = 0.00002834\n",
      "Iteration 58, loss = 0.00002834\n",
      "Iteration 59, loss = 0.00003160\n",
      "Iteration 60, loss = 0.00002576\n",
      "Iteration 61, loss = 0.00002533\n",
      "Iteration 62, loss = 0.00002692\n",
      "Iteration 63, loss = 0.00002468\n",
      "Iteration 64, loss = 0.00002438\n",
      "Iteration 65, loss = 0.00002683\n",
      "Iteration 66, loss = 0.00002438\n",
      "Iteration 67, loss = 0.00002308\n",
      "Iteration 68, loss = 0.00002430\n",
      "Iteration 69, loss = 0.00002176\n",
      "Iteration 70, loss = 0.00002660\n",
      "Iteration 71, loss = 0.00002337\n",
      "Iteration 72, loss = 0.00002066\n",
      "Iteration 73, loss = 0.00002433\n",
      "Iteration 74, loss = 0.00002047\n",
      "Iteration 75, loss = 0.00002123\n",
      "Iteration 76, loss = 0.00002263\n",
      "Iteration 77, loss = 0.00002216\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.19424683\n",
      "Iteration 2, loss = 0.12863101\n",
      "Iteration 3, loss = 0.08336580\n",
      "Iteration 4, loss = 0.04373042\n",
      "Iteration 5, loss = 0.02731934\n",
      "Iteration 6, loss = 0.01795120\n",
      "Iteration 7, loss = 0.01051491\n",
      "Iteration 8, loss = 0.00570788\n",
      "Iteration 9, loss = 0.00322920\n",
      "Iteration 10, loss = 0.00193413\n",
      "Iteration 11, loss = 0.00127714\n",
      "Iteration 12, loss = 0.00091424\n",
      "Iteration 13, loss = 0.00067162\n",
      "Iteration 14, loss = 0.00051256\n",
      "Iteration 15, loss = 0.00040211\n",
      "Iteration 16, loss = 0.00033049\n",
      "Iteration 17, loss = 0.00027020\n",
      "Iteration 18, loss = 0.00023115\n",
      "Iteration 19, loss = 0.00019754\n",
      "Iteration 20, loss = 0.00017241\n",
      "Iteration 21, loss = 0.00015278\n",
      "Iteration 22, loss = 0.00013730\n",
      "Iteration 23, loss = 0.00012460\n",
      "Iteration 24, loss = 0.00011332\n",
      "Iteration 25, loss = 0.00010597\n",
      "Iteration 26, loss = 0.00009888\n",
      "Iteration 27, loss = 0.00009687\n",
      "Iteration 28, loss = 0.00008868\n",
      "Iteration 29, loss = 0.00008940\n",
      "Iteration 30, loss = 0.00008268\n",
      "Iteration 31, loss = 0.00007865\n",
      "Iteration 32, loss = 0.00007458\n",
      "Iteration 33, loss = 0.00007215\n",
      "Iteration 34, loss = 0.00007161\n",
      "Iteration 35, loss = 0.00006799\n",
      "Iteration 36, loss = 0.00006677\n",
      "Iteration 37, loss = 0.00006549\n",
      "Iteration 38, loss = 0.00006372\n",
      "Iteration 39, loss = 0.00006315\n",
      "Iteration 40, loss = 0.00005958\n",
      "Iteration 41, loss = 0.00005777\n",
      "Iteration 42, loss = 0.00005774\n",
      "Iteration 43, loss = 0.00005545\n",
      "Iteration 44, loss = 0.00005258\n",
      "Iteration 45, loss = 0.00004994\n",
      "Iteration 46, loss = 0.00004910\n",
      "Iteration 47, loss = 0.00004979\n",
      "Iteration 48, loss = 0.00005048\n",
      "Iteration 49, loss = 0.00004972\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.19130852\n",
      "Iteration 2, loss = 0.11105552\n",
      "Iteration 3, loss = 0.05686126\n",
      "Iteration 4, loss = 0.03150321\n",
      "Iteration 5, loss = 0.02386059\n",
      "Iteration 6, loss = 0.01823110\n",
      "Iteration 7, loss = 0.01261040\n",
      "Iteration 8, loss = 0.00798986\n",
      "Iteration 9, loss = 0.00495599\n",
      "Iteration 10, loss = 0.00325168\n",
      "Iteration 11, loss = 0.00219951\n",
      "Iteration 12, loss = 0.00156709\n",
      "Iteration 13, loss = 0.00115677\n",
      "Iteration 14, loss = 0.00088031\n",
      "Iteration 15, loss = 0.00067150\n",
      "Iteration 16, loss = 0.00053834\n",
      "Iteration 17, loss = 0.00042777\n",
      "Iteration 18, loss = 0.00035025\n",
      "Iteration 19, loss = 0.00029104\n",
      "Iteration 20, loss = 0.00024666\n",
      "Iteration 21, loss = 0.00020774\n",
      "Iteration 22, loss = 0.00017571\n",
      "Iteration 23, loss = 0.00015469\n",
      "Iteration 24, loss = 0.00013199\n",
      "Iteration 25, loss = 0.00011708\n",
      "Iteration 26, loss = 0.00010650\n",
      "Iteration 27, loss = 0.00009626\n",
      "Iteration 28, loss = 0.00008569\n",
      "Iteration 29, loss = 0.00007629\n",
      "Iteration 30, loss = 0.00007091\n",
      "Iteration 31, loss = 0.00006580\n",
      "Iteration 32, loss = 0.00006012\n",
      "Iteration 33, loss = 0.00005675\n",
      "Iteration 34, loss = 0.00005275\n",
      "Iteration 35, loss = 0.00004989\n",
      "Iteration 36, loss = 0.00004749\n",
      "Iteration 37, loss = 0.00004709\n",
      "Iteration 38, loss = 0.00004433\n",
      "Iteration 39, loss = 0.00004792\n",
      "Iteration 40, loss = 0.00004201\n",
      "Iteration 41, loss = 0.00003900\n",
      "Iteration 42, loss = 0.00003965\n",
      "Iteration 43, loss = 0.00003651\n",
      "Iteration 44, loss = 0.00003603\n",
      "Iteration 45, loss = 0.00003618\n",
      "Iteration 46, loss = 0.00003482\n",
      "Iteration 47, loss = 0.00003565\n",
      "Iteration 48, loss = 0.00003288\n",
      "Iteration 49, loss = 0.00003253\n",
      "Iteration 50, loss = 0.00003284\n",
      "Iteration 51, loss = 0.00003187\n",
      "Iteration 52, loss = 0.00003121\n",
      "Iteration 53, loss = 0.00003127\n",
      "Iteration 54, loss = 0.00003166\n",
      "Iteration 55, loss = 0.00003199\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.18561801\n",
      "Iteration 2, loss = 0.11741000\n",
      "Iteration 3, loss = 0.05775407\n",
      "Iteration 4, loss = 0.02560321\n",
      "Iteration 5, loss = 0.01557998\n",
      "Iteration 6, loss = 0.01003019\n",
      "Iteration 7, loss = 0.00644803\n",
      "Iteration 8, loss = 0.00424497\n",
      "Iteration 9, loss = 0.00285015\n",
      "Iteration 10, loss = 0.00191125\n",
      "Iteration 11, loss = 0.00137810\n",
      "Iteration 12, loss = 0.00097970\n",
      "Iteration 13, loss = 0.00073013\n",
      "Iteration 14, loss = 0.00056521\n",
      "Iteration 15, loss = 0.00046347\n",
      "Iteration 16, loss = 0.00037470\n",
      "Iteration 17, loss = 0.00031127\n",
      "Iteration 18, loss = 0.00027028\n",
      "Iteration 19, loss = 0.00023294\n",
      "Iteration 20, loss = 0.00019774\n",
      "Iteration 21, loss = 0.00017392\n",
      "Iteration 22, loss = 0.00015030\n",
      "Iteration 23, loss = 0.00012941\n",
      "Iteration 24, loss = 0.00011565\n",
      "Iteration 25, loss = 0.00010251\n",
      "Iteration 26, loss = 0.00009361\n",
      "Iteration 27, loss = 0.00008111\n",
      "Iteration 28, loss = 0.00007387\n",
      "Iteration 29, loss = 0.00006689\n",
      "Iteration 30, loss = 0.00006279\n",
      "Iteration 31, loss = 0.00005632\n",
      "Iteration 32, loss = 0.00005170\n",
      "Iteration 33, loss = 0.00005248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 34, loss = 0.00004332\n",
      "Iteration 35, loss = 0.00004044\n",
      "Iteration 36, loss = 0.00003778\n",
      "Iteration 37, loss = 0.00004044\n",
      "Iteration 38, loss = 0.00003543\n",
      "Iteration 39, loss = 0.00003640\n",
      "Iteration 40, loss = 0.00003079\n",
      "Iteration 41, loss = 0.00002819\n",
      "Iteration 42, loss = 0.00002725\n",
      "Iteration 43, loss = 0.00002714\n",
      "Iteration 44, loss = 0.00002467\n",
      "Iteration 45, loss = 0.00002439\n",
      "Iteration 46, loss = 0.00002597\n",
      "Iteration 47, loss = 0.00002371\n",
      "Iteration 48, loss = 0.00002471\n",
      "Iteration 49, loss = 0.00002045\n",
      "Iteration 50, loss = 0.00001977\n",
      "Iteration 51, loss = 0.00001860\n",
      "Iteration 52, loss = 0.00001958\n",
      "Iteration 53, loss = 0.00001836\n",
      "Iteration 54, loss = 0.00001893\n",
      "Iteration 55, loss = 0.00001991\n",
      "Iteration 56, loss = 0.00002441\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.19948469\n",
      "Iteration 2, loss = 0.10832664\n",
      "Iteration 3, loss = 0.04845474\n",
      "Iteration 4, loss = 0.02423172\n",
      "Iteration 5, loss = 0.01545194\n",
      "Iteration 6, loss = 0.00951342\n",
      "Iteration 7, loss = 0.00582022\n",
      "Iteration 8, loss = 0.00361069\n",
      "Iteration 9, loss = 0.00239821\n",
      "Iteration 10, loss = 0.00165225\n",
      "Iteration 11, loss = 0.00118600\n",
      "Iteration 12, loss = 0.00086957\n",
      "Iteration 13, loss = 0.00065797\n",
      "Iteration 14, loss = 0.00051488\n",
      "Iteration 15, loss = 0.00039944\n",
      "Iteration 16, loss = 0.00028093\n",
      "Iteration 17, loss = 0.00018553\n",
      "Iteration 18, loss = 0.00011913\n",
      "Iteration 19, loss = 0.00008241\n",
      "Iteration 20, loss = 0.00006035\n",
      "Iteration 21, loss = 0.00004644\n",
      "Iteration 22, loss = 0.00003647\n",
      "Iteration 23, loss = 0.00003113\n",
      "Iteration 24, loss = 0.00002595\n",
      "Iteration 25, loss = 0.00002315\n",
      "Iteration 26, loss = 0.00002164\n",
      "Iteration 27, loss = 0.00001920\n",
      "Iteration 28, loss = 0.00001835\n",
      "Iteration 29, loss = 0.00001726\n",
      "Iteration 30, loss = 0.00001636\n",
      "Iteration 31, loss = 0.00001524\n",
      "Iteration 32, loss = 0.00001487\n",
      "Iteration 33, loss = 0.00001479\n",
      "Iteration 34, loss = 0.00001411\n",
      "Iteration 35, loss = 0.00001410\n",
      "Iteration 36, loss = 0.00001341\n",
      "Iteration 37, loss = 0.00001325\n",
      "Iteration 38, loss = 0.00001353\n",
      "Iteration 39, loss = 0.00001249\n",
      "Iteration 40, loss = 0.00001278\n",
      "Iteration 41, loss = 0.00001307\n",
      "Iteration 42, loss = 0.00001241\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20828374\n",
      "Iteration 2, loss = 0.13822095\n",
      "Iteration 3, loss = 0.09184784\n",
      "Iteration 4, loss = 0.05590172\n",
      "Iteration 5, loss = 0.03532089\n",
      "Iteration 6, loss = 0.02632070\n",
      "Iteration 7, loss = 0.02113122\n",
      "Iteration 8, loss = 0.01714995\n",
      "Iteration 9, loss = 0.01369072\n",
      "Iteration 10, loss = 0.01058696\n",
      "Iteration 11, loss = 0.00812461\n",
      "Iteration 12, loss = 0.00619570\n",
      "Iteration 13, loss = 0.00475597\n",
      "Iteration 14, loss = 0.00361246\n",
      "Iteration 15, loss = 0.00255876\n",
      "Iteration 16, loss = 0.00183480\n",
      "Iteration 17, loss = 0.00141216\n",
      "Iteration 18, loss = 0.00113795\n",
      "Iteration 19, loss = 0.00094264\n",
      "Iteration 20, loss = 0.00081238\n",
      "Iteration 21, loss = 0.00068182\n",
      "Iteration 22, loss = 0.00060074\n",
      "Iteration 23, loss = 0.00052551\n",
      "Iteration 24, loss = 0.00046213\n",
      "Iteration 25, loss = 0.00041411\n",
      "Iteration 26, loss = 0.00037795\n",
      "Iteration 27, loss = 0.00033464\n",
      "Iteration 28, loss = 0.00030478\n",
      "Iteration 29, loss = 0.00027738\n",
      "Iteration 30, loss = 0.00025570\n",
      "Iteration 31, loss = 0.00023503\n",
      "Iteration 32, loss = 0.00021602\n",
      "Iteration 33, loss = 0.00020291\n",
      "Iteration 34, loss = 0.00018684\n",
      "Iteration 35, loss = 0.00017723\n",
      "Iteration 36, loss = 0.00016400\n",
      "Iteration 37, loss = 0.00015601\n",
      "Iteration 38, loss = 0.00015134\n",
      "Iteration 39, loss = 0.00014089\n",
      "Iteration 40, loss = 0.00013281\n",
      "Iteration 41, loss = 0.00012537\n",
      "Iteration 42, loss = 0.00012120\n",
      "Iteration 43, loss = 0.00011509\n",
      "Iteration 44, loss = 0.00011077\n",
      "Iteration 45, loss = 0.00010610\n",
      "Iteration 46, loss = 0.00010327\n",
      "Iteration 47, loss = 0.00010019\n",
      "Iteration 48, loss = 0.00009679\n",
      "Iteration 49, loss = 0.00009760\n",
      "Iteration 50, loss = 0.00009057\n",
      "Iteration 51, loss = 0.00008911\n",
      "Iteration 52, loss = 0.00008513\n",
      "Iteration 53, loss = 0.00008406\n",
      "Iteration 54, loss = 0.00008282\n",
      "Iteration 55, loss = 0.00007753\n",
      "Iteration 56, loss = 0.00007655\n",
      "Iteration 57, loss = 0.00007382\n",
      "Iteration 58, loss = 0.00007241\n",
      "Iteration 59, loss = 0.00007278\n",
      "Iteration 60, loss = 0.00007047\n",
      "Iteration 61, loss = 0.00007173\n",
      "Iteration 62, loss = 0.00006811\n",
      "Iteration 63, loss = 0.00006635\n",
      "Iteration 64, loss = 0.00006387\n",
      "Iteration 65, loss = 0.00006492\n",
      "Iteration 66, loss = 0.00006481\n",
      "Iteration 67, loss = 0.00006101\n",
      "Iteration 68, loss = 0.00006056\n",
      "Iteration 69, loss = 0.00005916\n",
      "Iteration 70, loss = 0.00005873\n",
      "Iteration 71, loss = 0.00005868\n",
      "Iteration 72, loss = 0.00005832\n",
      "Iteration 73, loss = 0.00005960\n",
      "Iteration 74, loss = 0.00005462\n",
      "Iteration 75, loss = 0.00005443\n",
      "Iteration 76, loss = 0.00005610\n",
      "Iteration 77, loss = 0.00005472\n",
      "Iteration 78, loss = 0.00005227\n",
      "Iteration 79, loss = 0.00005330\n",
      "Iteration 80, loss = 0.00005150\n",
      "Iteration 81, loss = 0.00005078\n",
      "Iteration 82, loss = 0.00005287\n",
      "Iteration 83, loss = 0.00005174\n",
      "Iteration 84, loss = 0.00004990\n",
      "Iteration 85, loss = 0.00004798\n",
      "Iteration 86, loss = 0.00004843\n",
      "Iteration 87, loss = 0.00004987\n",
      "Iteration 88, loss = 0.00004785\n",
      "Iteration 89, loss = 0.00004592\n",
      "Iteration 90, loss = 0.00004611\n",
      "Iteration 91, loss = 0.00004486\n",
      "Iteration 92, loss = 0.00004923\n",
      "Iteration 93, loss = 0.00004720\n",
      "Iteration 94, loss = 0.00004612\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.19963786\n",
      "Iteration 2, loss = 0.12647244\n",
      "Iteration 3, loss = 0.08208611\n",
      "Iteration 4, loss = 0.04393035\n",
      "Iteration 5, loss = 0.02342289\n",
      "Iteration 6, loss = 0.01440835\n",
      "Iteration 7, loss = 0.00910963\n",
      "Iteration 8, loss = 0.00579058\n",
      "Iteration 9, loss = 0.00378828\n",
      "Iteration 10, loss = 0.00260174\n",
      "Iteration 11, loss = 0.00187649\n",
      "Iteration 12, loss = 0.00141574\n",
      "Iteration 13, loss = 0.00109977\n",
      "Iteration 14, loss = 0.00087393\n",
      "Iteration 15, loss = 0.00071491\n",
      "Iteration 16, loss = 0.00059489\n",
      "Iteration 17, loss = 0.00050491\n",
      "Iteration 18, loss = 0.00043812\n",
      "Iteration 19, loss = 0.00037934\n",
      "Iteration 20, loss = 0.00032993\n",
      "Iteration 21, loss = 0.00029224\n",
      "Iteration 22, loss = 0.00026699\n",
      "Iteration 23, loss = 0.00023462\n",
      "Iteration 24, loss = 0.00021068\n",
      "Iteration 25, loss = 0.00018839\n",
      "Iteration 26, loss = 0.00017080\n",
      "Iteration 27, loss = 0.00015360\n",
      "Iteration 28, loss = 0.00014006\n",
      "Iteration 29, loss = 0.00012753\n",
      "Iteration 30, loss = 0.00011748\n",
      "Iteration 31, loss = 0.00010930\n",
      "Iteration 32, loss = 0.00010181\n",
      "Iteration 33, loss = 0.00009538\n",
      "Iteration 34, loss = 0.00009028\n",
      "Iteration 35, loss = 0.00008520\n",
      "Iteration 36, loss = 0.00008166\n",
      "Iteration 37, loss = 0.00007783\n",
      "Iteration 38, loss = 0.00007589\n",
      "Iteration 39, loss = 0.00007408\n",
      "Iteration 40, loss = 0.00007283\n",
      "Iteration 41, loss = 0.00007052\n",
      "Iteration 42, loss = 0.00006838\n",
      "Iteration 43, loss = 0.00006623\n",
      "Iteration 44, loss = 0.00006551\n",
      "Iteration 45, loss = 0.00006541\n",
      "Iteration 46, loss = 0.00006439\n",
      "Iteration 47, loss = 0.00006377\n",
      "Iteration 48, loss = 0.00006138\n",
      "Iteration 49, loss = 0.00005985\n",
      "Iteration 50, loss = 0.00006004\n",
      "Iteration 51, loss = 0.00005902\n",
      "Iteration 52, loss = 0.00005964\n",
      "Iteration 53, loss = 0.00005983\n",
      "Iteration 54, loss = 0.00005748\n",
      "Iteration 55, loss = 0.00005685\n",
      "Iteration 56, loss = 0.00005695\n",
      "Iteration 57, loss = 0.00005929\n",
      "Iteration 58, loss = 0.00005904\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.23370901\n",
      "Iteration 2, loss = 0.17994332\n",
      "Iteration 3, loss = 0.13517814\n",
      "Iteration 4, loss = 0.09914467\n",
      "Iteration 5, loss = 0.06273954\n",
      "Iteration 6, loss = 0.03705266\n",
      "Iteration 7, loss = 0.02556466\n",
      "Iteration 8, loss = 0.01984382\n",
      "Iteration 9, loss = 0.01546490\n",
      "Iteration 10, loss = 0.01142510\n",
      "Iteration 11, loss = 0.00824019\n",
      "Iteration 12, loss = 0.00593733\n",
      "Iteration 13, loss = 0.00431027\n",
      "Iteration 14, loss = 0.00317766\n",
      "Iteration 15, loss = 0.00243037\n",
      "Iteration 16, loss = 0.00191154\n",
      "Iteration 17, loss = 0.00155217\n",
      "Iteration 18, loss = 0.00126403\n",
      "Iteration 19, loss = 0.00106334\n",
      "Iteration 20, loss = 0.00090770\n",
      "Iteration 21, loss = 0.00078829\n",
      "Iteration 22, loss = 0.00069733\n",
      "Iteration 23, loss = 0.00062835\n",
      "Iteration 24, loss = 0.00056958\n",
      "Iteration 25, loss = 0.00051885\n",
      "Iteration 26, loss = 0.00047472\n",
      "Iteration 27, loss = 0.00043725\n",
      "Iteration 28, loss = 0.00040976\n",
      "Iteration 29, loss = 0.00038258\n",
      "Iteration 30, loss = 0.00035664\n",
      "Iteration 31, loss = 0.00033942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 32, loss = 0.00031772\n",
      "Iteration 33, loss = 0.00030192\n",
      "Iteration 34, loss = 0.00028583\n",
      "Iteration 35, loss = 0.00026697\n",
      "Iteration 36, loss = 0.00025471\n",
      "Iteration 37, loss = 0.00024095\n",
      "Iteration 38, loss = 0.00021698\n",
      "Iteration 39, loss = 0.00019209\n",
      "Iteration 40, loss = 0.00017365\n",
      "Iteration 41, loss = 0.00015937\n",
      "Iteration 42, loss = 0.00014920\n",
      "Iteration 43, loss = 0.00013931\n",
      "Iteration 44, loss = 0.00013269\n",
      "Iteration 45, loss = 0.00012645\n",
      "Iteration 46, loss = 0.00012025\n",
      "Iteration 47, loss = 0.00011844\n",
      "Iteration 48, loss = 0.00011288\n",
      "Iteration 49, loss = 0.00011063\n",
      "Iteration 50, loss = 0.00010717\n",
      "Iteration 51, loss = 0.00010424\n",
      "Iteration 52, loss = 0.00010206\n",
      "Iteration 53, loss = 0.00010208\n",
      "Iteration 54, loss = 0.00009615\n",
      "Iteration 55, loss = 0.00009543\n",
      "Iteration 56, loss = 0.00009383\n",
      "Iteration 57, loss = 0.00009187\n",
      "Iteration 58, loss = 0.00009118\n",
      "Iteration 59, loss = 0.00009092\n",
      "Iteration 60, loss = 0.00008732\n",
      "Iteration 61, loss = 0.00008617\n",
      "Iteration 62, loss = 0.00008617\n",
      "Iteration 63, loss = 0.00008126\n",
      "Iteration 64, loss = 0.00008068\n",
      "Iteration 65, loss = 0.00007840\n",
      "Iteration 66, loss = 0.00007790\n",
      "Iteration 67, loss = 0.00007850\n",
      "Iteration 68, loss = 0.00007688\n",
      "Iteration 69, loss = 0.00007617\n",
      "Iteration 70, loss = 0.00007407\n",
      "Iteration 71, loss = 0.00007126\n",
      "Iteration 72, loss = 0.00007184\n",
      "Iteration 73, loss = 0.00007131\n",
      "Iteration 74, loss = 0.00006998\n",
      "Iteration 75, loss = 0.00006852\n",
      "Iteration 76, loss = 0.00006908\n",
      "Iteration 77, loss = 0.00006770\n",
      "Iteration 78, loss = 0.00006834\n",
      "Iteration 79, loss = 0.00006714\n",
      "Iteration 80, loss = 0.00006612\n",
      "Iteration 81, loss = 0.00006387\n",
      "Iteration 82, loss = 0.00006297\n",
      "Iteration 83, loss = 0.00006504\n",
      "Iteration 84, loss = 0.00006517\n",
      "Iteration 85, loss = 0.00006397\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20067167\n",
      "Iteration 2, loss = 0.13846355\n",
      "Iteration 3, loss = 0.09624265\n",
      "Iteration 4, loss = 0.05340534\n",
      "Iteration 5, loss = 0.03338628\n",
      "Iteration 6, loss = 0.02496841\n",
      "Iteration 7, loss = 0.01871735\n",
      "Iteration 8, loss = 0.01313051\n",
      "Iteration 9, loss = 0.00853944\n",
      "Iteration 10, loss = 0.00547214\n",
      "Iteration 11, loss = 0.00350548\n",
      "Iteration 12, loss = 0.00225750\n",
      "Iteration 13, loss = 0.00157235\n",
      "Iteration 14, loss = 0.00114498\n",
      "Iteration 15, loss = 0.00086716\n",
      "Iteration 16, loss = 0.00069008\n",
      "Iteration 17, loss = 0.00055255\n",
      "Iteration 18, loss = 0.00045408\n",
      "Iteration 19, loss = 0.00038446\n",
      "Iteration 20, loss = 0.00032878\n",
      "Iteration 21, loss = 0.00028943\n",
      "Iteration 22, loss = 0.00025414\n",
      "Iteration 23, loss = 0.00022684\n",
      "Iteration 24, loss = 0.00020685\n",
      "Iteration 25, loss = 0.00018562\n",
      "Iteration 26, loss = 0.00016608\n",
      "Iteration 27, loss = 0.00015530\n",
      "Iteration 28, loss = 0.00014224\n",
      "Iteration 29, loss = 0.00013299\n",
      "Iteration 30, loss = 0.00012438\n",
      "Iteration 31, loss = 0.00011638\n",
      "Iteration 32, loss = 0.00011364\n",
      "Iteration 33, loss = 0.00010261\n",
      "Iteration 34, loss = 0.00009653\n",
      "Iteration 35, loss = 0.00009456\n",
      "Iteration 36, loss = 0.00008861\n",
      "Iteration 37, loss = 0.00008459\n",
      "Iteration 38, loss = 0.00008385\n",
      "Iteration 39, loss = 0.00008311\n",
      "Iteration 40, loss = 0.00007854\n",
      "Iteration 41, loss = 0.00007756\n",
      "Iteration 42, loss = 0.00007329\n",
      "Iteration 43, loss = 0.00007082\n",
      "Iteration 44, loss = 0.00007336\n",
      "Iteration 45, loss = 0.00006936\n",
      "Iteration 46, loss = 0.00006723\n",
      "Iteration 47, loss = 0.00006448\n",
      "Iteration 48, loss = 0.00006592\n",
      "Iteration 49, loss = 0.00006070\n",
      "Iteration 50, loss = 0.00006196\n",
      "Iteration 51, loss = 0.00006078\n",
      "Iteration 52, loss = 0.00005991\n",
      "Iteration 53, loss = 0.00005873\n",
      "Iteration 54, loss = 0.00005702\n",
      "Iteration 55, loss = 0.00005440\n",
      "Iteration 56, loss = 0.00006058\n",
      "Iteration 57, loss = 0.00005290\n",
      "Iteration 58, loss = 0.00005166\n",
      "Iteration 59, loss = 0.00005095\n",
      "Iteration 60, loss = 0.00005204\n",
      "Iteration 61, loss = 0.00005302\n",
      "Iteration 62, loss = 0.00004995\n",
      "Iteration 63, loss = 0.00005252\n",
      "Iteration 64, loss = 0.00005507\n",
      "Iteration 65, loss = 0.00004797\n",
      "Iteration 66, loss = 0.00004885\n",
      "Iteration 67, loss = 0.00004823\n",
      "Iteration 68, loss = 0.00004949\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.19016525\n",
      "Iteration 2, loss = 0.11682644\n",
      "Iteration 3, loss = 0.07218071\n",
      "Iteration 4, loss = 0.03832422\n",
      "Iteration 5, loss = 0.02459578\n",
      "Iteration 6, loss = 0.01748430\n",
      "Iteration 7, loss = 0.01210088\n",
      "Iteration 8, loss = 0.00799764\n",
      "Iteration 9, loss = 0.00523414\n",
      "Iteration 10, loss = 0.00347429\n",
      "Iteration 11, loss = 0.00239124\n",
      "Iteration 12, loss = 0.00171405\n",
      "Iteration 13, loss = 0.00129026\n",
      "Iteration 14, loss = 0.00099738\n",
      "Iteration 15, loss = 0.00079743\n",
      "Iteration 16, loss = 0.00065176\n",
      "Iteration 17, loss = 0.00053968\n",
      "Iteration 18, loss = 0.00045630\n",
      "Iteration 19, loss = 0.00039168\n",
      "Iteration 20, loss = 0.00033849\n",
      "Iteration 21, loss = 0.00029646\n",
      "Iteration 22, loss = 0.00026723\n",
      "Iteration 23, loss = 0.00023067\n",
      "Iteration 24, loss = 0.00020611\n",
      "Iteration 25, loss = 0.00018554\n",
      "Iteration 26, loss = 0.00016836\n",
      "Iteration 27, loss = 0.00015103\n",
      "Iteration 28, loss = 0.00014156\n",
      "Iteration 29, loss = 0.00012851\n",
      "Iteration 30, loss = 0.00011972\n",
      "Iteration 31, loss = 0.00011311\n",
      "Iteration 32, loss = 0.00010265\n",
      "Iteration 33, loss = 0.00009703\n",
      "Iteration 34, loss = 0.00009027\n",
      "Iteration 35, loss = 0.00008727\n",
      "Iteration 36, loss = 0.00008158\n",
      "Iteration 37, loss = 0.00007748\n",
      "Iteration 38, loss = 0.00007339\n",
      "Iteration 39, loss = 0.00007046\n",
      "Iteration 40, loss = 0.00006794\n",
      "Iteration 41, loss = 0.00006575\n",
      "Iteration 42, loss = 0.00006421\n",
      "Iteration 43, loss = 0.00006150\n",
      "Iteration 44, loss = 0.00005982\n",
      "Iteration 45, loss = 0.00005975\n",
      "Iteration 46, loss = 0.00005793\n",
      "Iteration 47, loss = 0.00005478\n",
      "Iteration 48, loss = 0.00005380\n",
      "Iteration 49, loss = 0.00005196\n",
      "Iteration 50, loss = 0.00005222\n",
      "Iteration 51, loss = 0.00005203\n",
      "Iteration 52, loss = 0.00005064\n",
      "Iteration 53, loss = 0.00004942\n",
      "Iteration 54, loss = 0.00004754\n",
      "Iteration 55, loss = 0.00004660\n",
      "Iteration 56, loss = 0.00004787\n",
      "Iteration 57, loss = 0.00004430\n",
      "Iteration 58, loss = 0.00004445\n",
      "Iteration 59, loss = 0.00004459\n",
      "Iteration 60, loss = 0.00004241\n",
      "Iteration 61, loss = 0.00004304\n",
      "Iteration 62, loss = 0.00004208\n",
      "Iteration 63, loss = 0.00004276\n",
      "Iteration 64, loss = 0.00004272\n",
      "Iteration 65, loss = 0.00004013\n",
      "Iteration 66, loss = 0.00004002\n",
      "Iteration 67, loss = 0.00003967\n",
      "Iteration 68, loss = 0.00004115\n",
      "Iteration 69, loss = 0.00004201\n",
      "Iteration 70, loss = 0.00003917\n",
      "Iteration 71, loss = 0.00003869\n",
      "Iteration 72, loss = 0.00003753\n",
      "Iteration 73, loss = 0.00003775\n",
      "Iteration 74, loss = 0.00003710\n",
      "Iteration 75, loss = 0.00003744\n",
      "Iteration 76, loss = 0.00003864\n",
      "Iteration 77, loss = 0.00003554\n",
      "Iteration 78, loss = 0.00003860\n",
      "Iteration 79, loss = 0.00003850\n",
      "Iteration 80, loss = 0.00003458\n",
      "Iteration 81, loss = 0.00003418\n",
      "Iteration 82, loss = 0.00003604\n",
      "Iteration 83, loss = 0.00004040\n",
      "Iteration 84, loss = 0.00003764\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.18505320\n",
      "Iteration 2, loss = 0.11044104\n",
      "Iteration 3, loss = 0.06258542\n",
      "Iteration 4, loss = 0.03576405\n",
      "Iteration 5, loss = 0.02454271\n",
      "Iteration 6, loss = 0.01581968\n",
      "Iteration 7, loss = 0.00938799\n",
      "Iteration 8, loss = 0.00575231\n",
      "Iteration 9, loss = 0.00372123\n",
      "Iteration 10, loss = 0.00251458\n",
      "Iteration 11, loss = 0.00178479\n",
      "Iteration 12, loss = 0.00130133\n",
      "Iteration 13, loss = 0.00098986\n",
      "Iteration 14, loss = 0.00075866\n",
      "Iteration 15, loss = 0.00059367\n",
      "Iteration 16, loss = 0.00047727\n",
      "Iteration 17, loss = 0.00039410\n",
      "Iteration 18, loss = 0.00032430\n",
      "Iteration 19, loss = 0.00027348\n",
      "Iteration 20, loss = 0.00023079\n",
      "Iteration 21, loss = 0.00019996\n",
      "Iteration 22, loss = 0.00017308\n",
      "Iteration 23, loss = 0.00015118\n",
      "Iteration 24, loss = 0.00013406\n",
      "Iteration 25, loss = 0.00011821\n",
      "Iteration 26, loss = 0.00010796\n",
      "Iteration 27, loss = 0.00009882\n",
      "Iteration 28, loss = 0.00009035\n",
      "Iteration 29, loss = 0.00008198\n",
      "Iteration 30, loss = 0.00007534\n",
      "Iteration 31, loss = 0.00007046\n",
      "Iteration 32, loss = 0.00006740\n",
      "Iteration 33, loss = 0.00006198\n",
      "Iteration 34, loss = 0.00005800\n",
      "Iteration 35, loss = 0.00005554\n",
      "Iteration 36, loss = 0.00005463\n",
      "Iteration 37, loss = 0.00005034\n",
      "Iteration 38, loss = 0.00004884\n",
      "Iteration 39, loss = 0.00004725\n",
      "Iteration 40, loss = 0.00004561\n",
      "Iteration 41, loss = 0.00004425\n",
      "Iteration 42, loss = 0.00004321\n",
      "Iteration 43, loss = 0.00004196\n",
      "Iteration 44, loss = 0.00004044\n",
      "Iteration 45, loss = 0.00004047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 46, loss = 0.00003862\n",
      "Iteration 47, loss = 0.00003989\n",
      "Iteration 48, loss = 0.00003977\n",
      "Iteration 49, loss = 0.00003714\n",
      "Iteration 50, loss = 0.00003730\n",
      "Iteration 51, loss = 0.00003658\n",
      "Iteration 52, loss = 0.00003809\n",
      "Iteration 53, loss = 0.00003638\n",
      "Iteration 54, loss = 0.00003545\n",
      "Iteration 55, loss = 0.00003503\n",
      "Iteration 56, loss = 0.00003619\n",
      "Iteration 57, loss = 0.00003581\n",
      "Iteration 58, loss = 0.00003478\n",
      "Iteration 59, loss = 0.00003407\n",
      "Iteration 60, loss = 0.00003386\n",
      "Iteration 61, loss = 0.00003602\n",
      "Iteration 62, loss = 0.00003594\n",
      "Iteration 63, loss = 0.00003419\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20384274\n",
      "Iteration 2, loss = 0.12596332\n",
      "Iteration 3, loss = 0.06295752\n",
      "Iteration 4, loss = 0.02856895\n",
      "Iteration 5, loss = 0.01778040\n",
      "Iteration 6, loss = 0.01120426\n",
      "Iteration 7, loss = 0.00667421\n",
      "Iteration 8, loss = 0.00404668\n",
      "Iteration 9, loss = 0.00257133\n",
      "Iteration 10, loss = 0.00177079\n",
      "Iteration 11, loss = 0.00127179\n",
      "Iteration 12, loss = 0.00096846\n",
      "Iteration 13, loss = 0.00076325\n",
      "Iteration 14, loss = 0.00062414\n",
      "Iteration 15, loss = 0.00052066\n",
      "Iteration 16, loss = 0.00042653\n",
      "Iteration 17, loss = 0.00036386\n",
      "Iteration 18, loss = 0.00031611\n",
      "Iteration 19, loss = 0.00028409\n",
      "Iteration 20, loss = 0.00024853\n",
      "Iteration 21, loss = 0.00022266\n",
      "Iteration 22, loss = 0.00020164\n",
      "Iteration 23, loss = 0.00018249\n",
      "Iteration 24, loss = 0.00016516\n",
      "Iteration 25, loss = 0.00015159\n",
      "Iteration 26, loss = 0.00014144\n",
      "Iteration 27, loss = 0.00013014\n",
      "Iteration 28, loss = 0.00012081\n",
      "Iteration 29, loss = 0.00011259\n",
      "Iteration 30, loss = 0.00010561\n",
      "Iteration 31, loss = 0.00010073\n",
      "Iteration 32, loss = 0.00009580\n",
      "Iteration 33, loss = 0.00008867\n",
      "Iteration 34, loss = 0.00008337\n",
      "Iteration 35, loss = 0.00008153\n",
      "Iteration 36, loss = 0.00007732\n",
      "Iteration 37, loss = 0.00007234\n",
      "Iteration 38, loss = 0.00006938\n",
      "Iteration 39, loss = 0.00006719\n",
      "Iteration 40, loss = 0.00006459\n",
      "Iteration 41, loss = 0.00006179\n",
      "Iteration 42, loss = 0.00006012\n",
      "Iteration 43, loss = 0.00005855\n",
      "Iteration 44, loss = 0.00005708\n",
      "Iteration 45, loss = 0.00005642\n",
      "Iteration 46, loss = 0.00005401\n",
      "Iteration 47, loss = 0.00005294\n",
      "Iteration 48, loss = 0.00005271\n",
      "Iteration 49, loss = 0.00005039\n",
      "Iteration 50, loss = 0.00004847\n",
      "Iteration 51, loss = 0.00005069\n",
      "Iteration 52, loss = 0.00004720\n",
      "Iteration 53, loss = 0.00004851\n",
      "Iteration 54, loss = 0.00004765\n",
      "Iteration 55, loss = 0.00004540\n",
      "Iteration 56, loss = 0.00004483\n",
      "Iteration 57, loss = 0.00004524\n",
      "Iteration 58, loss = 0.00004443\n",
      "Iteration 59, loss = 0.00004478\n",
      "Iteration 60, loss = 0.00004404\n",
      "Iteration 61, loss = 0.00004308\n",
      "Iteration 62, loss = 0.00004327\n",
      "Iteration 63, loss = 0.00004138\n",
      "Iteration 64, loss = 0.00004357\n",
      "Iteration 65, loss = 0.00004070\n",
      "Iteration 66, loss = 0.00004570\n",
      "Iteration 67, loss = 0.00004245\n",
      "Iteration 68, loss = 0.00004024\n",
      "Iteration 69, loss = 0.00004244\n",
      "Iteration 70, loss = 0.00003934\n",
      "Iteration 71, loss = 0.00004202\n",
      "Iteration 72, loss = 0.00004241\n",
      "Iteration 73, loss = 0.00003944\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20816813\n",
      "Iteration 2, loss = 0.12045478\n",
      "Iteration 3, loss = 0.06682407\n",
      "Iteration 4, loss = 0.03320567\n",
      "Iteration 5, loss = 0.01983521\n",
      "Iteration 6, loss = 0.01226917\n",
      "Iteration 7, loss = 0.00736584\n",
      "Iteration 8, loss = 0.00477396\n",
      "Iteration 9, loss = 0.00333130\n",
      "Iteration 10, loss = 0.00242405\n",
      "Iteration 11, loss = 0.00182077\n",
      "Iteration 12, loss = 0.00142389\n",
      "Iteration 13, loss = 0.00113172\n",
      "Iteration 14, loss = 0.00091557\n",
      "Iteration 15, loss = 0.00075487\n",
      "Iteration 16, loss = 0.00062460\n",
      "Iteration 17, loss = 0.00052519\n",
      "Iteration 18, loss = 0.00044519\n",
      "Iteration 19, loss = 0.00038039\n",
      "Iteration 20, loss = 0.00032703\n",
      "Iteration 21, loss = 0.00028625\n",
      "Iteration 22, loss = 0.00025085\n",
      "Iteration 23, loss = 0.00022357\n",
      "Iteration 24, loss = 0.00020063\n",
      "Iteration 25, loss = 0.00017715\n",
      "Iteration 26, loss = 0.00016269\n",
      "Iteration 27, loss = 0.00014706\n",
      "Iteration 28, loss = 0.00013362\n",
      "Iteration 29, loss = 0.00012328\n",
      "Iteration 30, loss = 0.00011329\n",
      "Iteration 31, loss = 0.00010452\n",
      "Iteration 32, loss = 0.00009825\n",
      "Iteration 33, loss = 0.00009207\n",
      "Iteration 34, loss = 0.00008586\n",
      "Iteration 35, loss = 0.00008360\n",
      "Iteration 36, loss = 0.00007732\n",
      "Iteration 37, loss = 0.00007372\n",
      "Iteration 38, loss = 0.00007093\n",
      "Iteration 39, loss = 0.00006681\n",
      "Iteration 40, loss = 0.00006477\n",
      "Iteration 41, loss = 0.00006220\n",
      "Iteration 42, loss = 0.00005966\n",
      "Iteration 43, loss = 0.00005809\n",
      "Iteration 44, loss = 0.00005720\n",
      "Iteration 45, loss = 0.00005494\n",
      "Iteration 46, loss = 0.00005576\n",
      "Iteration 47, loss = 0.00005259\n",
      "Iteration 48, loss = 0.00005191\n",
      "Iteration 49, loss = 0.00005055\n",
      "Iteration 50, loss = 0.00004909\n",
      "Iteration 51, loss = 0.00004776\n",
      "Iteration 52, loss = 0.00004812\n",
      "Iteration 53, loss = 0.00004703\n",
      "Iteration 54, loss = 0.00004655\n",
      "Iteration 55, loss = 0.00004761\n",
      "Iteration 56, loss = 0.00004883\n",
      "Iteration 57, loss = 0.00004433\n",
      "Iteration 58, loss = 0.00004531\n",
      "Iteration 59, loss = 0.00004390\n",
      "Iteration 60, loss = 0.00004408\n",
      "Iteration 61, loss = 0.00004521\n",
      "Iteration 62, loss = 0.00004212\n",
      "Iteration 63, loss = 0.00004100\n",
      "Iteration 64, loss = 0.00004646\n",
      "Iteration 65, loss = 0.00004325\n",
      "Iteration 66, loss = 0.00004269\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20044378\n",
      "Iteration 2, loss = 0.12424034\n",
      "Iteration 3, loss = 0.06607336\n",
      "Iteration 4, loss = 0.03159686\n",
      "Iteration 5, loss = 0.02198419\n",
      "Iteration 6, loss = 0.01610929\n",
      "Iteration 7, loss = 0.01105193\n",
      "Iteration 8, loss = 0.00726337\n",
      "Iteration 9, loss = 0.00473270\n",
      "Iteration 10, loss = 0.00319060\n",
      "Iteration 11, loss = 0.00224022\n",
      "Iteration 12, loss = 0.00154456\n",
      "Iteration 13, loss = 0.00109942\n",
      "Iteration 14, loss = 0.00084135\n",
      "Iteration 15, loss = 0.00067867\n",
      "Iteration 16, loss = 0.00053875\n",
      "Iteration 17, loss = 0.00039798\n",
      "Iteration 18, loss = 0.00034113\n",
      "Iteration 19, loss = 0.00029491\n",
      "Iteration 20, loss = 0.00025860\n",
      "Iteration 21, loss = 0.00023313\n",
      "Iteration 22, loss = 0.00020600\n",
      "Iteration 23, loss = 0.00018347\n",
      "Iteration 24, loss = 0.00016429\n",
      "Iteration 25, loss = 0.00014907\n",
      "Iteration 26, loss = 0.00012427\n",
      "Iteration 27, loss = 0.00010357\n",
      "Iteration 28, loss = 0.00009088\n",
      "Iteration 29, loss = 0.00008193\n",
      "Iteration 30, loss = 0.00007498\n",
      "Iteration 31, loss = 0.00006937\n",
      "Iteration 32, loss = 0.00006490\n",
      "Iteration 33, loss = 0.00005978\n",
      "Iteration 34, loss = 0.00005827\n",
      "Iteration 35, loss = 0.00005445\n",
      "Iteration 36, loss = 0.00005061\n",
      "Iteration 37, loss = 0.00004870\n",
      "Iteration 38, loss = 0.00004610\n",
      "Iteration 39, loss = 0.00004550\n",
      "Iteration 40, loss = 0.00004540\n",
      "Iteration 41, loss = 0.00004373\n",
      "Iteration 42, loss = 0.00004212\n",
      "Iteration 43, loss = 0.00004077\n",
      "Iteration 44, loss = 0.00003997\n",
      "Iteration 45, loss = 0.00003907\n",
      "Iteration 46, loss = 0.00003881\n",
      "Iteration 47, loss = 0.00003829\n",
      "Iteration 48, loss = 0.00003649\n",
      "Iteration 49, loss = 0.00003673\n",
      "Iteration 50, loss = 0.00003732\n",
      "Iteration 51, loss = 0.00003690\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.21415614\n",
      "Iteration 2, loss = 0.13365133\n",
      "Iteration 3, loss = 0.09890046\n",
      "Iteration 4, loss = 0.06716024\n",
      "Iteration 5, loss = 0.04471417\n",
      "Iteration 6, loss = 0.03365361\n",
      "Iteration 7, loss = 0.02820641\n",
      "Iteration 8, loss = 0.02368649\n",
      "Iteration 9, loss = 0.01931266\n",
      "Iteration 10, loss = 0.01519126\n",
      "Iteration 11, loss = 0.01148552\n",
      "Iteration 12, loss = 0.00849775\n",
      "Iteration 13, loss = 0.00625941\n",
      "Iteration 14, loss = 0.00470107\n",
      "Iteration 15, loss = 0.00362518\n",
      "Iteration 16, loss = 0.00287371\n",
      "Iteration 17, loss = 0.00230662\n",
      "Iteration 18, loss = 0.00185642\n",
      "Iteration 19, loss = 0.00155724\n",
      "Iteration 20, loss = 0.00134140\n",
      "Iteration 21, loss = 0.00117041\n",
      "Iteration 22, loss = 0.00105172\n",
      "Iteration 23, loss = 0.00092785\n",
      "Iteration 24, loss = 0.00084950\n",
      "Iteration 25, loss = 0.00078222\n",
      "Iteration 26, loss = 0.00071229\n",
      "Iteration 27, loss = 0.00066761\n",
      "Iteration 28, loss = 0.00061331\n",
      "Iteration 29, loss = 0.00056787\n",
      "Iteration 30, loss = 0.00053944\n",
      "Iteration 31, loss = 0.00051113\n",
      "Iteration 32, loss = 0.00047897\n",
      "Iteration 33, loss = 0.00045255\n",
      "Iteration 34, loss = 0.00042999\n",
      "Iteration 35, loss = 0.00041144\n",
      "Iteration 36, loss = 0.00039549\n",
      "Iteration 37, loss = 0.00038249\n",
      "Iteration 38, loss = 0.00036770\n",
      "Iteration 39, loss = 0.00035179\n",
      "Iteration 40, loss = 0.00033937\n",
      "Iteration 41, loss = 0.00032957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 42, loss = 0.00032161\n",
      "Iteration 43, loss = 0.00031648\n",
      "Iteration 44, loss = 0.00030490\n",
      "Iteration 45, loss = 0.00030023\n",
      "Iteration 46, loss = 0.00029269\n",
      "Iteration 47, loss = 0.00028792\n",
      "Iteration 48, loss = 0.00028205\n",
      "Iteration 49, loss = 0.00027777\n",
      "Iteration 50, loss = 0.00027309\n",
      "Iteration 51, loss = 0.00027020\n",
      "Iteration 52, loss = 0.00026372\n",
      "Iteration 53, loss = 0.00026316\n",
      "Iteration 54, loss = 0.00025900\n",
      "Iteration 55, loss = 0.00025600\n",
      "Iteration 56, loss = 0.00025101\n",
      "Iteration 57, loss = 0.00025189\n",
      "Iteration 58, loss = 0.00025021\n",
      "Iteration 59, loss = 0.00024517\n",
      "Iteration 60, loss = 0.00024245\n",
      "Iteration 61, loss = 0.00024189\n",
      "Iteration 62, loss = 0.00023874\n",
      "Iteration 63, loss = 0.00023880\n",
      "Iteration 64, loss = 0.00023688\n",
      "Iteration 65, loss = 0.00023630\n",
      "Iteration 66, loss = 0.00023245\n",
      "Iteration 67, loss = 0.00023367\n",
      "Iteration 68, loss = 0.00022976\n",
      "Iteration 69, loss = 0.00022798\n",
      "Iteration 70, loss = 0.00022817\n",
      "Iteration 71, loss = 0.00022643\n",
      "Iteration 72, loss = 0.00022452\n",
      "Iteration 73, loss = 0.00022334\n",
      "Iteration 74, loss = 0.00022501\n",
      "Iteration 75, loss = 0.00022131\n",
      "Iteration 76, loss = 0.00022341\n",
      "Iteration 77, loss = 0.00021858\n",
      "Iteration 78, loss = 0.00021726\n",
      "Iteration 79, loss = 0.00022025\n",
      "Iteration 80, loss = 0.00021560\n",
      "Iteration 81, loss = 0.00021855\n",
      "Iteration 82, loss = 0.00021542\n",
      "Iteration 83, loss = 0.00021401\n",
      "Iteration 84, loss = 0.00021556\n",
      "Iteration 85, loss = 0.00021321\n",
      "Iteration 86, loss = 0.00021170\n",
      "Iteration 87, loss = 0.00021087\n",
      "Iteration 88, loss = 0.00020978\n",
      "Iteration 89, loss = 0.00021810\n",
      "Iteration 90, loss = 0.00021079\n",
      "Iteration 91, loss = 0.00021073\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.21691570\n",
      "Iteration 2, loss = 0.15805782\n",
      "Iteration 3, loss = 0.12430471\n",
      "Iteration 4, loss = 0.09514855\n",
      "Iteration 5, loss = 0.06489502\n",
      "Iteration 6, loss = 0.04233842\n",
      "Iteration 7, loss = 0.03046076\n",
      "Iteration 8, loss = 0.02387760\n",
      "Iteration 9, loss = 0.01899658\n",
      "Iteration 10, loss = 0.01501150\n",
      "Iteration 11, loss = 0.01174453\n",
      "Iteration 12, loss = 0.00915313\n",
      "Iteration 13, loss = 0.00711122\n",
      "Iteration 14, loss = 0.00558665\n",
      "Iteration 15, loss = 0.00445607\n",
      "Iteration 16, loss = 0.00355902\n",
      "Iteration 17, loss = 0.00290911\n",
      "Iteration 18, loss = 0.00239393\n",
      "Iteration 19, loss = 0.00201032\n",
      "Iteration 20, loss = 0.00170799\n",
      "Iteration 21, loss = 0.00146685\n",
      "Iteration 22, loss = 0.00127584\n",
      "Iteration 23, loss = 0.00111425\n",
      "Iteration 24, loss = 0.00097911\n",
      "Iteration 25, loss = 0.00087843\n",
      "Iteration 26, loss = 0.00079463\n",
      "Iteration 27, loss = 0.00072102\n",
      "Iteration 28, loss = 0.00065792\n",
      "Iteration 29, loss = 0.00060920\n",
      "Iteration 30, loss = 0.00057215\n",
      "Iteration 31, loss = 0.00053166\n",
      "Iteration 32, loss = 0.00050114\n",
      "Iteration 33, loss = 0.00047134\n",
      "Iteration 34, loss = 0.00044962\n",
      "Iteration 35, loss = 0.00042791\n",
      "Iteration 36, loss = 0.00040759\n",
      "Iteration 37, loss = 0.00039366\n",
      "Iteration 38, loss = 0.00037269\n",
      "Iteration 39, loss = 0.00035967\n",
      "Iteration 40, loss = 0.00034710\n",
      "Iteration 41, loss = 0.00033166\n",
      "Iteration 42, loss = 0.00032439\n",
      "Iteration 43, loss = 0.00031214\n",
      "Iteration 44, loss = 0.00030031\n",
      "Iteration 45, loss = 0.00028681\n",
      "Iteration 46, loss = 0.00027473\n",
      "Iteration 47, loss = 0.00026827\n",
      "Iteration 48, loss = 0.00026173\n",
      "Iteration 49, loss = 0.00025350\n",
      "Iteration 50, loss = 0.00024875\n",
      "Iteration 51, loss = 0.00024607\n",
      "Iteration 52, loss = 0.00024053\n",
      "Iteration 53, loss = 0.00023727\n",
      "Iteration 54, loss = 0.00023620\n",
      "Iteration 55, loss = 0.00023029\n",
      "Iteration 56, loss = 0.00022985\n",
      "Iteration 57, loss = 0.00022359\n",
      "Iteration 58, loss = 0.00022259\n",
      "Iteration 59, loss = 0.00022036\n",
      "Iteration 60, loss = 0.00021840\n",
      "Iteration 61, loss = 0.00021619\n",
      "Iteration 62, loss = 0.00021573\n",
      "Iteration 63, loss = 0.00021328\n",
      "Iteration 64, loss = 0.00021234\n",
      "Iteration 65, loss = 0.00021166\n",
      "Iteration 66, loss = 0.00021054\n",
      "Iteration 67, loss = 0.00020821\n",
      "Iteration 68, loss = 0.00020666\n",
      "Iteration 69, loss = 0.00020820\n",
      "Iteration 70, loss = 0.00020428\n",
      "Iteration 71, loss = 0.00020443\n",
      "Iteration 72, loss = 0.00020195\n",
      "Iteration 73, loss = 0.00020196\n",
      "Iteration 74, loss = 0.00020122\n",
      "Iteration 75, loss = 0.00020216\n",
      "Iteration 76, loss = 0.00020170\n",
      "Iteration 77, loss = 0.00019890\n",
      "Iteration 78, loss = 0.00019747\n",
      "Iteration 79, loss = 0.00020210\n",
      "Iteration 80, loss = 0.00020292\n",
      "Iteration 81, loss = 0.00019580\n",
      "Iteration 82, loss = 0.00019743\n",
      "Iteration 83, loss = 0.00019639\n",
      "Iteration 84, loss = 0.00019589\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20955462\n",
      "Iteration 2, loss = 0.16166901\n",
      "Iteration 3, loss = 0.13305242\n",
      "Iteration 4, loss = 0.09838289\n",
      "Iteration 5, loss = 0.06034110\n",
      "Iteration 6, loss = 0.03721206\n",
      "Iteration 7, loss = 0.02721947\n",
      "Iteration 8, loss = 0.02059306\n",
      "Iteration 9, loss = 0.01516905\n",
      "Iteration 10, loss = 0.01094903\n",
      "Iteration 11, loss = 0.00791100\n",
      "Iteration 12, loss = 0.00581330\n",
      "Iteration 13, loss = 0.00440851\n",
      "Iteration 14, loss = 0.00342113\n",
      "Iteration 15, loss = 0.00271390\n",
      "Iteration 16, loss = 0.00222174\n",
      "Iteration 17, loss = 0.00186006\n",
      "Iteration 18, loss = 0.00157851\n",
      "Iteration 19, loss = 0.00135926\n",
      "Iteration 20, loss = 0.00118810\n",
      "Iteration 21, loss = 0.00105444\n",
      "Iteration 22, loss = 0.00094688\n",
      "Iteration 23, loss = 0.00085973\n",
      "Iteration 24, loss = 0.00078454\n",
      "Iteration 25, loss = 0.00072146\n",
      "Iteration 26, loss = 0.00067330\n",
      "Iteration 27, loss = 0.00062523\n",
      "Iteration 28, loss = 0.00058646\n",
      "Iteration 29, loss = 0.00055505\n",
      "Iteration 30, loss = 0.00051773\n",
      "Iteration 31, loss = 0.00046752\n",
      "Iteration 32, loss = 0.00041809\n",
      "Iteration 33, loss = 0.00037657\n",
      "Iteration 34, loss = 0.00034832\n",
      "Iteration 35, loss = 0.00032674\n",
      "Iteration 36, loss = 0.00031596\n",
      "Iteration 37, loss = 0.00030341\n",
      "Iteration 38, loss = 0.00029744\n",
      "Iteration 39, loss = 0.00029140\n",
      "Iteration 40, loss = 0.00028550\n",
      "Iteration 41, loss = 0.00028265\n",
      "Iteration 42, loss = 0.00027887\n",
      "Iteration 43, loss = 0.00027575\n",
      "Iteration 44, loss = 0.00027302\n",
      "Iteration 45, loss = 0.00027203\n",
      "Iteration 46, loss = 0.00026753\n",
      "Iteration 47, loss = 0.00026713\n",
      "Iteration 48, loss = 0.00026375\n",
      "Iteration 49, loss = 0.00026488\n",
      "Iteration 50, loss = 0.00026207\n",
      "Iteration 51, loss = 0.00026190\n",
      "Iteration 52, loss = 0.00025819\n",
      "Iteration 53, loss = 0.00025813\n",
      "Iteration 54, loss = 0.00025617\n",
      "Iteration 55, loss = 0.00025391\n",
      "Iteration 56, loss = 0.00025362\n",
      "Iteration 57, loss = 0.00025359\n",
      "Iteration 58, loss = 0.00024969\n",
      "Iteration 59, loss = 0.00025001\n",
      "Iteration 60, loss = 0.00024851\n",
      "Iteration 61, loss = 0.00024997\n",
      "Iteration 62, loss = 0.00024712\n",
      "Iteration 63, loss = 0.00024516\n",
      "Iteration 64, loss = 0.00024524\n",
      "Iteration 65, loss = 0.00024467\n",
      "Iteration 66, loss = 0.00024206\n",
      "Iteration 67, loss = 0.00024444\n",
      "Iteration 68, loss = 0.00023951\n",
      "Iteration 69, loss = 0.00023742\n",
      "Iteration 70, loss = 0.00023748\n",
      "Iteration 71, loss = 0.00023862\n",
      "Iteration 72, loss = 0.00023599\n",
      "Iteration 73, loss = 0.00023522\n",
      "Iteration 74, loss = 0.00023561\n",
      "Iteration 75, loss = 0.00023672\n",
      "Iteration 76, loss = 0.00023249\n",
      "Iteration 77, loss = 0.00023375\n",
      "Iteration 78, loss = 0.00023204\n",
      "Iteration 79, loss = 0.00023106\n",
      "Iteration 80, loss = 0.00023172\n",
      "Iteration 81, loss = 0.00022944\n",
      "Iteration 82, loss = 0.00022896\n",
      "Iteration 83, loss = 0.00023010\n",
      "Iteration 84, loss = 0.00022749\n",
      "Iteration 85, loss = 0.00022778\n",
      "Iteration 86, loss = 0.00022543\n",
      "Iteration 87, loss = 0.00022432\n",
      "Iteration 88, loss = 0.00022347\n",
      "Iteration 89, loss = 0.00022550\n",
      "Iteration 90, loss = 0.00022430\n",
      "Iteration 91, loss = 0.00022295\n",
      "Iteration 92, loss = 0.00022351\n",
      "Iteration 93, loss = 0.00021962\n",
      "Iteration 94, loss = 0.00022173\n",
      "Iteration 95, loss = 0.00022001\n",
      "Iteration 96, loss = 0.00021903\n",
      "Iteration 97, loss = 0.00022002\n",
      "Iteration 98, loss = 0.00021979\n",
      "Iteration 99, loss = 0.00021871\n",
      "Iteration 100, loss = 0.00021814\n",
      "Iteration 101, loss = 0.00021753\n",
      "Iteration 102, loss = 0.00021434\n",
      "Iteration 103, loss = 0.00021501\n",
      "Iteration 104, loss = 0.00021761\n",
      "Iteration 105, loss = 0.00021410\n",
      "Iteration 106, loss = 0.00021406\n",
      "Iteration 107, loss = 0.00021269\n",
      "Iteration 108, loss = 0.00021357\n",
      "Iteration 109, loss = 0.00021354\n",
      "Iteration 110, loss = 0.00021367\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.18772018\n",
      "Iteration 2, loss = 0.11941475\n",
      "Iteration 3, loss = 0.06662585\n",
      "Iteration 4, loss = 0.03257879\n",
      "Iteration 5, loss = 0.01897671\n",
      "Iteration 6, loss = 0.01059304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.00588177\n",
      "Iteration 8, loss = 0.00360094\n",
      "Iteration 9, loss = 0.00237476\n",
      "Iteration 10, loss = 0.00164685\n",
      "Iteration 11, loss = 0.00121628\n",
      "Iteration 12, loss = 0.00094413\n",
      "Iteration 13, loss = 0.00076209\n",
      "Iteration 14, loss = 0.00063499\n",
      "Iteration 15, loss = 0.00054662\n",
      "Iteration 16, loss = 0.00047790\n",
      "Iteration 17, loss = 0.00042724\n",
      "Iteration 18, loss = 0.00038896\n",
      "Iteration 19, loss = 0.00035781\n",
      "Iteration 20, loss = 0.00033333\n",
      "Iteration 21, loss = 0.00031423\n",
      "Iteration 22, loss = 0.00030114\n",
      "Iteration 23, loss = 0.00028736\n",
      "Iteration 24, loss = 0.00027587\n",
      "Iteration 25, loss = 0.00026371\n",
      "Iteration 26, loss = 0.00025425\n",
      "Iteration 27, loss = 0.00024815\n",
      "Iteration 28, loss = 0.00024326\n",
      "Iteration 29, loss = 0.00023871\n",
      "Iteration 30, loss = 0.00023539\n",
      "Iteration 31, loss = 0.00023373\n",
      "Iteration 32, loss = 0.00023028\n",
      "Iteration 33, loss = 0.00022760\n",
      "Iteration 34, loss = 0.00022514\n",
      "Iteration 35, loss = 0.00022357\n",
      "Iteration 36, loss = 0.00022222\n",
      "Iteration 37, loss = 0.00022094\n",
      "Iteration 38, loss = 0.00021943\n",
      "Iteration 39, loss = 0.00021788\n",
      "Iteration 40, loss = 0.00021706\n",
      "Iteration 41, loss = 0.00021629\n",
      "Iteration 42, loss = 0.00021474\n",
      "Iteration 43, loss = 0.00021344\n",
      "Iteration 44, loss = 0.00021247\n",
      "Iteration 45, loss = 0.00021140\n",
      "Iteration 46, loss = 0.00021061\n",
      "Iteration 47, loss = 0.00020976\n",
      "Iteration 48, loss = 0.00020951\n",
      "Iteration 49, loss = 0.00020914\n",
      "Iteration 50, loss = 0.00020821\n",
      "Iteration 51, loss = 0.00020676\n",
      "Iteration 52, loss = 0.00020678\n",
      "Iteration 53, loss = 0.00020628\n",
      "Iteration 54, loss = 0.00020428\n",
      "Iteration 55, loss = 0.00020372\n",
      "Iteration 56, loss = 0.00020347\n",
      "Iteration 57, loss = 0.00020257\n",
      "Iteration 58, loss = 0.00020124\n",
      "Iteration 59, loss = 0.00020153\n",
      "Iteration 60, loss = 0.00020013\n",
      "Iteration 61, loss = 0.00020017\n",
      "Iteration 62, loss = 0.00019933\n",
      "Iteration 63, loss = 0.00019975\n",
      "Iteration 64, loss = 0.00019855\n",
      "Iteration 65, loss = 0.00019728\n",
      "Iteration 66, loss = 0.00019898\n",
      "Iteration 67, loss = 0.00019697\n",
      "Iteration 68, loss = 0.00019572\n",
      "Iteration 69, loss = 0.00019610\n",
      "Iteration 70, loss = 0.00019500\n",
      "Iteration 71, loss = 0.00019445\n",
      "Iteration 72, loss = 0.00019627\n",
      "Iteration 73, loss = 0.00019403\n",
      "Iteration 74, loss = 0.00019241\n",
      "Iteration 75, loss = 0.00019366\n",
      "Iteration 76, loss = 0.00019070\n",
      "Iteration 77, loss = 0.00019087\n",
      "Iteration 78, loss = 0.00019274\n",
      "Iteration 79, loss = 0.00018987\n",
      "Iteration 80, loss = 0.00019030\n",
      "Iteration 81, loss = 0.00018807\n",
      "Iteration 82, loss = 0.00018941\n",
      "Iteration 83, loss = 0.00018770\n",
      "Iteration 84, loss = 0.00018626\n",
      "Iteration 85, loss = 0.00018549\n",
      "Iteration 86, loss = 0.00018578\n",
      "Iteration 87, loss = 0.00018558\n",
      "Iteration 88, loss = 0.00018443\n",
      "Iteration 89, loss = 0.00018409\n",
      "Iteration 90, loss = 0.00018352\n",
      "Iteration 91, loss = 0.00018316\n",
      "Iteration 92, loss = 0.00018446\n",
      "Iteration 93, loss = 0.00018187\n",
      "Iteration 94, loss = 0.00018176\n",
      "Iteration 95, loss = 0.00018143\n",
      "Iteration 96, loss = 0.00017942\n",
      "Iteration 97, loss = 0.00017999\n",
      "Iteration 98, loss = 0.00018098\n",
      "Iteration 99, loss = 0.00017927\n",
      "Iteration 100, loss = 0.00017690\n",
      "Iteration 101, loss = 0.00017668\n",
      "Iteration 102, loss = 0.00017686\n",
      "Iteration 103, loss = 0.00017629\n",
      "Iteration 104, loss = 0.00017676\n",
      "Iteration 105, loss = 0.00017703\n",
      "Iteration 106, loss = 0.00017580\n",
      "Iteration 107, loss = 0.00017668\n",
      "Iteration 108, loss = 0.00017310\n",
      "Iteration 109, loss = 0.00017142\n",
      "Iteration 110, loss = 0.00017276\n",
      "Iteration 111, loss = 0.00017177\n",
      "Iteration 112, loss = 0.00017134\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20186932\n",
      "Iteration 2, loss = 0.12797561\n",
      "Iteration 3, loss = 0.07693254\n",
      "Iteration 4, loss = 0.03621732\n",
      "Iteration 5, loss = 0.01826053\n",
      "Iteration 6, loss = 0.01083153\n",
      "Iteration 7, loss = 0.00673736\n",
      "Iteration 8, loss = 0.00432840\n",
      "Iteration 9, loss = 0.00298949\n",
      "Iteration 10, loss = 0.00219383\n",
      "Iteration 11, loss = 0.00169194\n",
      "Iteration 12, loss = 0.00134806\n",
      "Iteration 13, loss = 0.00110049\n",
      "Iteration 14, loss = 0.00091632\n",
      "Iteration 15, loss = 0.00078417\n",
      "Iteration 16, loss = 0.00068318\n",
      "Iteration 17, loss = 0.00060466\n",
      "Iteration 18, loss = 0.00053395\n",
      "Iteration 19, loss = 0.00048306\n",
      "Iteration 20, loss = 0.00044406\n",
      "Iteration 21, loss = 0.00040919\n",
      "Iteration 22, loss = 0.00038362\n",
      "Iteration 23, loss = 0.00035961\n",
      "Iteration 24, loss = 0.00034001\n",
      "Iteration 25, loss = 0.00032425\n",
      "Iteration 26, loss = 0.00031364\n",
      "Iteration 27, loss = 0.00029912\n",
      "Iteration 28, loss = 0.00028958\n",
      "Iteration 29, loss = 0.00027933\n",
      "Iteration 30, loss = 0.00027161\n",
      "Iteration 31, loss = 0.00026372\n",
      "Iteration 32, loss = 0.00025745\n",
      "Iteration 33, loss = 0.00025300\n",
      "Iteration 34, loss = 0.00024798\n",
      "Iteration 35, loss = 0.00024372\n",
      "Iteration 36, loss = 0.00023998\n",
      "Iteration 37, loss = 0.00023877\n",
      "Iteration 38, loss = 0.00023547\n",
      "Iteration 39, loss = 0.00023180\n",
      "Iteration 40, loss = 0.00022939\n",
      "Iteration 41, loss = 0.00022716\n",
      "Iteration 42, loss = 0.00022470\n",
      "Iteration 43, loss = 0.00022243\n",
      "Iteration 44, loss = 0.00022158\n",
      "Iteration 45, loss = 0.00021941\n",
      "Iteration 46, loss = 0.00021817\n",
      "Iteration 47, loss = 0.00021717\n",
      "Iteration 48, loss = 0.00021580\n",
      "Iteration 49, loss = 0.00021425\n",
      "Iteration 50, loss = 0.00021327\n",
      "Iteration 51, loss = 0.00021254\n",
      "Iteration 52, loss = 0.00021273\n",
      "Iteration 53, loss = 0.00020952\n",
      "Iteration 54, loss = 0.00020904\n",
      "Iteration 55, loss = 0.00020928\n",
      "Iteration 56, loss = 0.00020854\n",
      "Iteration 57, loss = 0.00020666\n",
      "Iteration 58, loss = 0.00020746\n",
      "Iteration 59, loss = 0.00020708\n",
      "Iteration 60, loss = 0.00020457\n",
      "Iteration 61, loss = 0.00020509\n",
      "Iteration 62, loss = 0.00020406\n",
      "Iteration 63, loss = 0.00020216\n",
      "Iteration 64, loss = 0.00020192\n",
      "Iteration 65, loss = 0.00020262\n",
      "Iteration 66, loss = 0.00020055\n",
      "Iteration 67, loss = 0.00020110\n",
      "Iteration 68, loss = 0.00019945\n",
      "Iteration 69, loss = 0.00019843\n",
      "Iteration 70, loss = 0.00019776\n",
      "Iteration 71, loss = 0.00019679\n",
      "Iteration 72, loss = 0.00019666\n",
      "Iteration 73, loss = 0.00020073\n",
      "Iteration 74, loss = 0.00019807\n",
      "Iteration 75, loss = 0.00019674\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20230619\n",
      "Iteration 2, loss = 0.12248342\n",
      "Iteration 3, loss = 0.07831144\n",
      "Iteration 4, loss = 0.04315231\n",
      "Iteration 5, loss = 0.02871602\n",
      "Iteration 6, loss = 0.02155257\n",
      "Iteration 7, loss = 0.01520491\n",
      "Iteration 8, loss = 0.01017033\n",
      "Iteration 9, loss = 0.00675180\n",
      "Iteration 10, loss = 0.00462760\n",
      "Iteration 11, loss = 0.00337391\n",
      "Iteration 12, loss = 0.00254929\n",
      "Iteration 13, loss = 0.00199955\n",
      "Iteration 14, loss = 0.00158968\n",
      "Iteration 15, loss = 0.00127929\n",
      "Iteration 16, loss = 0.00107548\n",
      "Iteration 17, loss = 0.00091699\n",
      "Iteration 18, loss = 0.00078996\n",
      "Iteration 19, loss = 0.00069724\n",
      "Iteration 20, loss = 0.00062424\n",
      "Iteration 21, loss = 0.00056544\n",
      "Iteration 22, loss = 0.00052268\n",
      "Iteration 23, loss = 0.00047733\n",
      "Iteration 24, loss = 0.00044021\n",
      "Iteration 25, loss = 0.00041395\n",
      "Iteration 26, loss = 0.00038959\n",
      "Iteration 27, loss = 0.00037081\n",
      "Iteration 28, loss = 0.00034916\n",
      "Iteration 29, loss = 0.00033499\n",
      "Iteration 30, loss = 0.00031995\n",
      "Iteration 31, loss = 0.00031071\n",
      "Iteration 32, loss = 0.00029851\n",
      "Iteration 33, loss = 0.00029014\n",
      "Iteration 34, loss = 0.00028348\n",
      "Iteration 35, loss = 0.00027534\n",
      "Iteration 36, loss = 0.00026984\n",
      "Iteration 37, loss = 0.00026485\n",
      "Iteration 38, loss = 0.00026044\n",
      "Iteration 39, loss = 0.00025330\n",
      "Iteration 40, loss = 0.00024967\n",
      "Iteration 41, loss = 0.00024652\n",
      "Iteration 42, loss = 0.00024301\n",
      "Iteration 43, loss = 0.00023977\n",
      "Iteration 44, loss = 0.00023768\n",
      "Iteration 45, loss = 0.00023408\n",
      "Iteration 46, loss = 0.00023364\n",
      "Iteration 47, loss = 0.00023024\n",
      "Iteration 48, loss = 0.00022808\n",
      "Iteration 49, loss = 0.00022657\n",
      "Iteration 50, loss = 0.00022503\n",
      "Iteration 51, loss = 0.00022314\n",
      "Iteration 52, loss = 0.00022182\n",
      "Iteration 53, loss = 0.00022049\n",
      "Iteration 54, loss = 0.00021915\n",
      "Iteration 55, loss = 0.00021827\n",
      "Iteration 56, loss = 0.00021748\n",
      "Iteration 57, loss = 0.00021610\n",
      "Iteration 58, loss = 0.00021709\n",
      "Iteration 59, loss = 0.00021339\n",
      "Iteration 60, loss = 0.00021294\n",
      "Iteration 61, loss = 0.00021290\n",
      "Iteration 62, loss = 0.00021234\n",
      "Iteration 63, loss = 0.00021107\n",
      "Iteration 64, loss = 0.00021236\n",
      "Iteration 65, loss = 0.00020813\n",
      "Iteration 66, loss = 0.00021082\n",
      "Iteration 67, loss = 0.00020822\n",
      "Iteration 68, loss = 0.00020779\n",
      "Iteration 69, loss = 0.00020533\n",
      "Iteration 70, loss = 0.00020535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 71, loss = 0.00020455\n",
      "Iteration 72, loss = 0.00020315\n",
      "Iteration 73, loss = 0.00020356\n",
      "Iteration 74, loss = 0.00020304\n",
      "Iteration 75, loss = 0.00020094\n",
      "Iteration 76, loss = 0.00020093\n",
      "Iteration 77, loss = 0.00020194\n",
      "Iteration 78, loss = 0.00020264\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.17319812\n",
      "Iteration 2, loss = 0.09625878\n",
      "Iteration 3, loss = 0.04292644\n",
      "Iteration 4, loss = 0.02226533\n",
      "Iteration 5, loss = 0.01418693\n",
      "Iteration 6, loss = 0.00868974\n",
      "Iteration 7, loss = 0.00537667\n",
      "Iteration 8, loss = 0.00351904\n",
      "Iteration 9, loss = 0.00247117\n",
      "Iteration 10, loss = 0.00184608\n",
      "Iteration 11, loss = 0.00146323\n",
      "Iteration 12, loss = 0.00119435\n",
      "Iteration 13, loss = 0.00100092\n",
      "Iteration 14, loss = 0.00085428\n",
      "Iteration 15, loss = 0.00076153\n",
      "Iteration 16, loss = 0.00067592\n",
      "Iteration 17, loss = 0.00061734\n",
      "Iteration 18, loss = 0.00056646\n",
      "Iteration 19, loss = 0.00053048\n",
      "Iteration 20, loss = 0.00049647\n",
      "Iteration 21, loss = 0.00047046\n",
      "Iteration 22, loss = 0.00045079\n",
      "Iteration 23, loss = 0.00043162\n",
      "Iteration 24, loss = 0.00041314\n",
      "Iteration 25, loss = 0.00039661\n",
      "Iteration 26, loss = 0.00038335\n",
      "Iteration 27, loss = 0.00036623\n",
      "Iteration 28, loss = 0.00035543\n",
      "Iteration 29, loss = 0.00034926\n",
      "Iteration 30, loss = 0.00034067\n",
      "Iteration 31, loss = 0.00033046\n",
      "Iteration 32, loss = 0.00032507\n",
      "Iteration 33, loss = 0.00032022\n",
      "Iteration 34, loss = 0.00031524\n",
      "Iteration 35, loss = 0.00030786\n",
      "Iteration 36, loss = 0.00030583\n",
      "Iteration 37, loss = 0.00030109\n",
      "Iteration 38, loss = 0.00029630\n",
      "Iteration 39, loss = 0.00029278\n",
      "Iteration 40, loss = 0.00029185\n",
      "Iteration 41, loss = 0.00028741\n",
      "Iteration 42, loss = 0.00028523\n",
      "Iteration 43, loss = 0.00028019\n",
      "Iteration 44, loss = 0.00028116\n",
      "Iteration 45, loss = 0.00027791\n",
      "Iteration 46, loss = 0.00027557\n",
      "Iteration 47, loss = 0.00027372\n",
      "Iteration 48, loss = 0.00027331\n",
      "Iteration 49, loss = 0.00027214\n",
      "Iteration 50, loss = 0.00027113\n",
      "Iteration 51, loss = 0.00026802\n",
      "Iteration 52, loss = 0.00026693\n",
      "Iteration 53, loss = 0.00025590\n",
      "Iteration 54, loss = 0.00025282\n",
      "Iteration 55, loss = 0.00025256\n",
      "Iteration 56, loss = 0.00025471\n",
      "Iteration 57, loss = 0.00025046\n",
      "Iteration 58, loss = 0.00025274\n",
      "Iteration 59, loss = 0.00024835\n",
      "Iteration 60, loss = 0.00024929\n",
      "Iteration 61, loss = 0.00024718\n",
      "Iteration 62, loss = 0.00024998\n",
      "Iteration 63, loss = 0.00025569\n",
      "Iteration 64, loss = 0.00024247\n",
      "Iteration 65, loss = 0.00024323\n",
      "Iteration 66, loss = 0.00024155\n",
      "Iteration 67, loss = 0.00024729\n",
      "Iteration 68, loss = 0.00024047\n",
      "Iteration 69, loss = 0.00024043\n",
      "Iteration 70, loss = 0.00024163\n",
      "Iteration 71, loss = 0.00023668\n",
      "Iteration 72, loss = 0.00024390\n",
      "Iteration 73, loss = 0.00023859\n",
      "Iteration 74, loss = 0.00023897\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.19447944\n",
      "Iteration 2, loss = 0.12521867\n",
      "Iteration 3, loss = 0.07181340\n",
      "Iteration 4, loss = 0.03637140\n",
      "Iteration 5, loss = 0.02356723\n",
      "Iteration 6, loss = 0.01542674\n",
      "Iteration 7, loss = 0.00952949\n",
      "Iteration 8, loss = 0.00577921\n",
      "Iteration 9, loss = 0.00369041\n",
      "Iteration 10, loss = 0.00248501\n",
      "Iteration 11, loss = 0.00177625\n",
      "Iteration 12, loss = 0.00132808\n",
      "Iteration 13, loss = 0.00103566\n",
      "Iteration 14, loss = 0.00084126\n",
      "Iteration 15, loss = 0.00070097\n",
      "Iteration 16, loss = 0.00060881\n",
      "Iteration 17, loss = 0.00054332\n",
      "Iteration 18, loss = 0.00050130\n",
      "Iteration 19, loss = 0.00045536\n",
      "Iteration 20, loss = 0.00042278\n",
      "Iteration 21, loss = 0.00040207\n",
      "Iteration 22, loss = 0.00037703\n",
      "Iteration 23, loss = 0.00036124\n",
      "Iteration 24, loss = 0.00034507\n",
      "Iteration 25, loss = 0.00033107\n",
      "Iteration 26, loss = 0.00032199\n",
      "Iteration 27, loss = 0.00031340\n",
      "Iteration 28, loss = 0.00030779\n",
      "Iteration 29, loss = 0.00030137\n",
      "Iteration 30, loss = 0.00029554\n",
      "Iteration 31, loss = 0.00028898\n",
      "Iteration 32, loss = 0.00028575\n",
      "Iteration 33, loss = 0.00028178\n",
      "Iteration 34, loss = 0.00027693\n",
      "Iteration 35, loss = 0.00027582\n",
      "Iteration 36, loss = 0.00027149\n",
      "Iteration 37, loss = 0.00026955\n",
      "Iteration 38, loss = 0.00026837\n",
      "Iteration 39, loss = 0.00026330\n",
      "Iteration 40, loss = 0.00026063\n",
      "Iteration 41, loss = 0.00025902\n",
      "Iteration 42, loss = 0.00025692\n",
      "Iteration 43, loss = 0.00025605\n",
      "Iteration 44, loss = 0.00025475\n",
      "Iteration 45, loss = 0.00025306\n",
      "Iteration 46, loss = 0.00025162\n",
      "Iteration 47, loss = 0.00024951\n",
      "Iteration 48, loss = 0.00025006\n",
      "Iteration 49, loss = 0.00024687\n",
      "Iteration 50, loss = 0.00024814\n",
      "Iteration 51, loss = 0.00024550\n",
      "Iteration 52, loss = 0.00024569\n",
      "Iteration 53, loss = 0.00024428\n",
      "Iteration 54, loss = 0.00024239\n",
      "Iteration 55, loss = 0.00024293\n",
      "Iteration 56, loss = 0.00024101\n",
      "Iteration 57, loss = 0.00024028\n",
      "Iteration 58, loss = 0.00023807\n",
      "Iteration 59, loss = 0.00024009\n",
      "Iteration 60, loss = 0.00023887\n",
      "Iteration 61, loss = 0.00023789\n",
      "Iteration 62, loss = 0.00023815\n",
      "Iteration 63, loss = 0.00023477\n",
      "Iteration 64, loss = 0.00023403\n",
      "Iteration 65, loss = 0.00023370\n",
      "Iteration 66, loss = 0.00023158\n",
      "Iteration 67, loss = 0.00023295\n",
      "Iteration 68, loss = 0.00023356\n",
      "Iteration 69, loss = 0.00023575\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20060449\n",
      "Iteration 2, loss = 0.12727427\n",
      "Iteration 3, loss = 0.06987670\n",
      "Iteration 4, loss = 0.03225384\n",
      "Iteration 5, loss = 0.02034352\n",
      "Iteration 6, loss = 0.01321168\n",
      "Iteration 7, loss = 0.00779130\n",
      "Iteration 8, loss = 0.00451140\n",
      "Iteration 9, loss = 0.00287342\n",
      "Iteration 10, loss = 0.00199496\n",
      "Iteration 11, loss = 0.00150207\n",
      "Iteration 12, loss = 0.00116567\n",
      "Iteration 13, loss = 0.00093592\n",
      "Iteration 14, loss = 0.00080343\n",
      "Iteration 15, loss = 0.00068594\n",
      "Iteration 16, loss = 0.00059192\n",
      "Iteration 17, loss = 0.00052305\n",
      "Iteration 18, loss = 0.00047791\n",
      "Iteration 19, loss = 0.00043817\n",
      "Iteration 20, loss = 0.00040702\n",
      "Iteration 21, loss = 0.00038090\n",
      "Iteration 22, loss = 0.00036497\n",
      "Iteration 23, loss = 0.00035041\n",
      "Iteration 24, loss = 0.00032926\n",
      "Iteration 25, loss = 0.00032054\n",
      "Iteration 26, loss = 0.00031340\n",
      "Iteration 27, loss = 0.00030680\n",
      "Iteration 28, loss = 0.00029517\n",
      "Iteration 29, loss = 0.00028788\n",
      "Iteration 30, loss = 0.00028255\n",
      "Iteration 31, loss = 0.00028071\n",
      "Iteration 32, loss = 0.00027490\n",
      "Iteration 33, loss = 0.00026926\n",
      "Iteration 34, loss = 0.00026732\n",
      "Iteration 35, loss = 0.00026518\n",
      "Iteration 36, loss = 0.00026315\n",
      "Iteration 37, loss = 0.00025947\n",
      "Iteration 38, loss = 0.00025920\n",
      "Iteration 39, loss = 0.00025575\n",
      "Iteration 40, loss = 0.00025444\n",
      "Iteration 41, loss = 0.00024985\n",
      "Iteration 42, loss = 0.00025296\n",
      "Iteration 43, loss = 0.00024699\n",
      "Iteration 44, loss = 0.00024789\n",
      "Iteration 45, loss = 0.00024452\n",
      "Iteration 46, loss = 0.00024641\n",
      "Iteration 47, loss = 0.00024340\n",
      "Iteration 48, loss = 0.00024197\n",
      "Iteration 49, loss = 0.00024063\n",
      "Iteration 50, loss = 0.00024067\n",
      "Iteration 51, loss = 0.00024017\n",
      "Iteration 52, loss = 0.00023859\n",
      "Iteration 53, loss = 0.00023569\n",
      "Iteration 54, loss = 0.00023494\n",
      "Iteration 55, loss = 0.00024034\n",
      "Iteration 56, loss = 0.00023912\n",
      "Iteration 57, loss = 0.00023396\n",
      "Iteration 58, loss = 0.00023319\n",
      "Iteration 59, loss = 0.00023448\n",
      "Iteration 60, loss = 0.00023181\n",
      "Iteration 61, loss = 0.00023513\n",
      "Iteration 62, loss = 0.00023001\n",
      "Iteration 63, loss = 0.00022915\n",
      "Iteration 64, loss = 0.00023673\n",
      "Iteration 65, loss = 0.00023604\n",
      "Iteration 66, loss = 0.00022841\n",
      "Iteration 67, loss = 0.00022844\n",
      "Iteration 68, loss = 0.00022725\n",
      "Iteration 69, loss = 0.00022608\n",
      "Iteration 70, loss = 0.00022574\n",
      "Iteration 71, loss = 0.00022268\n",
      "Iteration 72, loss = 0.00022302\n",
      "Iteration 73, loss = 0.00022210\n",
      "Iteration 74, loss = 0.00022079\n",
      "Iteration 75, loss = 0.00022310\n",
      "Iteration 76, loss = 0.00022818\n",
      "Iteration 77, loss = 0.00022439\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.22567263\n",
      "Iteration 2, loss = 0.16078310\n",
      "Iteration 3, loss = 0.12836475\n",
      "Iteration 4, loss = 0.10035230\n",
      "Iteration 5, loss = 0.06981822\n",
      "Iteration 6, loss = 0.04580089\n",
      "Iteration 7, loss = 0.03303469\n",
      "Iteration 8, loss = 0.02615699\n",
      "Iteration 9, loss = 0.02033910\n",
      "Iteration 10, loss = 0.01515986\n",
      "Iteration 11, loss = 0.01082932\n",
      "Iteration 12, loss = 0.00765047\n",
      "Iteration 13, loss = 0.00554061\n",
      "Iteration 14, loss = 0.00419286\n",
      "Iteration 15, loss = 0.00332714\n",
      "Iteration 16, loss = 0.00276865\n",
      "Iteration 17, loss = 0.00241337\n",
      "Iteration 18, loss = 0.00217620\n",
      "Iteration 19, loss = 0.00202287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20, loss = 0.00191598\n",
      "Iteration 21, loss = 0.00183890\n",
      "Iteration 22, loss = 0.00176535\n",
      "Iteration 23, loss = 0.00171556\n",
      "Iteration 24, loss = 0.00168632\n",
      "Iteration 25, loss = 0.00167250\n",
      "Iteration 26, loss = 0.00165130\n",
      "Iteration 27, loss = 0.00164398\n",
      "Iteration 28, loss = 0.00163175\n",
      "Iteration 29, loss = 0.00162314\n",
      "Iteration 30, loss = 0.00161782\n",
      "Iteration 31, loss = 0.00161143\n",
      "Iteration 32, loss = 0.00160891\n",
      "Iteration 33, loss = 0.00160284\n",
      "Iteration 34, loss = 0.00159693\n",
      "Iteration 35, loss = 0.00159330\n",
      "Iteration 36, loss = 0.00159019\n",
      "Iteration 37, loss = 0.00158597\n",
      "Iteration 38, loss = 0.00158096\n",
      "Iteration 39, loss = 0.00157645\n",
      "Iteration 40, loss = 0.00157230\n",
      "Iteration 41, loss = 0.00156831\n",
      "Iteration 42, loss = 0.00156522\n",
      "Iteration 43, loss = 0.00155970\n",
      "Iteration 44, loss = 0.00155455\n",
      "Iteration 45, loss = 0.00155130\n",
      "Iteration 46, loss = 0.00154613\n",
      "Iteration 47, loss = 0.00153877\n",
      "Iteration 48, loss = 0.00153560\n",
      "Iteration 49, loss = 0.00153086\n",
      "Iteration 50, loss = 0.00152522\n",
      "Iteration 51, loss = 0.00152395\n",
      "Iteration 52, loss = 0.00151594\n",
      "Iteration 53, loss = 0.00151014\n",
      "Iteration 54, loss = 0.00150385\n",
      "Iteration 55, loss = 0.00149992\n",
      "Iteration 56, loss = 0.00149230\n",
      "Iteration 57, loss = 0.00148818\n",
      "Iteration 58, loss = 0.00148211\n",
      "Iteration 59, loss = 0.00147871\n",
      "Iteration 60, loss = 0.00147313\n",
      "Iteration 61, loss = 0.00147016\n",
      "Iteration 62, loss = 0.00145851\n",
      "Iteration 63, loss = 0.00145460\n",
      "Iteration 64, loss = 0.00145047\n",
      "Iteration 65, loss = 0.00144675\n",
      "Iteration 66, loss = 0.00144097\n",
      "Iteration 67, loss = 0.00143047\n",
      "Iteration 68, loss = 0.00142551\n",
      "Iteration 69, loss = 0.00142140\n",
      "Iteration 70, loss = 0.00141935\n",
      "Iteration 71, loss = 0.00141259\n",
      "Iteration 72, loss = 0.00140259\n",
      "Iteration 73, loss = 0.00140033\n",
      "Iteration 74, loss = 0.00139182\n",
      "Iteration 75, loss = 0.00139149\n",
      "Iteration 76, loss = 0.00138171\n",
      "Iteration 77, loss = 0.00137896\n",
      "Iteration 78, loss = 0.00137145\n",
      "Iteration 79, loss = 0.00136746\n",
      "Iteration 80, loss = 0.00136166\n",
      "Iteration 81, loss = 0.00135843\n",
      "Iteration 82, loss = 0.00135401\n",
      "Iteration 83, loss = 0.00134932\n",
      "Iteration 84, loss = 0.00134262\n",
      "Iteration 85, loss = 0.00133466\n",
      "Iteration 86, loss = 0.00132947\n",
      "Iteration 87, loss = 0.00132505\n",
      "Iteration 88, loss = 0.00132253\n",
      "Iteration 89, loss = 0.00131590\n",
      "Iteration 90, loss = 0.00131184\n",
      "Iteration 91, loss = 0.00130743\n",
      "Iteration 92, loss = 0.00130813\n",
      "Iteration 93, loss = 0.00129853\n",
      "Iteration 94, loss = 0.00129372\n",
      "Iteration 95, loss = 0.00129454\n",
      "Iteration 96, loss = 0.00128923\n",
      "Iteration 97, loss = 0.00128550\n",
      "Iteration 98, loss = 0.00127606\n",
      "Iteration 99, loss = 0.00127544\n",
      "Iteration 100, loss = 0.00127027\n",
      "Iteration 101, loss = 0.00126830\n",
      "Iteration 102, loss = 0.00126078\n",
      "Iteration 103, loss = 0.00125554\n",
      "Iteration 104, loss = 0.00125646\n",
      "Iteration 105, loss = 0.00125406\n",
      "Iteration 106, loss = 0.00124635\n",
      "Iteration 107, loss = 0.00124527\n",
      "Iteration 108, loss = 0.00124060\n",
      "Iteration 109, loss = 0.00123459\n",
      "Iteration 110, loss = 0.00123445\n",
      "Iteration 111, loss = 0.00122807\n",
      "Iteration 112, loss = 0.00123120\n",
      "Iteration 113, loss = 0.00122459\n",
      "Iteration 114, loss = 0.00122204\n",
      "Iteration 115, loss = 0.00121789\n",
      "Iteration 116, loss = 0.00121233\n",
      "Iteration 117, loss = 0.00121027\n",
      "Iteration 118, loss = 0.00120774\n",
      "Iteration 119, loss = 0.00120265\n",
      "Iteration 120, loss = 0.00119847\n",
      "Iteration 121, loss = 0.00120081\n",
      "Iteration 122, loss = 0.00119712\n",
      "Iteration 123, loss = 0.00119243\n",
      "Iteration 124, loss = 0.00118790\n",
      "Iteration 125, loss = 0.00118646\n",
      "Iteration 126, loss = 0.00118905\n",
      "Iteration 127, loss = 0.00118310\n",
      "Iteration 128, loss = 0.00118310\n",
      "Iteration 129, loss = 0.00117917\n",
      "Iteration 130, loss = 0.00117497\n",
      "Iteration 131, loss = 0.00117101\n",
      "Iteration 132, loss = 0.00116834\n",
      "Iteration 133, loss = 0.00116944\n",
      "Iteration 134, loss = 0.00116716\n",
      "Iteration 135, loss = 0.00116170\n",
      "Iteration 136, loss = 0.00116076\n",
      "Iteration 137, loss = 0.00115535\n",
      "Iteration 138, loss = 0.00115395\n",
      "Iteration 139, loss = 0.00115413\n",
      "Iteration 140, loss = 0.00115203\n",
      "Iteration 141, loss = 0.00114986\n",
      "Iteration 142, loss = 0.00114657\n",
      "Iteration 143, loss = 0.00114519\n",
      "Iteration 144, loss = 0.00114278\n",
      "Iteration 145, loss = 0.00114244\n",
      "Iteration 146, loss = 0.00113926\n",
      "Iteration 147, loss = 0.00113740\n",
      "Iteration 148, loss = 0.00113594\n",
      "Iteration 149, loss = 0.00113190\n",
      "Iteration 150, loss = 0.00112915\n",
      "Iteration 151, loss = 0.00112818\n",
      "Iteration 152, loss = 0.00112884\n",
      "Iteration 153, loss = 0.00112713\n",
      "Iteration 154, loss = 0.00112375\n",
      "Iteration 155, loss = 0.00112044\n",
      "Iteration 156, loss = 0.00111916\n",
      "Iteration 157, loss = 0.00111920\n",
      "Iteration 158, loss = 0.00111787\n",
      "Iteration 159, loss = 0.00111526\n",
      "Iteration 160, loss = 0.00111161\n",
      "Iteration 161, loss = 0.00111096\n",
      "Iteration 162, loss = 0.00110998\n",
      "Iteration 163, loss = 0.00110856\n",
      "Iteration 164, loss = 0.00110674\n",
      "Iteration 165, loss = 0.00110469\n",
      "Iteration 166, loss = 0.00110319\n",
      "Iteration 167, loss = 0.00110223\n",
      "Iteration 168, loss = 0.00109985\n",
      "Iteration 169, loss = 0.00109806\n",
      "Iteration 170, loss = 0.00109903\n",
      "Iteration 171, loss = 0.00109446\n",
      "Iteration 172, loss = 0.00109527\n",
      "Iteration 173, loss = 0.00109651\n",
      "Iteration 174, loss = 0.00109160\n",
      "Iteration 175, loss = 0.00108982\n",
      "Iteration 176, loss = 0.00108989\n",
      "Iteration 177, loss = 0.00108886\n",
      "Iteration 178, loss = 0.00108515\n",
      "Iteration 179, loss = 0.00108461\n",
      "Iteration 180, loss = 0.00108155\n",
      "Iteration 181, loss = 0.00108858\n",
      "Iteration 182, loss = 0.00108072\n",
      "Iteration 183, loss = 0.00108072\n",
      "Iteration 184, loss = 0.00107777\n",
      "Iteration 185, loss = 0.00107882\n",
      "Iteration 186, loss = 0.00107607\n",
      "Iteration 187, loss = 0.00107404\n",
      "Iteration 188, loss = 0.00107286\n",
      "Iteration 189, loss = 0.00107598\n",
      "Iteration 190, loss = 0.00107796\n",
      "Iteration 191, loss = 0.00106870\n",
      "Iteration 192, loss = 0.00106948\n",
      "Iteration 193, loss = 0.00106850\n",
      "Iteration 194, loss = 0.00106695\n",
      "Iteration 195, loss = 0.00106558\n",
      "Iteration 196, loss = 0.00106356\n",
      "Iteration 197, loss = 0.00106634\n",
      "Iteration 198, loss = 0.00106381\n",
      "Iteration 199, loss = 0.00106088\n",
      "Iteration 200, loss = 0.00106166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zuza/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.21333687\n",
      "Iteration 2, loss = 0.15181383\n",
      "Iteration 3, loss = 0.11258270\n",
      "Iteration 4, loss = 0.07474683\n",
      "Iteration 5, loss = 0.04387544\n",
      "Iteration 6, loss = 0.02877417\n",
      "Iteration 7, loss = 0.01972089\n",
      "Iteration 8, loss = 0.01333140\n",
      "Iteration 9, loss = 0.00917433\n",
      "Iteration 10, loss = 0.00657075\n",
      "Iteration 11, loss = 0.00500844\n",
      "Iteration 12, loss = 0.00410751\n",
      "Iteration 13, loss = 0.00350047\n",
      "Iteration 14, loss = 0.00308508\n",
      "Iteration 15, loss = 0.00280019\n",
      "Iteration 16, loss = 0.00259162\n",
      "Iteration 17, loss = 0.00243521\n",
      "Iteration 18, loss = 0.00232077\n",
      "Iteration 19, loss = 0.00223043\n",
      "Iteration 20, loss = 0.00216242\n",
      "Iteration 21, loss = 0.00210924\n",
      "Iteration 22, loss = 0.00205954\n",
      "Iteration 23, loss = 0.00202308\n",
      "Iteration 24, loss = 0.00199277\n",
      "Iteration 25, loss = 0.00196251\n",
      "Iteration 26, loss = 0.00193908\n",
      "Iteration 27, loss = 0.00191798\n",
      "Iteration 28, loss = 0.00190027\n",
      "Iteration 29, loss = 0.00187788\n",
      "Iteration 30, loss = 0.00186040\n",
      "Iteration 31, loss = 0.00184557\n",
      "Iteration 32, loss = 0.00182798\n",
      "Iteration 33, loss = 0.00181550\n",
      "Iteration 34, loss = 0.00180123\n",
      "Iteration 35, loss = 0.00178755\n",
      "Iteration 36, loss = 0.00177210\n",
      "Iteration 37, loss = 0.00175916\n",
      "Iteration 38, loss = 0.00174657\n",
      "Iteration 39, loss = 0.00173563\n",
      "Iteration 40, loss = 0.00172036\n",
      "Iteration 41, loss = 0.00170657\n",
      "Iteration 42, loss = 0.00169462\n",
      "Iteration 43, loss = 0.00168350\n",
      "Iteration 44, loss = 0.00167289\n",
      "Iteration 45, loss = 0.00166127\n",
      "Iteration 46, loss = 0.00165041\n",
      "Iteration 47, loss = 0.00163498\n",
      "Iteration 48, loss = 0.00162335\n",
      "Iteration 49, loss = 0.00161477\n",
      "Iteration 50, loss = 0.00160338\n",
      "Iteration 51, loss = 0.00159318\n",
      "Iteration 52, loss = 0.00158160\n",
      "Iteration 53, loss = 0.00156842\n",
      "Iteration 54, loss = 0.00156045\n",
      "Iteration 55, loss = 0.00155098\n",
      "Iteration 56, loss = 0.00153885\n",
      "Iteration 57, loss = 0.00153026\n",
      "Iteration 58, loss = 0.00151874\n",
      "Iteration 59, loss = 0.00150773\n",
      "Iteration 60, loss = 0.00149772\n",
      "Iteration 61, loss = 0.00148871\n",
      "Iteration 62, loss = 0.00148134\n",
      "Iteration 63, loss = 0.00147243\n",
      "Iteration 64, loss = 0.00146254\n",
      "Iteration 65, loss = 0.00145008\n",
      "Iteration 66, loss = 0.00144385\n",
      "Iteration 67, loss = 0.00143410\n",
      "Iteration 68, loss = 0.00142765\n",
      "Iteration 69, loss = 0.00141815\n",
      "Iteration 70, loss = 0.00141064\n",
      "Iteration 71, loss = 0.00140158\n",
      "Iteration 72, loss = 0.00139262\n",
      "Iteration 73, loss = 0.00138707\n",
      "Iteration 74, loss = 0.00137610\n",
      "Iteration 75, loss = 0.00137041\n",
      "Iteration 76, loss = 0.00135942\n",
      "Iteration 77, loss = 0.00135357\n",
      "Iteration 78, loss = 0.00134863\n",
      "Iteration 79, loss = 0.00133913\n",
      "Iteration 80, loss = 0.00133541\n",
      "Iteration 81, loss = 0.00132363\n",
      "Iteration 82, loss = 0.00131931\n",
      "Iteration 83, loss = 0.00131337\n",
      "Iteration 84, loss = 0.00130600\n",
      "Iteration 85, loss = 0.00129792\n",
      "Iteration 86, loss = 0.00129381\n",
      "Iteration 87, loss = 0.00128655\n",
      "Iteration 88, loss = 0.00127783\n",
      "Iteration 89, loss = 0.00127085\n",
      "Iteration 90, loss = 0.00126466\n",
      "Iteration 91, loss = 0.00125794\n",
      "Iteration 92, loss = 0.00125276\n",
      "Iteration 93, loss = 0.00124642\n",
      "Iteration 94, loss = 0.00123906\n",
      "Iteration 95, loss = 0.00123773\n",
      "Iteration 96, loss = 0.00122923\n",
      "Iteration 97, loss = 0.00122219\n",
      "Iteration 98, loss = 0.00121856\n",
      "Iteration 99, loss = 0.00121421\n",
      "Iteration 100, loss = 0.00120998\n",
      "Iteration 101, loss = 0.00120138\n",
      "Iteration 102, loss = 0.00119624\n",
      "Iteration 103, loss = 0.00119483\n",
      "Iteration 104, loss = 0.00118875\n",
      "Iteration 105, loss = 0.00118457\n",
      "Iteration 106, loss = 0.00118184\n",
      "Iteration 107, loss = 0.00117652\n",
      "Iteration 108, loss = 0.00117049\n",
      "Iteration 109, loss = 0.00116531\n",
      "Iteration 110, loss = 0.00116318\n",
      "Iteration 111, loss = 0.00116398\n",
      "Iteration 112, loss = 0.00115608\n",
      "Iteration 113, loss = 0.00114976\n",
      "Iteration 114, loss = 0.00114751\n",
      "Iteration 115, loss = 0.00114315\n",
      "Iteration 116, loss = 0.00114096\n",
      "Iteration 117, loss = 0.00113588\n",
      "Iteration 118, loss = 0.00113161\n",
      "Iteration 119, loss = 0.00112882\n",
      "Iteration 120, loss = 0.00112594\n",
      "Iteration 121, loss = 0.00112403\n",
      "Iteration 122, loss = 0.00111903\n",
      "Iteration 123, loss = 0.00111570\n",
      "Iteration 124, loss = 0.00111269\n",
      "Iteration 125, loss = 0.00111474\n",
      "Iteration 126, loss = 0.00110583\n",
      "Iteration 127, loss = 0.00110460\n",
      "Iteration 128, loss = 0.00110029\n",
      "Iteration 129, loss = 0.00109912\n",
      "Iteration 130, loss = 0.00109805\n",
      "Iteration 131, loss = 0.00109507\n",
      "Iteration 132, loss = 0.00108962\n",
      "Iteration 133, loss = 0.00108749\n",
      "Iteration 134, loss = 0.00108625\n",
      "Iteration 135, loss = 0.00108473\n",
      "Iteration 136, loss = 0.00108152\n",
      "Iteration 137, loss = 0.00107878\n",
      "Iteration 138, loss = 0.00107698\n",
      "Iteration 139, loss = 0.00107300\n",
      "Iteration 140, loss = 0.00107113\n",
      "Iteration 141, loss = 0.00107330\n",
      "Iteration 142, loss = 0.00106621\n",
      "Iteration 143, loss = 0.00106610\n",
      "Iteration 144, loss = 0.00106414\n",
      "Iteration 145, loss = 0.00106307\n",
      "Iteration 146, loss = 0.00105916\n",
      "Iteration 147, loss = 0.00105869\n",
      "Iteration 148, loss = 0.00105601\n",
      "Iteration 149, loss = 0.00105320\n",
      "Iteration 150, loss = 0.00105101\n",
      "Iteration 151, loss = 0.00105089\n",
      "Iteration 152, loss = 0.00105083\n",
      "Iteration 153, loss = 0.00104723\n",
      "Iteration 154, loss = 0.00104416\n",
      "Iteration 155, loss = 0.00104242\n",
      "Iteration 156, loss = 0.00104163\n",
      "Iteration 157, loss = 0.00104008\n",
      "Iteration 158, loss = 0.00103783\n",
      "Iteration 159, loss = 0.00103916\n",
      "Iteration 160, loss = 0.00103507\n",
      "Iteration 161, loss = 0.00103631\n",
      "Iteration 162, loss = 0.00103084\n",
      "Iteration 163, loss = 0.00103020\n",
      "Iteration 164, loss = 0.00102857\n",
      "Iteration 165, loss = 0.00102757\n",
      "Iteration 166, loss = 0.00102696\n",
      "Iteration 167, loss = 0.00102537\n",
      "Iteration 168, loss = 0.00102544\n",
      "Iteration 169, loss = 0.00102770\n",
      "Iteration 170, loss = 0.00101862\n",
      "Iteration 171, loss = 0.00102020\n",
      "Iteration 172, loss = 0.00102000\n",
      "Iteration 173, loss = 0.00101620\n",
      "Iteration 174, loss = 0.00101481\n",
      "Iteration 175, loss = 0.00101415\n",
      "Iteration 176, loss = 0.00101405\n",
      "Iteration 177, loss = 0.00101575\n",
      "Iteration 178, loss = 0.00101656\n",
      "Iteration 179, loss = 0.00100935\n",
      "Iteration 180, loss = 0.00100810\n",
      "Iteration 181, loss = 0.00100946\n",
      "Iteration 182, loss = 0.00101048\n",
      "Iteration 183, loss = 0.00100738\n",
      "Iteration 184, loss = 0.00100382\n",
      "Iteration 185, loss = 0.00100418\n",
      "Iteration 186, loss = 0.00100417\n",
      "Iteration 187, loss = 0.00100066\n",
      "Iteration 188, loss = 0.00100378\n",
      "Iteration 189, loss = 0.00099962\n",
      "Iteration 190, loss = 0.00099883\n",
      "Iteration 191, loss = 0.00099884\n",
      "Iteration 192, loss = 0.00099715\n",
      "Iteration 193, loss = 0.00099511\n",
      "Iteration 194, loss = 0.00099457\n",
      "Iteration 195, loss = 0.00099409\n",
      "Iteration 196, loss = 0.00099189\n",
      "Iteration 197, loss = 0.00099181\n",
      "Iteration 198, loss = 0.00099011\n",
      "Iteration 199, loss = 0.00099064\n",
      "Iteration 200, loss = 0.00099082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zuza/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.20803149\n",
      "Iteration 2, loss = 0.13296621\n",
      "Iteration 3, loss = 0.08113563\n",
      "Iteration 4, loss = 0.04794883\n",
      "Iteration 5, loss = 0.03216581\n",
      "Iteration 6, loss = 0.02383249\n",
      "Iteration 7, loss = 0.01811790\n",
      "Iteration 8, loss = 0.01319316\n",
      "Iteration 9, loss = 0.00928494\n",
      "Iteration 10, loss = 0.00671397\n",
      "Iteration 11, loss = 0.00509001\n",
      "Iteration 12, loss = 0.00409786\n",
      "Iteration 13, loss = 0.00343920\n",
      "Iteration 14, loss = 0.00298803\n",
      "Iteration 15, loss = 0.00268095\n",
      "Iteration 16, loss = 0.00245098\n",
      "Iteration 17, loss = 0.00227915\n",
      "Iteration 18, loss = 0.00214759\n",
      "Iteration 19, loss = 0.00204529\n",
      "Iteration 20, loss = 0.00197047\n",
      "Iteration 21, loss = 0.00190846\n",
      "Iteration 22, loss = 0.00185677\n",
      "Iteration 23, loss = 0.00182128\n",
      "Iteration 24, loss = 0.00178026\n",
      "Iteration 25, loss = 0.00175146\n",
      "Iteration 26, loss = 0.00172538\n",
      "Iteration 27, loss = 0.00170105\n",
      "Iteration 28, loss = 0.00168099\n",
      "Iteration 29, loss = 0.00166255\n",
      "Iteration 30, loss = 0.00165064\n",
      "Iteration 31, loss = 0.00162993\n",
      "Iteration 32, loss = 0.00161499\n",
      "Iteration 33, loss = 0.00160111\n",
      "Iteration 34, loss = 0.00158931\n",
      "Iteration 35, loss = 0.00157741\n",
      "Iteration 36, loss = 0.00156427\n",
      "Iteration 37, loss = 0.00155267\n",
      "Iteration 38, loss = 0.00154211\n",
      "Iteration 39, loss = 0.00153077\n",
      "Iteration 40, loss = 0.00151938\n",
      "Iteration 41, loss = 0.00151020\n",
      "Iteration 42, loss = 0.00150001\n",
      "Iteration 43, loss = 0.00149082\n",
      "Iteration 44, loss = 0.00148090\n",
      "Iteration 45, loss = 0.00146882\n",
      "Iteration 46, loss = 0.00146040\n",
      "Iteration 47, loss = 0.00145159\n",
      "Iteration 48, loss = 0.00144330\n",
      "Iteration 49, loss = 0.00143545\n",
      "Iteration 50, loss = 0.00142571\n",
      "Iteration 51, loss = 0.00141924\n",
      "Iteration 52, loss = 0.00141037\n",
      "Iteration 53, loss = 0.00140169\n",
      "Iteration 54, loss = 0.00139483\n",
      "Iteration 55, loss = 0.00138614\n",
      "Iteration 56, loss = 0.00137855\n",
      "Iteration 57, loss = 0.00137051\n",
      "Iteration 58, loss = 0.00136170\n",
      "Iteration 59, loss = 0.00135552\n",
      "Iteration 60, loss = 0.00134661\n",
      "Iteration 61, loss = 0.00133976\n",
      "Iteration 62, loss = 0.00133416\n",
      "Iteration 63, loss = 0.00132528\n",
      "Iteration 64, loss = 0.00131788\n",
      "Iteration 65, loss = 0.00131037\n",
      "Iteration 66, loss = 0.00130244\n",
      "Iteration 67, loss = 0.00129541\n",
      "Iteration 68, loss = 0.00128961\n",
      "Iteration 69, loss = 0.00128112\n",
      "Iteration 70, loss = 0.00127601\n",
      "Iteration 71, loss = 0.00126883\n",
      "Iteration 72, loss = 0.00126071\n",
      "Iteration 73, loss = 0.00125390\n",
      "Iteration 74, loss = 0.00124998\n",
      "Iteration 75, loss = 0.00124121\n",
      "Iteration 76, loss = 0.00123874\n",
      "Iteration 77, loss = 0.00123108\n",
      "Iteration 78, loss = 0.00122445\n",
      "Iteration 79, loss = 0.00121730\n",
      "Iteration 80, loss = 0.00121358\n",
      "Iteration 81, loss = 0.00120457\n",
      "Iteration 82, loss = 0.00119904\n",
      "Iteration 83, loss = 0.00119507\n",
      "Iteration 84, loss = 0.00118979\n",
      "Iteration 85, loss = 0.00118428\n",
      "Iteration 86, loss = 0.00117868\n",
      "Iteration 87, loss = 0.00117269\n",
      "Iteration 88, loss = 0.00116995\n",
      "Iteration 89, loss = 0.00116300\n",
      "Iteration 90, loss = 0.00115786\n",
      "Iteration 91, loss = 0.00115397\n",
      "Iteration 92, loss = 0.00115099\n",
      "Iteration 93, loss = 0.00114464\n",
      "Iteration 94, loss = 0.00113951\n",
      "Iteration 95, loss = 0.00113440\n",
      "Iteration 96, loss = 0.00113445\n",
      "Iteration 97, loss = 0.00113060\n",
      "Iteration 98, loss = 0.00112248\n",
      "Iteration 99, loss = 0.00111902\n",
      "Iteration 100, loss = 0.00111613\n",
      "Iteration 101, loss = 0.00111176\n",
      "Iteration 102, loss = 0.00110759\n",
      "Iteration 103, loss = 0.00110436\n",
      "Iteration 104, loss = 0.00110266\n",
      "Iteration 105, loss = 0.00109971\n",
      "Iteration 106, loss = 0.00109467\n",
      "Iteration 107, loss = 0.00109059\n",
      "Iteration 108, loss = 0.00108846\n",
      "Iteration 109, loss = 0.00108548\n",
      "Iteration 110, loss = 0.00108223\n",
      "Iteration 111, loss = 0.00107845\n",
      "Iteration 112, loss = 0.00107920\n",
      "Iteration 113, loss = 0.00107384\n",
      "Iteration 114, loss = 0.00107144\n",
      "Iteration 115, loss = 0.00106920\n",
      "Iteration 116, loss = 0.00106779\n",
      "Iteration 117, loss = 0.00106410\n",
      "Iteration 118, loss = 0.00106142\n",
      "Iteration 119, loss = 0.00105757\n",
      "Iteration 120, loss = 0.00105652\n",
      "Iteration 121, loss = 0.00105487\n",
      "Iteration 122, loss = 0.00105181\n",
      "Iteration 123, loss = 0.00104831\n",
      "Iteration 124, loss = 0.00104808\n",
      "Iteration 125, loss = 0.00104519\n",
      "Iteration 126, loss = 0.00104216\n",
      "Iteration 127, loss = 0.00103985\n",
      "Iteration 128, loss = 0.00103833\n",
      "Iteration 129, loss = 0.00103590\n",
      "Iteration 130, loss = 0.00103432\n",
      "Iteration 131, loss = 0.00103553\n",
      "Iteration 132, loss = 0.00103043\n",
      "Iteration 133, loss = 0.00102909\n",
      "Iteration 134, loss = 0.00102804\n",
      "Iteration 135, loss = 0.00102540\n",
      "Iteration 136, loss = 0.00102638\n",
      "Iteration 137, loss = 0.00102336\n",
      "Iteration 138, loss = 0.00102140\n",
      "Iteration 139, loss = 0.00101953\n",
      "Iteration 140, loss = 0.00101876\n",
      "Iteration 141, loss = 0.00101522\n",
      "Iteration 142, loss = 0.00101363\n",
      "Iteration 143, loss = 0.00101368\n",
      "Iteration 144, loss = 0.00101258\n",
      "Iteration 145, loss = 0.00101029\n",
      "Iteration 146, loss = 0.00101095\n",
      "Iteration 147, loss = 0.00100712\n",
      "Iteration 148, loss = 0.00100568\n",
      "Iteration 149, loss = 0.00100439\n",
      "Iteration 150, loss = 0.00100396\n",
      "Iteration 151, loss = 0.00100228\n",
      "Iteration 152, loss = 0.00100116\n",
      "Iteration 153, loss = 0.00099967\n",
      "Iteration 154, loss = 0.00099840\n",
      "Iteration 155, loss = 0.00099604\n",
      "Iteration 156, loss = 0.00099429\n",
      "Iteration 157, loss = 0.00099604\n",
      "Iteration 158, loss = 0.00099339\n",
      "Iteration 159, loss = 0.00099224\n",
      "Iteration 160, loss = 0.00099109\n",
      "Iteration 161, loss = 0.00099004\n",
      "Iteration 162, loss = 0.00098866\n",
      "Iteration 163, loss = 0.00098756\n",
      "Iteration 164, loss = 0.00098715\n",
      "Iteration 165, loss = 0.00098436\n",
      "Iteration 166, loss = 0.00098427\n",
      "Iteration 167, loss = 0.00098348\n",
      "Iteration 168, loss = 0.00098213\n",
      "Iteration 169, loss = 0.00098228\n",
      "Iteration 170, loss = 0.00098120\n",
      "Iteration 171, loss = 0.00097968\n",
      "Iteration 172, loss = 0.00097746\n",
      "Iteration 173, loss = 0.00097489\n",
      "Iteration 174, loss = 0.00097308\n",
      "Iteration 175, loss = 0.00097328\n",
      "Iteration 176, loss = 0.00097348\n",
      "Iteration 177, loss = 0.00097153\n",
      "Iteration 178, loss = 0.00096950\n",
      "Iteration 179, loss = 0.00096951\n",
      "Iteration 180, loss = 0.00096844\n",
      "Iteration 181, loss = 0.00096698\n",
      "Iteration 182, loss = 0.00096785\n",
      "Iteration 183, loss = 0.00096579\n",
      "Iteration 184, loss = 0.00096460\n",
      "Iteration 185, loss = 0.00096354\n",
      "Iteration 186, loss = 0.00096290\n",
      "Iteration 187, loss = 0.00096257\n",
      "Iteration 188, loss = 0.00096215\n",
      "Iteration 189, loss = 0.00096020\n",
      "Iteration 190, loss = 0.00095937\n",
      "Iteration 191, loss = 0.00095836\n",
      "Iteration 192, loss = 0.00095905\n",
      "Iteration 193, loss = 0.00095649\n",
      "Iteration 194, loss = 0.00095580\n",
      "Iteration 195, loss = 0.00095625\n",
      "Iteration 196, loss = 0.00095459\n",
      "Iteration 197, loss = 0.00095479\n",
      "Iteration 198, loss = 0.00095397\n",
      "Iteration 199, loss = 0.00095314\n",
      "Iteration 200, loss = 0.00095097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zuza/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.19442538\n",
      "Iteration 2, loss = 0.12056656\n",
      "Iteration 3, loss = 0.07156676\n",
      "Iteration 4, loss = 0.03540988\n",
      "Iteration 5, loss = 0.01966448\n",
      "Iteration 6, loss = 0.01208485\n",
      "Iteration 7, loss = 0.00796288\n",
      "Iteration 8, loss = 0.00572245\n",
      "Iteration 9, loss = 0.00447558\n",
      "Iteration 10, loss = 0.00363990\n",
      "Iteration 11, loss = 0.00302396\n",
      "Iteration 12, loss = 0.00264209\n",
      "Iteration 13, loss = 0.00245214\n",
      "Iteration 14, loss = 0.00233584\n",
      "Iteration 15, loss = 0.00227048\n",
      "Iteration 16, loss = 0.00222611\n",
      "Iteration 17, loss = 0.00219104\n",
      "Iteration 18, loss = 0.00216333\n",
      "Iteration 19, loss = 0.00213675\n",
      "Iteration 20, loss = 0.00211596\n",
      "Iteration 21, loss = 0.00209251\n",
      "Iteration 22, loss = 0.00207261\n",
      "Iteration 23, loss = 0.00205361\n",
      "Iteration 24, loss = 0.00203491\n",
      "Iteration 25, loss = 0.00201518\n",
      "Iteration 26, loss = 0.00199731\n",
      "Iteration 27, loss = 0.00197872\n",
      "Iteration 28, loss = 0.00196067\n",
      "Iteration 29, loss = 0.00194440\n",
      "Iteration 30, loss = 0.00192653\n",
      "Iteration 31, loss = 0.00190665\n",
      "Iteration 32, loss = 0.00188913\n",
      "Iteration 33, loss = 0.00187142\n",
      "Iteration 34, loss = 0.00185662\n",
      "Iteration 35, loss = 0.00183862\n",
      "Iteration 36, loss = 0.00182055\n",
      "Iteration 37, loss = 0.00180344\n",
      "Iteration 38, loss = 0.00178748\n",
      "Iteration 39, loss = 0.00177127\n",
      "Iteration 40, loss = 0.00175429\n",
      "Iteration 41, loss = 0.00173783\n",
      "Iteration 42, loss = 0.00172144\n",
      "Iteration 43, loss = 0.00170485\n",
      "Iteration 44, loss = 0.00168741\n",
      "Iteration 45, loss = 0.00167395\n",
      "Iteration 46, loss = 0.00165602\n",
      "Iteration 47, loss = 0.00164127\n",
      "Iteration 48, loss = 0.00162564\n",
      "Iteration 49, loss = 0.00160936\n",
      "Iteration 50, loss = 0.00159594\n",
      "Iteration 51, loss = 0.00157934\n",
      "Iteration 52, loss = 0.00156670\n",
      "Iteration 53, loss = 0.00155028\n",
      "Iteration 54, loss = 0.00153668\n",
      "Iteration 55, loss = 0.00152242\n",
      "Iteration 56, loss = 0.00150805\n",
      "Iteration 57, loss = 0.00149326\n",
      "Iteration 58, loss = 0.00148127\n",
      "Iteration 59, loss = 0.00146737\n",
      "Iteration 60, loss = 0.00145370\n",
      "Iteration 61, loss = 0.00144083\n",
      "Iteration 62, loss = 0.00143025\n",
      "Iteration 63, loss = 0.00141387\n",
      "Iteration 64, loss = 0.00140366\n",
      "Iteration 65, loss = 0.00139052\n",
      "Iteration 66, loss = 0.00137914\n",
      "Iteration 67, loss = 0.00137145\n",
      "Iteration 68, loss = 0.00135587\n",
      "Iteration 69, loss = 0.00134971\n",
      "Iteration 70, loss = 0.00133303\n",
      "Iteration 71, loss = 0.00132610\n",
      "Iteration 72, loss = 0.00131351\n",
      "Iteration 73, loss = 0.00130149\n",
      "Iteration 74, loss = 0.00129306\n",
      "Iteration 75, loss = 0.00128310\n",
      "Iteration 76, loss = 0.00127324\n",
      "Iteration 77, loss = 0.00126442\n",
      "Iteration 78, loss = 0.00125848\n",
      "Iteration 79, loss = 0.00125235\n",
      "Iteration 80, loss = 0.00123985\n",
      "Iteration 81, loss = 0.00123097\n",
      "Iteration 82, loss = 0.00122614\n",
      "Iteration 83, loss = 0.00121403\n",
      "Iteration 84, loss = 0.00120956\n",
      "Iteration 85, loss = 0.00120068\n",
      "Iteration 86, loss = 0.00119562\n",
      "Iteration 87, loss = 0.00118936\n",
      "Iteration 88, loss = 0.00118320\n",
      "Iteration 89, loss = 0.00117794\n",
      "Iteration 90, loss = 0.00117211\n",
      "Iteration 91, loss = 0.00116245\n",
      "Iteration 92, loss = 0.00115775\n",
      "Iteration 93, loss = 0.00115384\n",
      "Iteration 94, loss = 0.00114433\n",
      "Iteration 95, loss = 0.00113888\n",
      "Iteration 96, loss = 0.00113430\n",
      "Iteration 97, loss = 0.00112928\n",
      "Iteration 98, loss = 0.00112585\n",
      "Iteration 99, loss = 0.00112049\n",
      "Iteration 100, loss = 0.00111552\n",
      "Iteration 101, loss = 0.00111225\n",
      "Iteration 102, loss = 0.00110663\n",
      "Iteration 103, loss = 0.00110169\n",
      "Iteration 104, loss = 0.00110753\n",
      "Iteration 105, loss = 0.00109739\n",
      "Iteration 106, loss = 0.00109113\n",
      "Iteration 107, loss = 0.00108694\n",
      "Iteration 108, loss = 0.00108285\n",
      "Iteration 109, loss = 0.00108115\n",
      "Iteration 110, loss = 0.00107974\n",
      "Iteration 111, loss = 0.00107536\n",
      "Iteration 112, loss = 0.00107482\n",
      "Iteration 113, loss = 0.00106501\n",
      "Iteration 114, loss = 0.00106137\n",
      "Iteration 115, loss = 0.00105900\n",
      "Iteration 116, loss = 0.00105576\n",
      "Iteration 117, loss = 0.00105747\n",
      "Iteration 118, loss = 0.00105437\n",
      "Iteration 119, loss = 0.00104892\n",
      "Iteration 120, loss = 0.00105015\n",
      "Iteration 121, loss = 0.00104318\n",
      "Iteration 122, loss = 0.00104285\n",
      "Iteration 123, loss = 0.00103973\n",
      "Iteration 124, loss = 0.00103528\n",
      "Iteration 125, loss = 0.00103255\n",
      "Iteration 126, loss = 0.00103400\n",
      "Iteration 127, loss = 0.00103012\n",
      "Iteration 128, loss = 0.00102572\n",
      "Iteration 129, loss = 0.00102777\n",
      "Iteration 130, loss = 0.00102157\n",
      "Iteration 131, loss = 0.00102189\n",
      "Iteration 132, loss = 0.00102021\n",
      "Iteration 133, loss = 0.00101608\n",
      "Iteration 134, loss = 0.00101741\n",
      "Iteration 135, loss = 0.00101064\n",
      "Iteration 136, loss = 0.00100900\n",
      "Iteration 137, loss = 0.00100938\n",
      "Iteration 138, loss = 0.00100853\n",
      "Iteration 139, loss = 0.00101020\n",
      "Iteration 140, loss = 0.00100606\n",
      "Iteration 141, loss = 0.00100353\n",
      "Iteration 142, loss = 0.00100257\n",
      "Iteration 143, loss = 0.00099953\n",
      "Iteration 144, loss = 0.00099628\n",
      "Iteration 145, loss = 0.00099516\n",
      "Iteration 146, loss = 0.00099270\n",
      "Iteration 147, loss = 0.00099294\n",
      "Iteration 148, loss = 0.00099249\n",
      "Iteration 149, loss = 0.00098787\n",
      "Iteration 150, loss = 0.00099136\n",
      "Iteration 151, loss = 0.00098962\n",
      "Iteration 152, loss = 0.00098553\n",
      "Iteration 153, loss = 0.00098290\n",
      "Iteration 154, loss = 0.00098316\n",
      "Iteration 155, loss = 0.00098308\n",
      "Iteration 156, loss = 0.00098371\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.21451364\n",
      "Iteration 2, loss = 0.15159917\n",
      "Iteration 3, loss = 0.10877032\n",
      "Iteration 4, loss = 0.06500954\n",
      "Iteration 5, loss = 0.03774226\n",
      "Iteration 6, loss = 0.02652400\n",
      "Iteration 7, loss = 0.01933582\n",
      "Iteration 8, loss = 0.01293510\n",
      "Iteration 9, loss = 0.00816113\n",
      "Iteration 10, loss = 0.00534524\n",
      "Iteration 11, loss = 0.00387524\n",
      "Iteration 12, loss = 0.00312494\n",
      "Iteration 13, loss = 0.00265321\n",
      "Iteration 14, loss = 0.00237814\n",
      "Iteration 15, loss = 0.00221717\n",
      "Iteration 16, loss = 0.00211647\n",
      "Iteration 17, loss = 0.00204231\n",
      "Iteration 18, loss = 0.00198863\n",
      "Iteration 19, loss = 0.00194823\n",
      "Iteration 20, loss = 0.00191484\n",
      "Iteration 21, loss = 0.00188986\n",
      "Iteration 22, loss = 0.00186617\n",
      "Iteration 23, loss = 0.00184600\n",
      "Iteration 24, loss = 0.00182748\n",
      "Iteration 25, loss = 0.00180959\n",
      "Iteration 26, loss = 0.00178886\n",
      "Iteration 27, loss = 0.00177540\n",
      "Iteration 28, loss = 0.00176108\n",
      "Iteration 29, loss = 0.00174803\n",
      "Iteration 30, loss = 0.00173447\n",
      "Iteration 31, loss = 0.00172209\n",
      "Iteration 32, loss = 0.00171106\n",
      "Iteration 33, loss = 0.00170131\n",
      "Iteration 34, loss = 0.00168855\n",
      "Iteration 35, loss = 0.00167789\n",
      "Iteration 36, loss = 0.00166711\n",
      "Iteration 37, loss = 0.00165603\n",
      "Iteration 38, loss = 0.00164444\n",
      "Iteration 39, loss = 0.00161538\n",
      "Iteration 40, loss = 0.00160471\n",
      "Iteration 41, loss = 0.00159382\n",
      "Iteration 42, loss = 0.00158300\n",
      "Iteration 43, loss = 0.00157169\n",
      "Iteration 44, loss = 0.00156194\n",
      "Iteration 45, loss = 0.00155099\n",
      "Iteration 46, loss = 0.00153977\n",
      "Iteration 47, loss = 0.00153254\n",
      "Iteration 48, loss = 0.00152051\n",
      "Iteration 49, loss = 0.00150912\n",
      "Iteration 50, loss = 0.00150173\n",
      "Iteration 51, loss = 0.00149036\n",
      "Iteration 52, loss = 0.00147778\n",
      "Iteration 53, loss = 0.00146814\n",
      "Iteration 54, loss = 0.00145694\n",
      "Iteration 55, loss = 0.00144572\n",
      "Iteration 56, loss = 0.00143824\n",
      "Iteration 57, loss = 0.00142509\n",
      "Iteration 58, loss = 0.00141599\n",
      "Iteration 59, loss = 0.00140629\n",
      "Iteration 60, loss = 0.00139705\n",
      "Iteration 61, loss = 0.00138208\n",
      "Iteration 62, loss = 0.00137136\n",
      "Iteration 63, loss = 0.00136105\n",
      "Iteration 64, loss = 0.00135207\n",
      "Iteration 65, loss = 0.00134432\n",
      "Iteration 66, loss = 0.00133343\n",
      "Iteration 67, loss = 0.00132324\n",
      "Iteration 68, loss = 0.00131349\n",
      "Iteration 69, loss = 0.00130419\n",
      "Iteration 70, loss = 0.00129449\n",
      "Iteration 71, loss = 0.00128641\n",
      "Iteration 72, loss = 0.00127712\n",
      "Iteration 73, loss = 0.00126913\n",
      "Iteration 74, loss = 0.00126106\n",
      "Iteration 75, loss = 0.00125533\n",
      "Iteration 76, loss = 0.00124556\n",
      "Iteration 77, loss = 0.00123477\n",
      "Iteration 78, loss = 0.00122719\n",
      "Iteration 79, loss = 0.00122409\n",
      "Iteration 80, loss = 0.00121262\n",
      "Iteration 81, loss = 0.00120372\n",
      "Iteration 82, loss = 0.00119953\n",
      "Iteration 83, loss = 0.00119065\n",
      "Iteration 84, loss = 0.00118523\n",
      "Iteration 85, loss = 0.00117707\n",
      "Iteration 86, loss = 0.00116906\n",
      "Iteration 87, loss = 0.00116398\n",
      "Iteration 88, loss = 0.00116054\n",
      "Iteration 89, loss = 0.00115018\n",
      "Iteration 90, loss = 0.00114542\n",
      "Iteration 91, loss = 0.00113831\n",
      "Iteration 92, loss = 0.00113373\n",
      "Iteration 93, loss = 0.00112712\n",
      "Iteration 94, loss = 0.00112253\n",
      "Iteration 95, loss = 0.00111745\n",
      "Iteration 96, loss = 0.00111175\n",
      "Iteration 97, loss = 0.00110896\n",
      "Iteration 98, loss = 0.00110639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 99, loss = 0.00109681\n",
      "Iteration 100, loss = 0.00109226\n",
      "Iteration 101, loss = 0.00108633\n",
      "Iteration 102, loss = 0.00108118\n",
      "Iteration 103, loss = 0.00107873\n",
      "Iteration 104, loss = 0.00107565\n",
      "Iteration 105, loss = 0.00107288\n",
      "Iteration 106, loss = 0.00106774\n",
      "Iteration 107, loss = 0.00106709\n",
      "Iteration 108, loss = 0.00106127\n",
      "Iteration 109, loss = 0.00105558\n",
      "Iteration 110, loss = 0.00105120\n",
      "Iteration 111, loss = 0.00104814\n",
      "Iteration 112, loss = 0.00104426\n",
      "Iteration 113, loss = 0.00104106\n",
      "Iteration 114, loss = 0.00103804\n",
      "Iteration 115, loss = 0.00103541\n",
      "Iteration 116, loss = 0.00103254\n",
      "Iteration 117, loss = 0.00102891\n",
      "Iteration 118, loss = 0.00102679\n",
      "Iteration 119, loss = 0.00102378\n",
      "Iteration 120, loss = 0.00102193\n",
      "Iteration 121, loss = 0.00101859\n",
      "Iteration 122, loss = 0.00102039\n",
      "Iteration 123, loss = 0.00101343\n",
      "Iteration 124, loss = 0.00101336\n",
      "Iteration 125, loss = 0.00101084\n",
      "Iteration 126, loss = 0.00100823\n",
      "Iteration 127, loss = 0.00100380\n",
      "Iteration 128, loss = 0.00100186\n",
      "Iteration 129, loss = 0.00099927\n",
      "Iteration 130, loss = 0.00099873\n",
      "Iteration 131, loss = 0.00099628\n",
      "Iteration 132, loss = 0.00099329\n",
      "Iteration 133, loss = 0.00099183\n",
      "Iteration 134, loss = 0.00099072\n",
      "Iteration 135, loss = 0.00098826\n",
      "Iteration 136, loss = 0.00098716\n",
      "Iteration 137, loss = 0.00098423\n",
      "Iteration 138, loss = 0.00098223\n",
      "Iteration 139, loss = 0.00098178\n",
      "Iteration 140, loss = 0.00098022\n",
      "Iteration 141, loss = 0.00097987\n",
      "Iteration 142, loss = 0.00097719\n",
      "Iteration 143, loss = 0.00097426\n",
      "Iteration 144, loss = 0.00097567\n",
      "Iteration 145, loss = 0.00097018\n",
      "Iteration 146, loss = 0.00096920\n",
      "Iteration 147, loss = 0.00097003\n",
      "Iteration 148, loss = 0.00096596\n",
      "Iteration 149, loss = 0.00096542\n",
      "Iteration 150, loss = 0.00096463\n",
      "Iteration 151, loss = 0.00096339\n",
      "Iteration 152, loss = 0.00096156\n",
      "Iteration 153, loss = 0.00096119\n",
      "Iteration 154, loss = 0.00095808\n",
      "Iteration 155, loss = 0.00095704\n",
      "Iteration 156, loss = 0.00095794\n",
      "Iteration 157, loss = 0.00095557\n",
      "Iteration 158, loss = 0.00095305\n",
      "Iteration 159, loss = 0.00095369\n",
      "Iteration 160, loss = 0.00095478\n",
      "Iteration 161, loss = 0.00094997\n",
      "Iteration 162, loss = 0.00095060\n",
      "Iteration 163, loss = 0.00094657\n",
      "Iteration 164, loss = 0.00094603\n",
      "Iteration 165, loss = 0.00094462\n",
      "Iteration 166, loss = 0.00094520\n",
      "Iteration 167, loss = 0.00094794\n",
      "Iteration 168, loss = 0.00094370\n",
      "Iteration 169, loss = 0.00094525\n",
      "Iteration 170, loss = 0.00094169\n",
      "Iteration 171, loss = 0.00094111\n",
      "Iteration 172, loss = 0.00094245\n",
      "Iteration 173, loss = 0.00093735\n",
      "Iteration 174, loss = 0.00093926\n",
      "Iteration 175, loss = 0.00093924\n",
      "Iteration 176, loss = 0.00093637\n",
      "Iteration 177, loss = 0.00093452\n",
      "Iteration 178, loss = 0.00093322\n",
      "Iteration 179, loss = 0.00093451\n",
      "Iteration 180, loss = 0.00093381\n",
      "Iteration 181, loss = 0.00093207\n",
      "Iteration 182, loss = 0.00092873\n",
      "Iteration 183, loss = 0.00093057\n",
      "Iteration 184, loss = 0.00092919\n",
      "Iteration 185, loss = 0.00093013\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.20735425\n",
      "Iteration 2, loss = 0.14500906\n",
      "Iteration 3, loss = 0.10782831\n",
      "Iteration 4, loss = 0.06526722\n",
      "Iteration 5, loss = 0.03736824\n",
      "Iteration 6, loss = 0.02571080\n",
      "Iteration 7, loss = 0.01804365\n",
      "Iteration 8, loss = 0.01186582\n",
      "Iteration 9, loss = 0.00757357\n",
      "Iteration 10, loss = 0.00518991\n",
      "Iteration 11, loss = 0.00385101\n",
      "Iteration 12, loss = 0.00313605\n",
      "Iteration 13, loss = 0.00269991\n",
      "Iteration 14, loss = 0.00244150\n",
      "Iteration 15, loss = 0.00227973\n",
      "Iteration 16, loss = 0.00217165\n",
      "Iteration 17, loss = 0.00209869\n",
      "Iteration 18, loss = 0.00205007\n",
      "Iteration 19, loss = 0.00201012\n",
      "Iteration 20, loss = 0.00198280\n",
      "Iteration 21, loss = 0.00196182\n",
      "Iteration 22, loss = 0.00194240\n",
      "Iteration 23, loss = 0.00192237\n",
      "Iteration 24, loss = 0.00190899\n",
      "Iteration 25, loss = 0.00189692\n",
      "Iteration 26, loss = 0.00188303\n",
      "Iteration 27, loss = 0.00187625\n",
      "Iteration 28, loss = 0.00186051\n",
      "Iteration 29, loss = 0.00185172\n",
      "Iteration 30, loss = 0.00183878\n",
      "Iteration 31, loss = 0.00182879\n",
      "Iteration 32, loss = 0.00181762\n",
      "Iteration 33, loss = 0.00180660\n",
      "Iteration 34, loss = 0.00179564\n",
      "Iteration 35, loss = 0.00178646\n",
      "Iteration 36, loss = 0.00177548\n",
      "Iteration 37, loss = 0.00176307\n",
      "Iteration 38, loss = 0.00175139\n",
      "Iteration 39, loss = 0.00174226\n",
      "Iteration 40, loss = 0.00173155\n",
      "Iteration 41, loss = 0.00171926\n",
      "Iteration 42, loss = 0.00170887\n",
      "Iteration 43, loss = 0.00169826\n",
      "Iteration 44, loss = 0.00168388\n",
      "Iteration 45, loss = 0.00167533\n",
      "Iteration 46, loss = 0.00166225\n",
      "Iteration 47, loss = 0.00165339\n",
      "Iteration 48, loss = 0.00163922\n",
      "Iteration 49, loss = 0.00162882\n",
      "Iteration 50, loss = 0.00161748\n",
      "Iteration 51, loss = 0.00160628\n",
      "Iteration 52, loss = 0.00159427\n",
      "Iteration 53, loss = 0.00158143\n",
      "Iteration 54, loss = 0.00157294\n",
      "Iteration 55, loss = 0.00156171\n",
      "Iteration 56, loss = 0.00155360\n",
      "Iteration 57, loss = 0.00153912\n",
      "Iteration 58, loss = 0.00152894\n",
      "Iteration 59, loss = 0.00152207\n",
      "Iteration 60, loss = 0.00150854\n",
      "Iteration 61, loss = 0.00149771\n",
      "Iteration 62, loss = 0.00148510\n",
      "Iteration 63, loss = 0.00147694\n",
      "Iteration 64, loss = 0.00146583\n",
      "Iteration 65, loss = 0.00146095\n",
      "Iteration 66, loss = 0.00144666\n",
      "Iteration 67, loss = 0.00143857\n",
      "Iteration 68, loss = 0.00142825\n",
      "Iteration 69, loss = 0.00142088\n",
      "Iteration 70, loss = 0.00141247\n",
      "Iteration 71, loss = 0.00139965\n",
      "Iteration 72, loss = 0.00139572\n",
      "Iteration 73, loss = 0.00137995\n",
      "Iteration 74, loss = 0.00137335\n",
      "Iteration 75, loss = 0.00137317\n",
      "Iteration 76, loss = 0.00135894\n",
      "Iteration 77, loss = 0.00134790\n",
      "Iteration 78, loss = 0.00134588\n",
      "Iteration 79, loss = 0.00133137\n",
      "Iteration 80, loss = 0.00132879\n",
      "Iteration 81, loss = 0.00132579\n",
      "Iteration 82, loss = 0.00131059\n",
      "Iteration 83, loss = 0.00130359\n",
      "Iteration 84, loss = 0.00129636\n",
      "Iteration 85, loss = 0.00129489\n",
      "Iteration 86, loss = 0.00128355\n",
      "Iteration 87, loss = 0.00127648\n",
      "Iteration 88, loss = 0.00127442\n",
      "Iteration 89, loss = 0.00126715\n",
      "Iteration 90, loss = 0.00126141\n",
      "Iteration 91, loss = 0.00125331\n",
      "Iteration 92, loss = 0.00124601\n",
      "Iteration 93, loss = 0.00124202\n",
      "Iteration 94, loss = 0.00123349\n",
      "Iteration 95, loss = 0.00122802\n",
      "Iteration 96, loss = 0.00122416\n",
      "Iteration 97, loss = 0.00122244\n",
      "Iteration 98, loss = 0.00121480\n",
      "Iteration 99, loss = 0.00120606\n",
      "Iteration 100, loss = 0.00120376\n",
      "Iteration 101, loss = 0.00119793\n",
      "Iteration 102, loss = 0.00119000\n",
      "Iteration 103, loss = 0.00119362\n",
      "Iteration 104, loss = 0.00118524\n",
      "Iteration 105, loss = 0.00117730\n",
      "Iteration 106, loss = 0.00117363\n",
      "Iteration 107, loss = 0.00117369\n",
      "Iteration 108, loss = 0.00116247\n",
      "Iteration 109, loss = 0.00115854\n",
      "Iteration 110, loss = 0.00116122\n",
      "Iteration 111, loss = 0.00115549\n",
      "Iteration 112, loss = 0.00114680\n",
      "Iteration 113, loss = 0.00114394\n",
      "Iteration 114, loss = 0.00114244\n",
      "Iteration 115, loss = 0.00113801\n",
      "Iteration 116, loss = 0.00113379\n",
      "Iteration 117, loss = 0.00113643\n",
      "Iteration 118, loss = 0.00112731\n",
      "Iteration 119, loss = 0.00112355\n",
      "Iteration 120, loss = 0.00111781\n",
      "Iteration 121, loss = 0.00111292\n",
      "Iteration 122, loss = 0.00111693\n",
      "Iteration 123, loss = 0.00110909\n",
      "Iteration 124, loss = 0.00110709\n",
      "Iteration 125, loss = 0.00110573\n",
      "Iteration 126, loss = 0.00110503\n",
      "Iteration 127, loss = 0.00109682\n",
      "Iteration 128, loss = 0.00109393\n",
      "Iteration 129, loss = 0.00109414\n",
      "Iteration 130, loss = 0.00108857\n",
      "Iteration 131, loss = 0.00108842\n",
      "Iteration 132, loss = 0.00108094\n",
      "Iteration 133, loss = 0.00107939\n",
      "Iteration 134, loss = 0.00107739\n",
      "Iteration 135, loss = 0.00107449\n",
      "Iteration 136, loss = 0.00107296\n",
      "Iteration 137, loss = 0.00106956\n",
      "Iteration 138, loss = 0.00106665\n",
      "Iteration 139, loss = 0.00106644\n",
      "Iteration 140, loss = 0.00106466\n",
      "Iteration 141, loss = 0.00106456\n",
      "Iteration 142, loss = 0.00105743\n",
      "Iteration 143, loss = 0.00105978\n",
      "Iteration 144, loss = 0.00105652\n",
      "Iteration 145, loss = 0.00105700\n",
      "Iteration 146, loss = 0.00104974\n",
      "Iteration 147, loss = 0.00104597\n",
      "Iteration 148, loss = 0.00104633\n",
      "Iteration 149, loss = 0.00104161\n",
      "Iteration 150, loss = 0.00103969\n",
      "Iteration 151, loss = 0.00103857\n",
      "Iteration 152, loss = 0.00103635\n",
      "Iteration 153, loss = 0.00103409\n",
      "Iteration 154, loss = 0.00103522\n",
      "Iteration 155, loss = 0.00103151\n",
      "Iteration 156, loss = 0.00103002\n",
      "Iteration 157, loss = 0.00102883\n",
      "Iteration 158, loss = 0.00102775\n",
      "Iteration 159, loss = 0.00102060\n",
      "Iteration 160, loss = 0.00102443\n",
      "Iteration 161, loss = 0.00101991\n",
      "Iteration 162, loss = 0.00101939\n",
      "Iteration 163, loss = 0.00101636\n",
      "Iteration 164, loss = 0.00102088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 165, loss = 0.00102152\n",
      "Iteration 166, loss = 0.00101062\n",
      "Iteration 167, loss = 0.00101173\n",
      "Iteration 168, loss = 0.00100849\n",
      "Iteration 169, loss = 0.00100536\n",
      "Iteration 170, loss = 0.00101022\n",
      "Iteration 171, loss = 0.00100454\n",
      "Iteration 172, loss = 0.00100791\n",
      "Iteration 173, loss = 0.00100213\n",
      "Iteration 174, loss = 0.00100140\n",
      "Iteration 175, loss = 0.00099880\n",
      "Iteration 176, loss = 0.00099784\n",
      "Iteration 177, loss = 0.00099387\n",
      "Iteration 178, loss = 0.00099445\n",
      "Iteration 179, loss = 0.00099548\n",
      "Iteration 180, loss = 0.00099138\n",
      "Iteration 181, loss = 0.00099007\n",
      "Iteration 182, loss = 0.00099158\n",
      "Iteration 183, loss = 0.00099401\n",
      "Iteration 184, loss = 0.00098515\n",
      "Iteration 185, loss = 0.00098866\n",
      "Iteration 186, loss = 0.00098750\n",
      "Iteration 187, loss = 0.00098485\n",
      "Iteration 188, loss = 0.00098208\n",
      "Iteration 189, loss = 0.00098126\n",
      "Iteration 190, loss = 0.00097975\n",
      "Iteration 191, loss = 0.00098399\n",
      "Iteration 192, loss = 0.00097688\n",
      "Iteration 193, loss = 0.00097439\n",
      "Iteration 194, loss = 0.00097586\n",
      "Iteration 195, loss = 0.00097377\n",
      "Iteration 196, loss = 0.00097311\n",
      "Iteration 197, loss = 0.00097522\n",
      "Iteration 198, loss = 0.00097026\n",
      "Iteration 199, loss = 0.00096784\n",
      "Iteration 200, loss = 0.00096750\n",
      "Iteration 1, loss = 0.19123116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zuza/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.10102404\n",
      "Iteration 3, loss = 0.04782796\n",
      "Iteration 4, loss = 0.02615603\n",
      "Iteration 5, loss = 0.01818331\n",
      "Iteration 6, loss = 0.01233986\n",
      "Iteration 7, loss = 0.00841061\n",
      "Iteration 8, loss = 0.00609190\n",
      "Iteration 9, loss = 0.00481886\n",
      "Iteration 10, loss = 0.00398881\n",
      "Iteration 11, loss = 0.00349602\n",
      "Iteration 12, loss = 0.00316098\n",
      "Iteration 13, loss = 0.00293574\n",
      "Iteration 14, loss = 0.00278534\n",
      "Iteration 15, loss = 0.00267250\n",
      "Iteration 16, loss = 0.00258812\n",
      "Iteration 17, loss = 0.00252870\n",
      "Iteration 18, loss = 0.00246985\n",
      "Iteration 19, loss = 0.00241837\n",
      "Iteration 20, loss = 0.00237574\n",
      "Iteration 21, loss = 0.00233837\n",
      "Iteration 22, loss = 0.00230170\n",
      "Iteration 23, loss = 0.00227065\n",
      "Iteration 24, loss = 0.00224208\n",
      "Iteration 25, loss = 0.00221259\n",
      "Iteration 26, loss = 0.00218391\n",
      "Iteration 27, loss = 0.00215402\n",
      "Iteration 28, loss = 0.00212547\n",
      "Iteration 29, loss = 0.00209833\n",
      "Iteration 30, loss = 0.00207377\n",
      "Iteration 31, loss = 0.00205021\n",
      "Iteration 32, loss = 0.00202643\n",
      "Iteration 33, loss = 0.00200088\n",
      "Iteration 34, loss = 0.00197561\n",
      "Iteration 35, loss = 0.00195177\n",
      "Iteration 36, loss = 0.00192686\n",
      "Iteration 37, loss = 0.00190319\n",
      "Iteration 38, loss = 0.00188339\n",
      "Iteration 39, loss = 0.00185886\n",
      "Iteration 40, loss = 0.00183402\n",
      "Iteration 41, loss = 0.00181247\n",
      "Iteration 42, loss = 0.00179245\n",
      "Iteration 43, loss = 0.00177699\n",
      "Iteration 44, loss = 0.00175536\n",
      "Iteration 45, loss = 0.00172853\n",
      "Iteration 46, loss = 0.00170964\n",
      "Iteration 47, loss = 0.00168884\n",
      "Iteration 48, loss = 0.00166906\n",
      "Iteration 49, loss = 0.00165009\n",
      "Iteration 50, loss = 0.00163300\n",
      "Iteration 51, loss = 0.00161700\n",
      "Iteration 52, loss = 0.00159795\n",
      "Iteration 53, loss = 0.00158318\n",
      "Iteration 54, loss = 0.00156096\n",
      "Iteration 55, loss = 0.00154763\n",
      "Iteration 56, loss = 0.00153238\n",
      "Iteration 57, loss = 0.00151455\n",
      "Iteration 58, loss = 0.00150355\n",
      "Iteration 59, loss = 0.00148507\n",
      "Iteration 60, loss = 0.00146897\n",
      "Iteration 61, loss = 0.00145989\n",
      "Iteration 62, loss = 0.00144068\n",
      "Iteration 63, loss = 0.00142672\n",
      "Iteration 64, loss = 0.00141632\n",
      "Iteration 65, loss = 0.00140522\n",
      "Iteration 66, loss = 0.00139035\n",
      "Iteration 67, loss = 0.00137860\n",
      "Iteration 68, loss = 0.00137103\n",
      "Iteration 69, loss = 0.00135462\n",
      "Iteration 70, loss = 0.00134426\n",
      "Iteration 71, loss = 0.00133811\n",
      "Iteration 72, loss = 0.00132363\n",
      "Iteration 73, loss = 0.00131250\n",
      "Iteration 74, loss = 0.00130636\n",
      "Iteration 75, loss = 0.00129424\n",
      "Iteration 76, loss = 0.00128499\n",
      "Iteration 77, loss = 0.00128017\n",
      "Iteration 78, loss = 0.00127748\n",
      "Iteration 79, loss = 0.00126396\n",
      "Iteration 80, loss = 0.00125713\n",
      "Iteration 81, loss = 0.00124433\n",
      "Iteration 82, loss = 0.00124348\n",
      "Iteration 83, loss = 0.00123386\n",
      "Iteration 84, loss = 0.00121977\n",
      "Iteration 85, loss = 0.00121690\n",
      "Iteration 86, loss = 0.00121099\n",
      "Iteration 87, loss = 0.00120267\n",
      "Iteration 88, loss = 0.00119520\n",
      "Iteration 89, loss = 0.00119019\n",
      "Iteration 90, loss = 0.00118533\n",
      "Iteration 91, loss = 0.00118174\n",
      "Iteration 92, loss = 0.00117586\n",
      "Iteration 93, loss = 0.00116796\n",
      "Iteration 94, loss = 0.00116422\n",
      "Iteration 95, loss = 0.00115651\n",
      "Iteration 96, loss = 0.00115490\n",
      "Iteration 97, loss = 0.00114708\n",
      "Iteration 98, loss = 0.00114801\n",
      "Iteration 99, loss = 0.00114038\n",
      "Iteration 100, loss = 0.00113572\n",
      "Iteration 101, loss = 0.00112671\n",
      "Iteration 102, loss = 0.00112303\n",
      "Iteration 103, loss = 0.00112420\n",
      "Iteration 104, loss = 0.00112116\n",
      "Iteration 105, loss = 0.00111721\n",
      "Iteration 106, loss = 0.00111956\n",
      "Iteration 107, loss = 0.00110715\n",
      "Iteration 108, loss = 0.00110510\n",
      "Iteration 109, loss = 0.00109956\n",
      "Iteration 110, loss = 0.00109776\n",
      "Iteration 111, loss = 0.00109047\n",
      "Iteration 112, loss = 0.00108869\n",
      "Iteration 113, loss = 0.00108704\n",
      "Iteration 114, loss = 0.00108344\n",
      "Iteration 115, loss = 0.00108018\n",
      "Iteration 116, loss = 0.00107734\n",
      "Iteration 117, loss = 0.00107538\n",
      "Iteration 118, loss = 0.00106834\n",
      "Iteration 119, loss = 0.00106675\n",
      "Iteration 120, loss = 0.00108040\n",
      "Iteration 121, loss = 0.00106197\n",
      "Iteration 122, loss = 0.00105932\n",
      "Iteration 123, loss = 0.00106075\n",
      "Iteration 124, loss = 0.00105291\n",
      "Iteration 125, loss = 0.00105181\n",
      "Iteration 126, loss = 0.00104872\n",
      "Iteration 127, loss = 0.00105262\n",
      "Iteration 128, loss = 0.00104613\n",
      "Iteration 129, loss = 0.00104227\n",
      "Iteration 130, loss = 0.00104044\n",
      "Iteration 131, loss = 0.00104052\n",
      "Iteration 132, loss = 0.00104006\n",
      "Iteration 133, loss = 0.00103566\n",
      "Iteration 134, loss = 0.00103142\n",
      "Iteration 135, loss = 0.00103044\n",
      "Iteration 136, loss = 0.00102771\n",
      "Iteration 137, loss = 0.00102837\n",
      "Iteration 138, loss = 0.00102538\n",
      "Iteration 139, loss = 0.00102265\n",
      "Iteration 140, loss = 0.00102789\n",
      "Iteration 141, loss = 0.00102421\n",
      "Iteration 142, loss = 0.00101726\n",
      "Iteration 143, loss = 0.00101110\n",
      "Iteration 144, loss = 0.00101222\n",
      "Iteration 145, loss = 0.00102086\n",
      "Iteration 146, loss = 0.00101369\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.21872471\n",
      "Iteration 2, loss = 0.12380877\n",
      "Iteration 3, loss = 0.06228208\n",
      "Iteration 4, loss = 0.03038519\n",
      "Iteration 5, loss = 0.01906261\n",
      "Iteration 6, loss = 0.01190567\n",
      "Iteration 7, loss = 0.00784854\n",
      "Iteration 8, loss = 0.00570341\n",
      "Iteration 9, loss = 0.00446102\n",
      "Iteration 10, loss = 0.00368593\n",
      "Iteration 11, loss = 0.00321650\n",
      "Iteration 12, loss = 0.00290605\n",
      "Iteration 13, loss = 0.00270820\n",
      "Iteration 14, loss = 0.00256833\n",
      "Iteration 15, loss = 0.00246645\n",
      "Iteration 16, loss = 0.00239301\n",
      "Iteration 17, loss = 0.00233047\n",
      "Iteration 18, loss = 0.00228696\n",
      "Iteration 19, loss = 0.00223073\n",
      "Iteration 20, loss = 0.00218487\n",
      "Iteration 21, loss = 0.00215041\n",
      "Iteration 22, loss = 0.00212296\n",
      "Iteration 23, loss = 0.00209683\n",
      "Iteration 24, loss = 0.00207306\n",
      "Iteration 25, loss = 0.00204731\n",
      "Iteration 26, loss = 0.00202464\n",
      "Iteration 27, loss = 0.00200286\n",
      "Iteration 28, loss = 0.00198073\n",
      "Iteration 29, loss = 0.00195993\n",
      "Iteration 30, loss = 0.00193881\n",
      "Iteration 31, loss = 0.00191898\n",
      "Iteration 32, loss = 0.00189644\n",
      "Iteration 33, loss = 0.00187788\n",
      "Iteration 34, loss = 0.00185784\n",
      "Iteration 35, loss = 0.00183803\n",
      "Iteration 36, loss = 0.00181806\n",
      "Iteration 37, loss = 0.00179852\n",
      "Iteration 38, loss = 0.00177946\n",
      "Iteration 39, loss = 0.00176042\n",
      "Iteration 40, loss = 0.00174208\n",
      "Iteration 41, loss = 0.00172529\n",
      "Iteration 42, loss = 0.00170400\n",
      "Iteration 43, loss = 0.00168622\n",
      "Iteration 44, loss = 0.00166830\n",
      "Iteration 45, loss = 0.00165084\n",
      "Iteration 46, loss = 0.00163661\n",
      "Iteration 47, loss = 0.00161671\n",
      "Iteration 48, loss = 0.00160114\n",
      "Iteration 49, loss = 0.00158334\n",
      "Iteration 50, loss = 0.00156724\n",
      "Iteration 51, loss = 0.00155164\n",
      "Iteration 52, loss = 0.00153736\n",
      "Iteration 53, loss = 0.00152082\n",
      "Iteration 54, loss = 0.00150540\n",
      "Iteration 55, loss = 0.00149299\n",
      "Iteration 56, loss = 0.00147657\n",
      "Iteration 57, loss = 0.00146249\n",
      "Iteration 58, loss = 0.00144855\n",
      "Iteration 59, loss = 0.00143599\n",
      "Iteration 60, loss = 0.00142521\n",
      "Iteration 61, loss = 0.00140898\n",
      "Iteration 62, loss = 0.00139713\n",
      "Iteration 63, loss = 0.00138448\n",
      "Iteration 64, loss = 0.00137038\n",
      "Iteration 65, loss = 0.00136283\n",
      "Iteration 66, loss = 0.00135205\n",
      "Iteration 67, loss = 0.00133660\n",
      "Iteration 68, loss = 0.00132638\n",
      "Iteration 69, loss = 0.00131596\n",
      "Iteration 70, loss = 0.00130574\n",
      "Iteration 71, loss = 0.00129474\n",
      "Iteration 72, loss = 0.00128819\n",
      "Iteration 73, loss = 0.00128211\n",
      "Iteration 74, loss = 0.00126468\n",
      "Iteration 75, loss = 0.00125661\n",
      "Iteration 76, loss = 0.00125135\n",
      "Iteration 77, loss = 0.00123942\n",
      "Iteration 78, loss = 0.00123298\n",
      "Iteration 79, loss = 0.00122244\n",
      "Iteration 80, loss = 0.00121258\n",
      "Iteration 81, loss = 0.00120709\n",
      "Iteration 82, loss = 0.00120121\n",
      "Iteration 83, loss = 0.00119536\n",
      "Iteration 84, loss = 0.00118486\n",
      "Iteration 85, loss = 0.00117930\n",
      "Iteration 86, loss = 0.00117045\n",
      "Iteration 87, loss = 0.00116732\n",
      "Iteration 88, loss = 0.00116262\n",
      "Iteration 89, loss = 0.00115591\n",
      "Iteration 90, loss = 0.00114773\n",
      "Iteration 91, loss = 0.00114257\n",
      "Iteration 92, loss = 0.00113672\n",
      "Iteration 93, loss = 0.00113358\n",
      "Iteration 94, loss = 0.00112640\n",
      "Iteration 95, loss = 0.00111979\n",
      "Iteration 96, loss = 0.00111826\n",
      "Iteration 97, loss = 0.00111381\n",
      "Iteration 98, loss = 0.00110479\n",
      "Iteration 99, loss = 0.00110880\n",
      "Iteration 100, loss = 0.00109811\n",
      "Iteration 101, loss = 0.00108947\n",
      "Iteration 102, loss = 0.00108427\n",
      "Iteration 103, loss = 0.00108795\n",
      "Iteration 104, loss = 0.00107646\n",
      "Iteration 105, loss = 0.00107272\n",
      "Iteration 106, loss = 0.00106726\n",
      "Iteration 107, loss = 0.00107110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 108, loss = 0.00106056\n",
      "Iteration 109, loss = 0.00106040\n",
      "Iteration 110, loss = 0.00105378\n",
      "Iteration 111, loss = 0.00105545\n",
      "Iteration 112, loss = 0.00105231\n",
      "Iteration 113, loss = 0.00104230\n",
      "Iteration 114, loss = 0.00104059\n",
      "Iteration 115, loss = 0.00103952\n",
      "Iteration 116, loss = 0.00103768\n",
      "Iteration 117, loss = 0.00103628\n",
      "Iteration 118, loss = 0.00103325\n",
      "Iteration 119, loss = 0.00102924\n",
      "Iteration 120, loss = 0.00102452\n",
      "Iteration 121, loss = 0.00102396\n",
      "Iteration 122, loss = 0.00101936\n",
      "Iteration 123, loss = 0.00101948\n",
      "Iteration 124, loss = 0.00101811\n",
      "Iteration 125, loss = 0.00102146\n",
      "Iteration 126, loss = 0.00101723\n",
      "Iteration 127, loss = 0.00100920\n",
      "Iteration 128, loss = 0.00100637\n",
      "Iteration 129, loss = 0.00100742\n",
      "Iteration 130, loss = 0.00100436\n",
      "Iteration 131, loss = 0.00100324\n",
      "Iteration 132, loss = 0.00100077\n",
      "Iteration 133, loss = 0.00099893\n",
      "Iteration 134, loss = 0.00100031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zuza/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.17864845\n",
      "Iteration 2, loss = 0.09418842\n",
      "Iteration 3, loss = 0.04136952\n",
      "Iteration 4, loss = 0.02227907\n",
      "Iteration 5, loss = 0.01342267\n",
      "Iteration 6, loss = 0.00817712\n",
      "Iteration 7, loss = 0.00556662\n",
      "Iteration 8, loss = 0.00416008\n",
      "Iteration 9, loss = 0.00348822\n",
      "Iteration 10, loss = 0.00311284\n",
      "Iteration 11, loss = 0.00287761\n",
      "Iteration 12, loss = 0.00271187\n",
      "Iteration 13, loss = 0.00260918\n",
      "Iteration 14, loss = 0.00252441\n",
      "Iteration 15, loss = 0.00245648\n",
      "Iteration 16, loss = 0.00241153\n",
      "Iteration 17, loss = 0.00237127\n",
      "Iteration 18, loss = 0.00233629\n",
      "Iteration 19, loss = 0.00230142\n",
      "Iteration 20, loss = 0.00226927\n",
      "Iteration 21, loss = 0.00224109\n",
      "Iteration 22, loss = 0.00221020\n",
      "Iteration 23, loss = 0.00218380\n",
      "Iteration 24, loss = 0.00215587\n",
      "Iteration 25, loss = 0.00212916\n",
      "Iteration 26, loss = 0.00210361\n",
      "Iteration 27, loss = 0.00207891\n",
      "Iteration 28, loss = 0.00205244\n",
      "Iteration 29, loss = 0.00202559\n",
      "Iteration 30, loss = 0.00200258\n",
      "Iteration 31, loss = 0.00197497\n",
      "Iteration 32, loss = 0.00194873\n",
      "Iteration 33, loss = 0.00192293\n",
      "Iteration 34, loss = 0.00189805\n",
      "Iteration 35, loss = 0.00187503\n",
      "Iteration 36, loss = 0.00185066\n",
      "Iteration 37, loss = 0.00182762\n",
      "Iteration 38, loss = 0.00180515\n",
      "Iteration 39, loss = 0.00178404\n",
      "Iteration 40, loss = 0.00176012\n",
      "Iteration 41, loss = 0.00173963\n",
      "Iteration 42, loss = 0.00171806\n",
      "Iteration 43, loss = 0.00169659\n",
      "Iteration 44, loss = 0.00167650\n",
      "Iteration 45, loss = 0.00165621\n",
      "Iteration 46, loss = 0.00163525\n",
      "Iteration 47, loss = 0.00161739\n",
      "Iteration 48, loss = 0.00159889\n",
      "Iteration 49, loss = 0.00157943\n",
      "Iteration 50, loss = 0.00156021\n",
      "Iteration 51, loss = 0.00154204\n",
      "Iteration 52, loss = 0.00152422\n",
      "Iteration 53, loss = 0.00151319\n",
      "Iteration 54, loss = 0.00149325\n",
      "Iteration 55, loss = 0.00147632\n",
      "Iteration 56, loss = 0.00145864\n",
      "Iteration 57, loss = 0.00144434\n",
      "Iteration 58, loss = 0.00142847\n",
      "Iteration 59, loss = 0.00141708\n",
      "Iteration 60, loss = 0.00140324\n",
      "Iteration 61, loss = 0.00138538\n",
      "Iteration 62, loss = 0.00137365\n",
      "Iteration 63, loss = 0.00136116\n",
      "Iteration 64, loss = 0.00134918\n",
      "Iteration 65, loss = 0.00133923\n",
      "Iteration 66, loss = 0.00132767\n",
      "Iteration 67, loss = 0.00131707\n",
      "Iteration 68, loss = 0.00130402\n",
      "Iteration 69, loss = 0.00129635\n",
      "Iteration 70, loss = 0.00128594\n",
      "Iteration 71, loss = 0.00127231\n",
      "Iteration 72, loss = 0.00126524\n",
      "Iteration 73, loss = 0.00125439\n",
      "Iteration 74, loss = 0.00124655\n",
      "Iteration 75, loss = 0.00123713\n",
      "Iteration 76, loss = 0.00122827\n",
      "Iteration 77, loss = 0.00122200\n",
      "Iteration 78, loss = 0.00121550\n",
      "Iteration 79, loss = 0.00120698\n",
      "Iteration 80, loss = 0.00120410\n",
      "Iteration 81, loss = 0.00119414\n",
      "Iteration 82, loss = 0.00119043\n",
      "Iteration 83, loss = 0.00117989\n",
      "Iteration 84, loss = 0.00116923\n",
      "Iteration 85, loss = 0.00116581\n",
      "Iteration 86, loss = 0.00115955\n",
      "Iteration 87, loss = 0.00115275\n",
      "Iteration 88, loss = 0.00115070\n",
      "Iteration 89, loss = 0.00115050\n",
      "Iteration 90, loss = 0.00114023\n",
      "Iteration 91, loss = 0.00113207\n",
      "Iteration 92, loss = 0.00112673\n",
      "Iteration 93, loss = 0.00112828\n",
      "Iteration 94, loss = 0.00111913\n",
      "Iteration 95, loss = 0.00111421\n",
      "Iteration 96, loss = 0.00110983\n",
      "Iteration 97, loss = 0.00110288\n",
      "Iteration 98, loss = 0.00110143\n",
      "Iteration 99, loss = 0.00109865\n",
      "Iteration 100, loss = 0.00109670\n",
      "Iteration 101, loss = 0.00109393\n",
      "Iteration 102, loss = 0.00108401\n",
      "Iteration 103, loss = 0.00108359\n",
      "Iteration 104, loss = 0.00107832\n",
      "Iteration 105, loss = 0.00107653\n",
      "Iteration 106, loss = 0.00107139\n",
      "Iteration 107, loss = 0.00107169\n",
      "Iteration 108, loss = 0.00107158\n",
      "Iteration 109, loss = 0.00106867\n",
      "Iteration 110, loss = 0.00107179\n",
      "Iteration 111, loss = 0.00105688\n",
      "Iteration 112, loss = 0.00105389\n",
      "Iteration 113, loss = 0.00105416\n",
      "Iteration 114, loss = 0.00105281\n",
      "Iteration 115, loss = 0.00104849\n",
      "Iteration 116, loss = 0.00104229\n",
      "Iteration 117, loss = 0.00104694\n",
      "Iteration 118, loss = 0.00104212\n",
      "Iteration 119, loss = 0.00103679\n",
      "Iteration 120, loss = 0.00103685\n",
      "Iteration 121, loss = 0.00103241\n",
      "Iteration 122, loss = 0.00103162\n",
      "Iteration 123, loss = 0.00103215\n",
      "Iteration 124, loss = 0.00103153\n",
      "Iteration 125, loss = 0.00102665\n",
      "Iteration 126, loss = 0.00102355\n",
      "Iteration 127, loss = 0.00102416\n",
      "Iteration 128, loss = 0.00102135\n",
      "Iteration 129, loss = 0.00101927\n",
      "Iteration 130, loss = 0.00101481\n",
      "Iteration 131, loss = 0.00101618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zuza/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.23590722\n",
      "Iteration 2, loss = 0.18004816\n",
      "Iteration 3, loss = 0.13322790\n",
      "Iteration 4, loss = 0.09669786\n",
      "Iteration 5, loss = 0.06533778\n",
      "Iteration 6, loss = 0.04708180\n",
      "Iteration 7, loss = 0.03913686\n",
      "Iteration 8, loss = 0.03463560\n",
      "Iteration 9, loss = 0.03101674\n",
      "Iteration 10, loss = 0.02764036\n",
      "Iteration 11, loss = 0.02452133\n",
      "Iteration 12, loss = 0.02173057\n",
      "Iteration 13, loss = 0.01937954\n",
      "Iteration 14, loss = 0.01753623\n",
      "Iteration 15, loss = 0.01614757\n",
      "Iteration 16, loss = 0.01514793\n",
      "Iteration 17, loss = 0.01441173\n",
      "Iteration 18, loss = 0.01378463\n",
      "Iteration 19, loss = 0.01335759\n",
      "Iteration 20, loss = 0.01304312\n",
      "Iteration 21, loss = 0.01279676\n",
      "Iteration 22, loss = 0.01257238\n",
      "Iteration 23, loss = 0.01238309\n",
      "Iteration 24, loss = 0.01221500\n",
      "Iteration 25, loss = 0.01206407\n",
      "Iteration 26, loss = 0.01189574\n",
      "Iteration 27, loss = 0.01175003\n",
      "Iteration 28, loss = 0.01162037\n",
      "Iteration 29, loss = 0.01150124\n",
      "Iteration 30, loss = 0.01138982\n",
      "Iteration 31, loss = 0.01127034\n",
      "Iteration 32, loss = 0.01115977\n",
      "Iteration 33, loss = 0.01105914\n",
      "Iteration 34, loss = 0.01095569\n",
      "Iteration 35, loss = 0.01085574\n",
      "Iteration 36, loss = 0.01076420\n",
      "Iteration 37, loss = 0.01066627\n",
      "Iteration 38, loss = 0.01057909\n",
      "Iteration 39, loss = 0.01049357\n",
      "Iteration 40, loss = 0.01041357\n",
      "Iteration 41, loss = 0.01032802\n",
      "Iteration 42, loss = 0.01025316\n",
      "Iteration 43, loss = 0.01017848\n",
      "Iteration 44, loss = 0.01011168\n",
      "Iteration 45, loss = 0.01004161\n",
      "Iteration 46, loss = 0.00997358\n",
      "Iteration 47, loss = 0.00990532\n",
      "Iteration 48, loss = 0.00984678\n",
      "Iteration 49, loss = 0.00978605\n",
      "Iteration 50, loss = 0.00973235\n",
      "Iteration 51, loss = 0.00967695\n",
      "Iteration 52, loss = 0.00962833\n",
      "Iteration 53, loss = 0.00957378\n",
      "Iteration 54, loss = 0.00952551\n",
      "Iteration 55, loss = 0.00947971\n",
      "Iteration 56, loss = 0.00943187\n",
      "Iteration 57, loss = 0.00939237\n",
      "Iteration 58, loss = 0.00934625\n",
      "Iteration 59, loss = 0.00931123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zuza/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.21532671\n",
      "Iteration 2, loss = 0.14739188\n",
      "Iteration 3, loss = 0.09640292\n",
      "Iteration 4, loss = 0.06557194\n",
      "Iteration 5, loss = 0.05102382\n",
      "Iteration 6, loss = 0.04322847\n",
      "Iteration 7, loss = 0.03685152\n",
      "Iteration 8, loss = 0.03110929\n",
      "Iteration 9, loss = 0.02637871\n",
      "Iteration 10, loss = 0.02275016\n",
      "Iteration 11, loss = 0.02018641\n",
      "Iteration 12, loss = 0.01845978\n",
      "Iteration 13, loss = 0.01725705\n",
      "Iteration 14, loss = 0.01635178\n",
      "Iteration 15, loss = 0.01566470\n",
      "Iteration 16, loss = 0.01513124\n",
      "Iteration 17, loss = 0.01468067\n",
      "Iteration 18, loss = 0.01431088\n",
      "Iteration 19, loss = 0.01396372\n",
      "Iteration 20, loss = 0.01366821\n",
      "Iteration 21, loss = 0.01339643\n",
      "Iteration 22, loss = 0.01313977\n",
      "Iteration 23, loss = 0.01291289\n",
      "Iteration 24, loss = 0.01269322\n",
      "Iteration 25, loss = 0.01248966\n",
      "Iteration 26, loss = 0.01229420\n",
      "Iteration 27, loss = 0.01211138\n",
      "Iteration 28, loss = 0.01194059\n",
      "Iteration 29, loss = 0.01177651\n",
      "Iteration 30, loss = 0.01162044\n",
      "Iteration 31, loss = 0.01147287\n",
      "Iteration 32, loss = 0.01132436\n",
      "Iteration 33, loss = 0.01119632\n",
      "Iteration 34, loss = 0.01106441\n",
      "Iteration 35, loss = 0.01094043\n",
      "Iteration 36, loss = 0.01082037\n",
      "Iteration 37, loss = 0.01071079\n",
      "Iteration 38, loss = 0.01060008\n",
      "Iteration 39, loss = 0.01049785\n",
      "Iteration 40, loss = 0.01040135\n",
      "Iteration 41, loss = 0.01029697\n",
      "Iteration 42, loss = 0.01020772\n",
      "Iteration 43, loss = 0.01011607\n",
      "Iteration 44, loss = 0.01002638\n",
      "Iteration 45, loss = 0.00994382\n",
      "Iteration 46, loss = 0.00986149\n",
      "Iteration 47, loss = 0.00978656\n",
      "Iteration 48, loss = 0.00972038\n",
      "Iteration 49, loss = 0.00964532\n",
      "Iteration 50, loss = 0.00958653\n",
      "Iteration 51, loss = 0.00952258\n",
      "Iteration 52, loss = 0.00946320\n",
      "Iteration 53, loss = 0.00941498\n",
      "Iteration 54, loss = 0.00935493\n",
      "Iteration 55, loss = 0.00930364\n",
      "Iteration 56, loss = 0.00925817\n",
      "Iteration 57, loss = 0.00921093\n",
      "Iteration 58, loss = 0.00916789\n",
      "Iteration 59, loss = 0.00912132\n",
      "Iteration 60, loss = 0.00908486\n",
      "Iteration 61, loss = 0.00905011\n",
      "Iteration 62, loss = 0.00901342\n",
      "Iteration 63, loss = 0.00897565\n",
      "Iteration 64, loss = 0.00893765\n",
      "Iteration 65, loss = 0.00891054\n",
      "Iteration 66, loss = 0.00887655\n",
      "Iteration 67, loss = 0.00885248\n",
      "Iteration 68, loss = 0.00881414\n",
      "Iteration 69, loss = 0.00878266\n",
      "Iteration 70, loss = 0.00875867\n",
      "Iteration 71, loss = 0.00873199\n",
      "Iteration 72, loss = 0.00870234\n",
      "Iteration 73, loss = 0.00867532\n",
      "Iteration 74, loss = 0.00863598\n",
      "Iteration 75, loss = 0.00861417\n",
      "Iteration 76, loss = 0.00859278\n",
      "Iteration 77, loss = 0.00857079\n",
      "Iteration 78, loss = 0.00854676\n",
      "Iteration 79, loss = 0.00852634\n",
      "Iteration 80, loss = 0.00850451\n",
      "Iteration 81, loss = 0.00850344\n",
      "Iteration 82, loss = 0.00846893\n",
      "Iteration 83, loss = 0.00844117\n",
      "Iteration 84, loss = 0.00842263\n",
      "Iteration 85, loss = 0.00840833\n",
      "Iteration 86, loss = 0.00838871\n",
      "Iteration 87, loss = 0.00836844\n",
      "Iteration 88, loss = 0.00835214\n",
      "Iteration 89, loss = 0.00833666\n",
      "Iteration 90, loss = 0.00831494\n",
      "Iteration 91, loss = 0.00829932\n",
      "Iteration 92, loss = 0.00828487\n",
      "Iteration 93, loss = 0.00826531\n",
      "Iteration 94, loss = 0.00825372\n",
      "Iteration 95, loss = 0.00824008\n",
      "Iteration 96, loss = 0.00822646\n",
      "Iteration 97, loss = 0.00821863\n",
      "Iteration 98, loss = 0.00820837\n",
      "Iteration 99, loss = 0.00819353\n",
      "Iteration 100, loss = 0.00817697\n",
      "Iteration 101, loss = 0.00816372\n",
      "Iteration 102, loss = 0.00815230\n",
      "Iteration 103, loss = 0.00814605\n",
      "Iteration 104, loss = 0.00813355\n",
      "Iteration 105, loss = 0.00812311\n",
      "Iteration 106, loss = 0.00811184\n",
      "Iteration 107, loss = 0.00810663\n",
      "Iteration 108, loss = 0.00809566\n",
      "Iteration 109, loss = 0.00808753\n",
      "Iteration 110, loss = 0.00808428\n",
      "Iteration 111, loss = 0.00807059\n",
      "Iteration 112, loss = 0.00805660\n",
      "Iteration 113, loss = 0.00804862\n",
      "Iteration 114, loss = 0.00803896\n",
      "Iteration 115, loss = 0.00803352\n",
      "Iteration 116, loss = 0.00802738\n",
      "Iteration 117, loss = 0.00801720\n",
      "Iteration 118, loss = 0.00801316\n",
      "Iteration 119, loss = 0.00800509\n",
      "Iteration 120, loss = 0.00799640\n",
      "Iteration 121, loss = 0.00799077\n",
      "Iteration 122, loss = 0.00798506\n",
      "Iteration 123, loss = 0.00797615\n",
      "Iteration 124, loss = 0.00796901\n",
      "Iteration 125, loss = 0.00796505\n",
      "Iteration 126, loss = 0.00796020\n",
      "Iteration 127, loss = 0.00795884\n",
      "Iteration 128, loss = 0.00795095\n",
      "Iteration 129, loss = 0.00794253\n",
      "Iteration 130, loss = 0.00793137\n",
      "Iteration 131, loss = 0.00792698\n",
      "Iteration 132, loss = 0.00792326\n",
      "Iteration 133, loss = 0.00791701\n",
      "Iteration 134, loss = 0.00791444\n",
      "Iteration 135, loss = 0.00790997\n",
      "Iteration 136, loss = 0.00790622\n",
      "Iteration 137, loss = 0.00789739\n",
      "Iteration 138, loss = 0.00789379\n",
      "Iteration 139, loss = 0.00788813\n",
      "Iteration 140, loss = 0.00788932\n",
      "Iteration 141, loss = 0.00787516\n",
      "Iteration 142, loss = 0.00787249\n",
      "Iteration 143, loss = 0.00786843\n",
      "Iteration 144, loss = 0.00786058\n",
      "Iteration 145, loss = 0.00785524\n",
      "Iteration 146, loss = 0.00785623\n",
      "Iteration 147, loss = 0.00785577\n",
      "Iteration 148, loss = 0.00784727\n",
      "Iteration 149, loss = 0.00784363\n",
      "Iteration 150, loss = 0.00783985\n",
      "Iteration 151, loss = 0.00783713\n",
      "Iteration 152, loss = 0.00783721\n",
      "Iteration 153, loss = 0.00782767\n",
      "Iteration 154, loss = 0.00781974\n",
      "Iteration 155, loss = 0.00782222\n",
      "Iteration 156, loss = 0.00782444\n",
      "Iteration 157, loss = 0.00781645\n",
      "Iteration 158, loss = 0.00780934\n",
      "Iteration 159, loss = 0.00780701\n",
      "Iteration 160, loss = 0.00780203\n",
      "Iteration 161, loss = 0.00779773\n",
      "Iteration 162, loss = 0.00779581\n",
      "Iteration 163, loss = 0.00779754\n",
      "Iteration 164, loss = 0.00779177\n",
      "Iteration 165, loss = 0.00778885\n",
      "Iteration 166, loss = 0.00777928\n",
      "Iteration 167, loss = 0.00777838\n",
      "Iteration 168, loss = 0.00777800\n",
      "Iteration 169, loss = 0.00777520\n",
      "Iteration 170, loss = 0.00777588\n",
      "Iteration 171, loss = 0.00776987\n",
      "Iteration 172, loss = 0.00777392\n",
      "Iteration 173, loss = 0.00776149\n",
      "Iteration 174, loss = 0.00776407\n",
      "Iteration 175, loss = 0.00775730\n",
      "Iteration 176, loss = 0.00775303\n",
      "Iteration 177, loss = 0.00774986\n",
      "Iteration 178, loss = 0.00774830\n",
      "Iteration 179, loss = 0.00774554\n",
      "Iteration 180, loss = 0.00774950\n",
      "Iteration 181, loss = 0.00774346\n",
      "Iteration 182, loss = 0.00773650\n",
      "Iteration 183, loss = 0.00773419\n",
      "Iteration 184, loss = 0.00773273\n",
      "Iteration 185, loss = 0.00773742\n",
      "Iteration 186, loss = 0.00772914\n",
      "Iteration 187, loss = 0.00772567\n",
      "Iteration 188, loss = 0.00772694\n",
      "Iteration 189, loss = 0.00772068\n",
      "Iteration 190, loss = 0.00771728\n",
      "Iteration 191, loss = 0.00772237\n",
      "Iteration 192, loss = 0.00771890\n",
      "Iteration 193, loss = 0.00770992\n",
      "Iteration 194, loss = 0.00771021\n",
      "Iteration 195, loss = 0.00770724\n",
      "Iteration 196, loss = 0.00770896\n",
      "Iteration 197, loss = 0.00770861\n",
      "Iteration 198, loss = 0.00770468\n",
      "Iteration 199, loss = 0.00770544\n",
      "Iteration 200, loss = 0.00769692\n",
      "Iteration 1, loss = 0.22977613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zuza/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.14730821\n",
      "Iteration 3, loss = 0.10263397\n",
      "Iteration 4, loss = 0.06822692\n",
      "Iteration 5, loss = 0.04743375\n",
      "Iteration 6, loss = 0.03918197\n",
      "Iteration 7, loss = 0.03520393\n",
      "Iteration 8, loss = 0.03218662\n",
      "Iteration 9, loss = 0.02942271\n",
      "Iteration 10, loss = 0.02665070\n",
      "Iteration 11, loss = 0.02401085\n",
      "Iteration 12, loss = 0.02158429\n",
      "Iteration 13, loss = 0.01943550\n",
      "Iteration 14, loss = 0.01773893\n",
      "Iteration 15, loss = 0.01650599\n",
      "Iteration 16, loss = 0.01557417\n",
      "Iteration 17, loss = 0.01486592\n",
      "Iteration 18, loss = 0.01432545\n",
      "Iteration 19, loss = 0.01388176\n",
      "Iteration 20, loss = 0.01352516\n",
      "Iteration 21, loss = 0.01321417\n",
      "Iteration 22, loss = 0.01296040\n",
      "Iteration 23, loss = 0.01271909\n",
      "Iteration 24, loss = 0.01250359\n",
      "Iteration 25, loss = 0.01229918\n",
      "Iteration 26, loss = 0.01211154\n",
      "Iteration 27, loss = 0.01193985\n",
      "Iteration 28, loss = 0.01177568\n",
      "Iteration 29, loss = 0.01161869\n",
      "Iteration 30, loss = 0.01148225\n",
      "Iteration 31, loss = 0.01134920\n",
      "Iteration 32, loss = 0.01121237\n",
      "Iteration 33, loss = 0.01109398\n",
      "Iteration 34, loss = 0.01097105\n",
      "Iteration 35, loss = 0.01084970\n",
      "Iteration 36, loss = 0.01073596\n",
      "Iteration 37, loss = 0.01063190\n",
      "Iteration 38, loss = 0.01052523\n",
      "Iteration 39, loss = 0.01041942\n",
      "Iteration 40, loss = 0.01032666\n",
      "Iteration 41, loss = 0.01023525\n",
      "Iteration 42, loss = 0.01013909\n",
      "Iteration 43, loss = 0.01005687\n",
      "Iteration 44, loss = 0.00997392\n",
      "Iteration 45, loss = 0.00989621\n",
      "Iteration 46, loss = 0.00982285\n",
      "Iteration 47, loss = 0.00974007\n",
      "Iteration 48, loss = 0.00967995\n",
      "Iteration 49, loss = 0.00960830\n",
      "Iteration 50, loss = 0.00954459\n",
      "Iteration 51, loss = 0.00948323\n",
      "Iteration 52, loss = 0.00942154\n",
      "Iteration 53, loss = 0.00937001\n",
      "Iteration 54, loss = 0.00931105\n",
      "Iteration 55, loss = 0.00926027\n",
      "Iteration 56, loss = 0.00921172\n",
      "Iteration 57, loss = 0.00916487\n",
      "Iteration 58, loss = 0.00911645\n",
      "Iteration 59, loss = 0.00907687\n",
      "Iteration 60, loss = 0.00903180\n",
      "Iteration 61, loss = 0.00898983\n",
      "Iteration 62, loss = 0.00895011\n",
      "Iteration 63, loss = 0.00891471\n",
      "Iteration 64, loss = 0.00887781\n",
      "Iteration 65, loss = 0.00884681\n",
      "Iteration 66, loss = 0.00881379\n",
      "Iteration 67, loss = 0.00878466\n",
      "Iteration 68, loss = 0.00875321\n",
      "Iteration 69, loss = 0.00872739\n",
      "Iteration 70, loss = 0.00869696\n",
      "Iteration 71, loss = 0.00867276\n",
      "Iteration 72, loss = 0.00864764\n",
      "Iteration 73, loss = 0.00861766\n",
      "Iteration 74, loss = 0.00859845\n",
      "Iteration 75, loss = 0.00857063\n",
      "Iteration 76, loss = 0.00854806\n",
      "Iteration 77, loss = 0.00852512\n",
      "Iteration 78, loss = 0.00850578\n",
      "Iteration 79, loss = 0.00849955\n",
      "Iteration 80, loss = 0.00847107\n",
      "Iteration 81, loss = 0.00845014\n",
      "Iteration 82, loss = 0.00842950\n",
      "Iteration 83, loss = 0.00841829\n",
      "Iteration 84, loss = 0.00839989\n",
      "Iteration 85, loss = 0.00837770\n",
      "Iteration 86, loss = 0.00836369\n",
      "Iteration 87, loss = 0.00834609\n",
      "Iteration 88, loss = 0.00833448\n",
      "Iteration 89, loss = 0.00831941\n",
      "Iteration 90, loss = 0.00830396\n",
      "Iteration 91, loss = 0.00829102\n",
      "Iteration 92, loss = 0.00827613\n",
      "Iteration 93, loss = 0.00826825\n",
      "Iteration 94, loss = 0.00825016\n",
      "Iteration 95, loss = 0.00823505\n",
      "Iteration 96, loss = 0.00822565\n",
      "Iteration 97, loss = 0.00820934\n",
      "Iteration 98, loss = 0.00819568\n",
      "Iteration 99, loss = 0.00818535\n",
      "Iteration 100, loss = 0.00817220\n",
      "Iteration 101, loss = 0.00815982\n",
      "Iteration 102, loss = 0.00815394\n",
      "Iteration 103, loss = 0.00814425\n",
      "Iteration 104, loss = 0.00812492\n",
      "Iteration 105, loss = 0.00811606\n",
      "Iteration 106, loss = 0.00811363\n",
      "Iteration 107, loss = 0.00809819\n",
      "Iteration 108, loss = 0.00808550\n",
      "Iteration 109, loss = 0.00807629\n",
      "Iteration 110, loss = 0.00807235\n",
      "Iteration 111, loss = 0.00805814\n",
      "Iteration 112, loss = 0.00804888\n",
      "Iteration 113, loss = 0.00804344\n",
      "Iteration 114, loss = 0.00803354\n",
      "Iteration 115, loss = 0.00802431\n",
      "Iteration 116, loss = 0.00801980\n",
      "Iteration 117, loss = 0.00800678\n",
      "Iteration 118, loss = 0.00799874\n",
      "Iteration 119, loss = 0.00799116\n",
      "Iteration 120, loss = 0.00798156\n",
      "Iteration 121, loss = 0.00797032\n",
      "Iteration 122, loss = 0.00796607\n",
      "Iteration 123, loss = 0.00795943\n",
      "Iteration 124, loss = 0.00795280\n",
      "Iteration 125, loss = 0.00794325\n",
      "Iteration 126, loss = 0.00794117\n",
      "Iteration 127, loss = 0.00792795\n",
      "Iteration 128, loss = 0.00792434\n",
      "Iteration 129, loss = 0.00791300\n",
      "Iteration 130, loss = 0.00790975\n",
      "Iteration 131, loss = 0.00790416\n",
      "Iteration 132, loss = 0.00789468\n",
      "Iteration 133, loss = 0.00788738\n",
      "Iteration 134, loss = 0.00788084\n",
      "Iteration 135, loss = 0.00787158\n",
      "Iteration 136, loss = 0.00786933\n",
      "Iteration 137, loss = 0.00786472\n",
      "Iteration 138, loss = 0.00785680\n",
      "Iteration 139, loss = 0.00785149\n",
      "Iteration 140, loss = 0.00784189\n",
      "Iteration 141, loss = 0.00783863\n",
      "Iteration 142, loss = 0.00783058\n",
      "Iteration 143, loss = 0.00782686\n",
      "Iteration 144, loss = 0.00781987\n",
      "Iteration 145, loss = 0.00781340\n",
      "Iteration 146, loss = 0.00780823\n",
      "Iteration 147, loss = 0.00780958\n",
      "Iteration 148, loss = 0.00779794\n",
      "Iteration 149, loss = 0.00780111\n",
      "Iteration 150, loss = 0.00779304\n",
      "Iteration 151, loss = 0.00779111\n",
      "Iteration 152, loss = 0.00778105\n",
      "Iteration 153, loss = 0.00777199\n",
      "Iteration 154, loss = 0.00776560\n",
      "Iteration 155, loss = 0.00776490\n",
      "Iteration 156, loss = 0.00776575\n",
      "Iteration 157, loss = 0.00776270\n",
      "Iteration 158, loss = 0.00775080\n",
      "Iteration 159, loss = 0.00774842\n",
      "Iteration 160, loss = 0.00774369\n",
      "Iteration 161, loss = 0.00774159\n",
      "Iteration 162, loss = 0.00773192\n",
      "Iteration 163, loss = 0.00773448\n",
      "Iteration 164, loss = 0.00772287\n",
      "Iteration 165, loss = 0.00772308\n",
      "Iteration 166, loss = 0.00772418\n",
      "Iteration 167, loss = 0.00771578\n",
      "Iteration 168, loss = 0.00770997\n",
      "Iteration 169, loss = 0.00770449\n",
      "Iteration 170, loss = 0.00771965\n",
      "Iteration 171, loss = 0.00769710\n",
      "Iteration 172, loss = 0.00769188\n",
      "Iteration 173, loss = 0.00769311\n",
      "Iteration 174, loss = 0.00769014\n",
      "Iteration 175, loss = 0.00768598\n",
      "Iteration 176, loss = 0.00767724\n",
      "Iteration 177, loss = 0.00768002\n",
      "Iteration 178, loss = 0.00767884\n",
      "Iteration 179, loss = 0.00766687\n",
      "Iteration 180, loss = 0.00766288\n",
      "Iteration 181, loss = 0.00767063\n",
      "Iteration 182, loss = 0.00765597\n",
      "Iteration 183, loss = 0.00765266\n",
      "Iteration 184, loss = 0.00765352\n",
      "Iteration 185, loss = 0.00764207\n",
      "Iteration 186, loss = 0.00764651\n",
      "Iteration 187, loss = 0.00764064\n",
      "Iteration 188, loss = 0.00764341\n",
      "Iteration 189, loss = 0.00763387\n",
      "Iteration 190, loss = 0.00764037\n",
      "Iteration 191, loss = 0.00763231\n",
      "Iteration 192, loss = 0.00762594\n",
      "Iteration 193, loss = 0.00762331\n",
      "Iteration 194, loss = 0.00762141\n",
      "Iteration 195, loss = 0.00761930\n",
      "Iteration 196, loss = 0.00761208\n",
      "Iteration 197, loss = 0.00760981\n",
      "Iteration 198, loss = 0.00760588\n",
      "Iteration 199, loss = 0.00760394\n",
      "Iteration 200, loss = 0.00760578\n",
      "Iteration 1, loss = 0.22973370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zuza/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.14487572\n",
      "Iteration 3, loss = 0.08794366\n",
      "Iteration 4, loss = 0.05348982\n",
      "Iteration 5, loss = 0.04088106\n",
      "Iteration 6, loss = 0.03437788\n",
      "Iteration 7, loss = 0.02888743\n",
      "Iteration 8, loss = 0.02451720\n",
      "Iteration 9, loss = 0.02138399\n",
      "Iteration 10, loss = 0.01934819\n",
      "Iteration 11, loss = 0.01793912\n",
      "Iteration 12, loss = 0.01695479\n",
      "Iteration 13, loss = 0.01620746\n",
      "Iteration 14, loss = 0.01558066\n",
      "Iteration 15, loss = 0.01505435\n",
      "Iteration 16, loss = 0.01459848\n",
      "Iteration 17, loss = 0.01417297\n",
      "Iteration 18, loss = 0.01379187\n",
      "Iteration 19, loss = 0.01344484\n",
      "Iteration 20, loss = 0.01313261\n",
      "Iteration 21, loss = 0.01283765\n",
      "Iteration 22, loss = 0.01255877\n",
      "Iteration 23, loss = 0.01229642\n",
      "Iteration 24, loss = 0.01205677\n",
      "Iteration 25, loss = 0.01183543\n",
      "Iteration 26, loss = 0.01162147\n",
      "Iteration 27, loss = 0.01141861\n",
      "Iteration 28, loss = 0.01124318\n",
      "Iteration 29, loss = 0.01106387\n",
      "Iteration 30, loss = 0.01090064\n",
      "Iteration 31, loss = 0.01074760\n",
      "Iteration 32, loss = 0.01061087\n",
      "Iteration 33, loss = 0.01046788\n",
      "Iteration 34, loss = 0.01034719\n",
      "Iteration 35, loss = 0.01023317\n",
      "Iteration 36, loss = 0.01011596\n",
      "Iteration 37, loss = 0.01000873\n",
      "Iteration 38, loss = 0.00990796\n",
      "Iteration 39, loss = 0.00979501\n",
      "Iteration 40, loss = 0.00970432\n",
      "Iteration 41, loss = 0.00961272\n",
      "Iteration 42, loss = 0.00953740\n",
      "Iteration 43, loss = 0.00945687\n",
      "Iteration 44, loss = 0.00938438\n",
      "Iteration 45, loss = 0.00932342\n",
      "Iteration 46, loss = 0.00926059\n",
      "Iteration 47, loss = 0.00920497\n",
      "Iteration 48, loss = 0.00913719\n",
      "Iteration 49, loss = 0.00907523\n",
      "Iteration 50, loss = 0.00902696\n",
      "Iteration 51, loss = 0.00897700\n",
      "Iteration 52, loss = 0.00893977\n",
      "Iteration 53, loss = 0.00890163\n",
      "Iteration 54, loss = 0.00885343\n",
      "Iteration 55, loss = 0.00882277\n",
      "Iteration 56, loss = 0.00878283\n",
      "Iteration 57, loss = 0.00874836\n",
      "Iteration 58, loss = 0.00871619\n",
      "Iteration 59, loss = 0.00868698\n",
      "Iteration 60, loss = 0.00865802\n",
      "Iteration 61, loss = 0.00863872\n",
      "Iteration 62, loss = 0.00861127\n",
      "Iteration 63, loss = 0.00857863\n",
      "Iteration 64, loss = 0.00855672\n",
      "Iteration 65, loss = 0.00853322\n",
      "Iteration 66, loss = 0.00850755\n",
      "Iteration 67, loss = 0.00849086\n",
      "Iteration 68, loss = 0.00847368\n",
      "Iteration 69, loss = 0.00845431\n",
      "Iteration 70, loss = 0.00842966\n",
      "Iteration 71, loss = 0.00841560\n",
      "Iteration 72, loss = 0.00839280\n",
      "Iteration 73, loss = 0.00838263\n",
      "Iteration 74, loss = 0.00835857\n",
      "Iteration 75, loss = 0.00834777\n",
      "Iteration 76, loss = 0.00833148\n",
      "Iteration 77, loss = 0.00831603\n",
      "Iteration 78, loss = 0.00830448\n",
      "Iteration 79, loss = 0.00828968\n",
      "Iteration 80, loss = 0.00826873\n",
      "Iteration 81, loss = 0.00825341\n",
      "Iteration 82, loss = 0.00824447\n",
      "Iteration 83, loss = 0.00822599\n",
      "Iteration 84, loss = 0.00821685\n",
      "Iteration 85, loss = 0.00820748\n",
      "Iteration 86, loss = 0.00819795\n",
      "Iteration 87, loss = 0.00817790\n",
      "Iteration 88, loss = 0.00816579\n",
      "Iteration 89, loss = 0.00816047\n",
      "Iteration 90, loss = 0.00814617\n",
      "Iteration 91, loss = 0.00814423\n",
      "Iteration 92, loss = 0.00812965\n",
      "Iteration 93, loss = 0.00811532\n",
      "Iteration 94, loss = 0.00810536\n",
      "Iteration 95, loss = 0.00809540\n",
      "Iteration 96, loss = 0.00808152\n",
      "Iteration 97, loss = 0.00806909\n",
      "Iteration 98, loss = 0.00806636\n",
      "Iteration 99, loss = 0.00805598\n",
      "Iteration 100, loss = 0.00805573\n",
      "Iteration 101, loss = 0.00803418\n",
      "Iteration 102, loss = 0.00802660\n",
      "Iteration 103, loss = 0.00801841\n",
      "Iteration 104, loss = 0.00801825\n",
      "Iteration 105, loss = 0.00800682\n",
      "Iteration 106, loss = 0.00799233\n",
      "Iteration 107, loss = 0.00799827\n",
      "Iteration 108, loss = 0.00797592\n",
      "Iteration 109, loss = 0.00797399\n",
      "Iteration 110, loss = 0.00796742\n",
      "Iteration 111, loss = 0.00795789\n",
      "Iteration 112, loss = 0.00794555\n",
      "Iteration 113, loss = 0.00794355\n",
      "Iteration 114, loss = 0.00793142\n",
      "Iteration 115, loss = 0.00792954\n",
      "Iteration 116, loss = 0.00792122\n",
      "Iteration 117, loss = 0.00792035\n",
      "Iteration 118, loss = 0.00790784\n",
      "Iteration 119, loss = 0.00790420\n",
      "Iteration 120, loss = 0.00789495\n",
      "Iteration 121, loss = 0.00789663\n",
      "Iteration 122, loss = 0.00787864\n",
      "Iteration 123, loss = 0.00787448\n",
      "Iteration 124, loss = 0.00786341\n",
      "Iteration 125, loss = 0.00785965\n",
      "Iteration 126, loss = 0.00785600\n",
      "Iteration 127, loss = 0.00785302\n",
      "Iteration 128, loss = 0.00784809\n",
      "Iteration 129, loss = 0.00784218\n",
      "Iteration 130, loss = 0.00783440\n",
      "Iteration 131, loss = 0.00782887\n",
      "Iteration 132, loss = 0.00781928\n",
      "Iteration 133, loss = 0.00782185\n",
      "Iteration 134, loss = 0.00781498\n",
      "Iteration 135, loss = 0.00780438\n",
      "Iteration 136, loss = 0.00779777\n",
      "Iteration 137, loss = 0.00779802\n",
      "Iteration 138, loss = 0.00779508\n",
      "Iteration 139, loss = 0.00778965\n",
      "Iteration 140, loss = 0.00778728\n",
      "Iteration 141, loss = 0.00777760\n",
      "Iteration 142, loss = 0.00776199\n",
      "Iteration 143, loss = 0.00776296\n",
      "Iteration 144, loss = 0.00775920\n",
      "Iteration 145, loss = 0.00776045\n",
      "Iteration 146, loss = 0.00774988\n",
      "Iteration 147, loss = 0.00774287\n",
      "Iteration 148, loss = 0.00774198\n",
      "Iteration 149, loss = 0.00773695\n",
      "Iteration 150, loss = 0.00772879\n",
      "Iteration 151, loss = 0.00772506\n",
      "Iteration 152, loss = 0.00771498\n",
      "Iteration 153, loss = 0.00771644\n",
      "Iteration 154, loss = 0.00771005\n",
      "Iteration 155, loss = 0.00770701\n",
      "Iteration 156, loss = 0.00770008\n",
      "Iteration 157, loss = 0.00770179\n",
      "Iteration 158, loss = 0.00768865\n",
      "Iteration 159, loss = 0.00768420\n",
      "Iteration 160, loss = 0.00768182\n",
      "Iteration 161, loss = 0.00767443\n",
      "Iteration 162, loss = 0.00767586\n",
      "Iteration 163, loss = 0.00767474\n",
      "Iteration 164, loss = 0.00766297\n",
      "Iteration 165, loss = 0.00766591\n",
      "Iteration 166, loss = 0.00765952\n",
      "Iteration 167, loss = 0.00765051\n",
      "Iteration 168, loss = 0.00764224\n",
      "Iteration 169, loss = 0.00764345\n",
      "Iteration 170, loss = 0.00763751\n",
      "Iteration 171, loss = 0.00762991\n",
      "Iteration 172, loss = 0.00762188\n",
      "Iteration 173, loss = 0.00762107\n",
      "Iteration 174, loss = 0.00761408\n",
      "Iteration 175, loss = 0.00761584\n",
      "Iteration 176, loss = 0.00760937\n",
      "Iteration 177, loss = 0.00760847\n",
      "Iteration 178, loss = 0.00759876\n",
      "Iteration 179, loss = 0.00759396\n",
      "Iteration 180, loss = 0.00759603\n",
      "Iteration 181, loss = 0.00759043\n",
      "Iteration 182, loss = 0.00758532\n",
      "Iteration 183, loss = 0.00758060\n",
      "Iteration 184, loss = 0.00757703\n",
      "Iteration 185, loss = 0.00757087\n",
      "Iteration 186, loss = 0.00756433\n",
      "Iteration 187, loss = 0.00756219\n",
      "Iteration 188, loss = 0.00755872\n",
      "Iteration 189, loss = 0.00755506\n",
      "Iteration 190, loss = 0.00755223\n",
      "Iteration 191, loss = 0.00755716\n",
      "Iteration 192, loss = 0.00754706\n",
      "Iteration 193, loss = 0.00754432\n",
      "Iteration 194, loss = 0.00753674\n",
      "Iteration 195, loss = 0.00753372\n",
      "Iteration 196, loss = 0.00752947\n",
      "Iteration 197, loss = 0.00753252\n",
      "Iteration 198, loss = 0.00752415\n",
      "Iteration 199, loss = 0.00752294\n",
      "Iteration 200, loss = 0.00751693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zuza/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.24809971\n",
      "Iteration 2, loss = 0.15006931\n",
      "Iteration 3, loss = 0.08725590\n",
      "Iteration 4, loss = 0.05026069\n",
      "Iteration 5, loss = 0.03722929\n",
      "Iteration 6, loss = 0.03117446\n",
      "Iteration 7, loss = 0.02671693\n",
      "Iteration 8, loss = 0.02334277\n",
      "Iteration 9, loss = 0.02087403\n",
      "Iteration 10, loss = 0.01913498\n",
      "Iteration 11, loss = 0.01780553\n",
      "Iteration 12, loss = 0.01671758\n",
      "Iteration 13, loss = 0.01599292\n",
      "Iteration 14, loss = 0.01542083\n",
      "Iteration 15, loss = 0.01493440\n",
      "Iteration 16, loss = 0.01448690\n",
      "Iteration 17, loss = 0.01408702\n",
      "Iteration 18, loss = 0.01371768\n",
      "Iteration 19, loss = 0.01338061\n",
      "Iteration 20, loss = 0.01306904\n",
      "Iteration 21, loss = 0.01277229\n",
      "Iteration 22, loss = 0.01249551\n",
      "Iteration 23, loss = 0.01223434\n",
      "Iteration 24, loss = 0.01199398\n",
      "Iteration 25, loss = 0.01175817\n",
      "Iteration 26, loss = 0.01154954\n",
      "Iteration 27, loss = 0.01134427\n",
      "Iteration 28, loss = 0.01115059\n",
      "Iteration 29, loss = 0.01097297\n",
      "Iteration 30, loss = 0.01080123\n",
      "Iteration 31, loss = 0.01064428\n",
      "Iteration 32, loss = 0.01049182\n",
      "Iteration 33, loss = 0.01034607\n",
      "Iteration 34, loss = 0.01021650\n",
      "Iteration 1, loss = 0.21079138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zuza/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:566: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2, loss = 0.13820019\n",
      "Iteration 3, loss = 0.08854541\n",
      "Iteration 4, loss = 0.05321682\n",
      "Iteration 5, loss = 0.04015095\n",
      "Iteration 6, loss = 0.03234320\n",
      "Iteration 7, loss = 0.02579538\n",
      "Iteration 8, loss = 0.02117407\n",
      "Iteration 9, loss = 0.01852467\n",
      "Iteration 10, loss = 0.01706404\n",
      "Iteration 11, loss = 0.01621975\n",
      "Iteration 12, loss = 0.01563441\n",
      "Iteration 13, loss = 0.01517704\n",
      "Iteration 14, loss = 0.01478297\n",
      "Iteration 15, loss = 0.01441974\n",
      "Iteration 16, loss = 0.01406860\n",
      "Iteration 17, loss = 0.01374013\n",
      "Iteration 18, loss = 0.01343560\n",
      "Iteration 19, loss = 0.01314206\n",
      "Iteration 20, loss = 0.01284558\n",
      "Iteration 21, loss = 0.01258334\n",
      "Iteration 22, loss = 0.01233195\n",
      "Iteration 23, loss = 0.01209483\n",
      "Iteration 24, loss = 0.01187386\n",
      "Iteration 25, loss = 0.01166110\n",
      "Iteration 26, loss = 0.01146660\n",
      "Iteration 27, loss = 0.01128173\n",
      "Iteration 28, loss = 0.01110880\n",
      "Iteration 29, loss = 0.01095331\n",
      "Iteration 30, loss = 0.01078683\n",
      "Iteration 31, loss = 0.01064816\n",
      "Iteration 32, loss = 0.01050830\n",
      "Iteration 33, loss = 0.01037783\n",
      "Iteration 34, loss = 0.01025972\n",
      "Iteration 35, loss = 0.01014414\n",
      "Iteration 36, loss = 0.01004061\n",
      "Iteration 37, loss = 0.00994299\n",
      "Iteration 38, loss = 0.00985351\n",
      "Iteration 39, loss = 0.00975551\n",
      "Iteration 40, loss = 0.00967556\n",
      "Iteration 41, loss = 0.00959017\n",
      "Iteration 42, loss = 0.00952128\n",
      "Iteration 43, loss = 0.00945357\n",
      "Iteration 44, loss = 0.00938320\n",
      "Iteration 45, loss = 0.00932851\n",
      "Iteration 46, loss = 0.00927025\n",
      "Iteration 47, loss = 0.00920536\n",
      "Iteration 48, loss = 0.00916308\n",
      "Iteration 49, loss = 0.00911038\n",
      "Iteration 50, loss = 0.00907439\n",
      "Iteration 51, loss = 0.00902313\n",
      "Iteration 52, loss = 0.00897815\n",
      "Iteration 53, loss = 0.00893863\n",
      "Iteration 54, loss = 0.00889916\n",
      "Iteration 55, loss = 0.00886562\n",
      "Iteration 56, loss = 0.00883597\n",
      "Iteration 57, loss = 0.00879644\n",
      "Iteration 58, loss = 0.00876683\n",
      "Iteration 59, loss = 0.00873852\n",
      "Iteration 60, loss = 0.00870574\n",
      "Iteration 61, loss = 0.00867395\n",
      "Iteration 62, loss = 0.00865486\n",
      "Iteration 63, loss = 0.00863243\n",
      "Iteration 64, loss = 0.00860470\n",
      "Iteration 65, loss = 0.00857767\n",
      "Iteration 66, loss = 0.00855790\n",
      "Iteration 67, loss = 0.00853001\n",
      "Iteration 68, loss = 0.00851224\n",
      "Iteration 69, loss = 0.00849711\n",
      "Iteration 70, loss = 0.00848818\n",
      "Iteration 71, loss = 0.00845597\n",
      "Iteration 72, loss = 0.00843468\n",
      "Iteration 73, loss = 0.00842802\n",
      "Iteration 74, loss = 0.00840008\n",
      "Iteration 75, loss = 0.00838205\n",
      "Iteration 76, loss = 0.00837270\n",
      "Iteration 77, loss = 0.00834633\n",
      "Iteration 78, loss = 0.00833130\n",
      "Iteration 79, loss = 0.00831565\n",
      "Iteration 80, loss = 0.00830201\n",
      "Iteration 81, loss = 0.00829476\n",
      "Iteration 82, loss = 0.00828306\n",
      "Iteration 83, loss = 0.00826237\n",
      "Iteration 84, loss = 0.00824867\n",
      "Iteration 85, loss = 0.00824633\n",
      "Iteration 86, loss = 0.00822837\n",
      "Iteration 87, loss = 0.00821250\n",
      "Iteration 88, loss = 0.00820132\n",
      "Iteration 89, loss = 0.00819347\n",
      "Iteration 90, loss = 0.00817970\n",
      "Iteration 91, loss = 0.00816849\n",
      "Iteration 92, loss = 0.00816712\n",
      "Iteration 93, loss = 0.00815832\n",
      "Iteration 94, loss = 0.00814048\n",
      "Iteration 95, loss = 0.00812912\n",
      "Iteration 96, loss = 0.00812147\n",
      "Iteration 97, loss = 0.00812271\n",
      "Iteration 98, loss = 0.00810647\n",
      "Iteration 99, loss = 0.00810295\n",
      "Iteration 100, loss = 0.00808904\n",
      "Iteration 101, loss = 0.00808060\n",
      "Iteration 102, loss = 0.00807073\n",
      "Iteration 103, loss = 0.00806504\n",
      "Iteration 104, loss = 0.00805237\n",
      "Iteration 105, loss = 0.00806183\n",
      "Iteration 106, loss = 0.00804465\n",
      "Iteration 107, loss = 0.00802968\n",
      "Iteration 108, loss = 0.00802621\n",
      "Iteration 109, loss = 0.00802453\n",
      "Iteration 110, loss = 0.00801262\n",
      "Iteration 111, loss = 0.00800757\n",
      "Iteration 112, loss = 0.00800541\n",
      "Iteration 113, loss = 0.00799472\n",
      "Iteration 114, loss = 0.00799402\n",
      "Iteration 115, loss = 0.00797979\n",
      "Iteration 116, loss = 0.00798020\n",
      "Iteration 117, loss = 0.00796742\n",
      "Iteration 118, loss = 0.00796219\n",
      "Iteration 119, loss = 0.00795902\n",
      "Iteration 120, loss = 0.00795359\n",
      "Iteration 121, loss = 0.00795140\n",
      "Iteration 122, loss = 0.00794293\n",
      "Iteration 123, loss = 0.00793342\n",
      "Iteration 124, loss = 0.00793979\n",
      "Iteration 125, loss = 0.00792235\n",
      "Iteration 126, loss = 0.00792151\n",
      "Iteration 127, loss = 0.00791543\n",
      "Iteration 128, loss = 0.00790605\n",
      "Iteration 129, loss = 0.00790467\n",
      "Iteration 130, loss = 0.00789579\n",
      "Iteration 131, loss = 0.00789869\n",
      "Iteration 132, loss = 0.00789039\n",
      "Iteration 133, loss = 0.00788023\n",
      "Iteration 134, loss = 0.00787820\n",
      "Iteration 135, loss = 0.00787265\n",
      "Iteration 136, loss = 0.00786909\n",
      "Iteration 137, loss = 0.00786152\n",
      "Iteration 138, loss = 0.00786265\n",
      "Iteration 139, loss = 0.00785479\n",
      "Iteration 140, loss = 0.00784584\n",
      "Iteration 141, loss = 0.00784761\n",
      "Iteration 142, loss = 0.00783945\n",
      "Iteration 143, loss = 0.00783862\n",
      "Iteration 144, loss = 0.00783608\n",
      "Iteration 145, loss = 0.00783684\n",
      "Iteration 146, loss = 0.00782178\n",
      "Iteration 147, loss = 0.00782084\n",
      "Iteration 148, loss = 0.00781264\n",
      "Iteration 149, loss = 0.00780841\n",
      "Iteration 150, loss = 0.00781020\n",
      "Iteration 151, loss = 0.00780623\n",
      "Iteration 152, loss = 0.00780111\n",
      "Iteration 153, loss = 0.00780048\n",
      "Iteration 154, loss = 0.00779084\n",
      "Iteration 155, loss = 0.00778388\n",
      "Iteration 156, loss = 0.00778769\n",
      "Iteration 157, loss = 0.00779155\n",
      "Iteration 158, loss = 0.00778164\n",
      "Iteration 159, loss = 0.00776751\n",
      "Iteration 160, loss = 0.00776708\n",
      "Iteration 161, loss = 0.00776299\n",
      "Iteration 162, loss = 0.00775968\n",
      "Iteration 163, loss = 0.00776106\n",
      "Iteration 164, loss = 0.00775621\n",
      "Iteration 165, loss = 0.00775318\n",
      "Iteration 166, loss = 0.00774439\n",
      "Iteration 167, loss = 0.00774706\n",
      "Iteration 168, loss = 0.00774921\n",
      "Iteration 169, loss = 0.00774107\n",
      "Iteration 170, loss = 0.00773377\n",
      "Iteration 171, loss = 0.00773813\n",
      "Iteration 172, loss = 0.00772800\n",
      "Iteration 173, loss = 0.00772848\n",
      "Iteration 174, loss = 0.00772863\n",
      "Iteration 175, loss = 0.00772170\n",
      "Iteration 176, loss = 0.00773496\n",
      "Iteration 177, loss = 0.00772026\n",
      "Iteration 178, loss = 0.00771614\n",
      "Iteration 179, loss = 0.00770735\n",
      "Iteration 180, loss = 0.00770661\n",
      "Iteration 181, loss = 0.00770494\n",
      "Iteration 182, loss = 0.00770446\n",
      "Iteration 183, loss = 0.00770376\n",
      "Iteration 184, loss = 0.00770276\n",
      "Iteration 185, loss = 0.00769775\n",
      "Iteration 186, loss = 0.00770107\n",
      "Iteration 187, loss = 0.00768792\n",
      "Iteration 188, loss = 0.00769193\n",
      "Iteration 189, loss = 0.00768461\n",
      "Iteration 190, loss = 0.00768349\n",
      "Iteration 191, loss = 0.00768084\n",
      "Iteration 192, loss = 0.00767659\n",
      "Iteration 193, loss = 0.00767320\n",
      "Iteration 194, loss = 0.00767549\n",
      "Iteration 195, loss = 0.00767213\n",
      "Iteration 196, loss = 0.00767282\n",
      "Iteration 197, loss = 0.00767779\n",
      "Iteration 198, loss = 0.00766337\n",
      "Iteration 199, loss = 0.00766557\n",
      "Iteration 200, loss = 0.00765840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zuza/anaconda3/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.20883892\n",
      "Iteration 2, loss = 0.14322016\n",
      "Iteration 3, loss = 0.09034773\n",
      "Iteration 4, loss = 0.04804076\n",
      "Iteration 5, loss = 0.03414730\n",
      "Iteration 6, loss = 0.02648691\n",
      "Iteration 7, loss = 0.02177017\n",
      "Iteration 8, loss = 0.01919036\n",
      "Iteration 9, loss = 0.01776608\n",
      "Iteration 10, loss = 0.01686904\n",
      "Iteration 11, loss = 0.01620798\n",
      "Iteration 12, loss = 0.01564702\n",
      "Iteration 13, loss = 0.01515845\n",
      "Iteration 14, loss = 0.01471247\n",
      "Iteration 15, loss = 0.01427376\n",
      "Iteration 16, loss = 0.01388459\n",
      "Iteration 17, loss = 0.01350293\n",
      "Iteration 18, loss = 0.01315823\n",
      "Iteration 19, loss = 0.01282254\n",
      "Iteration 20, loss = 0.01251797\n",
      "Iteration 21, loss = 0.01222741\n",
      "Iteration 22, loss = 0.01196107\n",
      "Iteration 23, loss = 0.01171597\n",
      "Iteration 24, loss = 0.01147935\n",
      "Iteration 25, loss = 0.01126102\n",
      "Iteration 26, loss = 0.01107192\n",
      "Iteration 27, loss = 0.01087297\n",
      "Iteration 28, loss = 0.01069313\n",
      "Iteration 29, loss = 0.01053391\n",
      "Iteration 30, loss = 0.01037223\n",
      "Iteration 31, loss = 0.01023310\n",
      "Iteration 32, loss = 0.01010669\n",
      "Iteration 33, loss = 0.00997471\n",
      "Iteration 34, loss = 0.00986225\n",
      "Iteration 35, loss = 0.00976080\n",
      "Iteration 36, loss = 0.00966078\n",
      "Iteration 37, loss = 0.00956394\n",
      "Iteration 38, loss = 0.00948079\n",
      "Iteration 39, loss = 0.00939463\n",
      "Iteration 40, loss = 0.00931490\n",
      "Iteration 41, loss = 0.00923672\n",
      "Iteration 42, loss = 0.00916828\n",
      "Iteration 43, loss = 0.00910634\n",
      "Iteration 44, loss = 0.00905134\n",
      "Iteration 45, loss = 0.00901254\n",
      "Iteration 46, loss = 0.00895524\n",
      "Iteration 47, loss = 0.00890800\n",
      "Iteration 48, loss = 0.00885920\n",
      "Iteration 49, loss = 0.00882259\n",
      "Iteration 50, loss = 0.00879851\n",
      "Iteration 51, loss = 0.00875222\n",
      "Iteration 52, loss = 0.00871638\n",
      "Iteration 53, loss = 0.00868098\n",
      "Iteration 54, loss = 0.00863728\n",
      "Iteration 55, loss = 0.00859827\n",
      "Iteration 56, loss = 0.00858640\n",
      "Iteration 57, loss = 0.00854757\n",
      "Iteration 58, loss = 0.00853016\n",
      "Iteration 59, loss = 0.00849353\n",
      "Iteration 60, loss = 0.00847327\n",
      "Iteration 61, loss = 0.00845102\n",
      "Iteration 62, loss = 0.00843347\n",
      "Iteration 63, loss = 0.00840411\n",
      "Iteration 64, loss = 0.00839355\n",
      "Iteration 65, loss = 0.00837105\n",
      "Iteration 66, loss = 0.00835164\n",
      "Iteration 67, loss = 0.00833278\n",
      "Iteration 68, loss = 0.00832923\n",
      "Iteration 69, loss = 0.00829659\n",
      "Iteration 70, loss = 0.00828040\n",
      "Iteration 71, loss = 0.00826137\n",
      "Iteration 72, loss = 0.00825415\n",
      "Iteration 73, loss = 0.00823933\n",
      "Iteration 74, loss = 0.00822137\n",
      "Iteration 75, loss = 0.00820163\n",
      "Iteration 76, loss = 0.00819266\n",
      "Iteration 77, loss = 0.00817887\n",
      "Iteration 78, loss = 0.00815960\n",
      "Iteration 79, loss = 0.00815234\n",
      "Iteration 80, loss = 0.00812989\n",
      "Iteration 81, loss = 0.00812336\n",
      "Iteration 82, loss = 0.00811250\n",
      "Iteration 83, loss = 0.00810154\n",
      "Iteration 84, loss = 0.00808667\n",
      "Iteration 85, loss = 0.00807319\n",
      "Iteration 86, loss = 0.00806534\n",
      "Iteration 87, loss = 0.00805132\n",
      "Iteration 88, loss = 0.00804550\n",
      "Iteration 89, loss = 0.00802846\n",
      "Iteration 90, loss = 0.00801593\n",
      "Iteration 91, loss = 0.00800691\n",
      "Iteration 92, loss = 0.00799398\n",
      "Iteration 93, loss = 0.00798382\n",
      "Iteration 94, loss = 0.00797778\n",
      "Iteration 95, loss = 0.00796799\n",
      "Iteration 96, loss = 0.00795941\n",
      "Iteration 97, loss = 0.00794556\n",
      "Iteration 98, loss = 0.00793495\n",
      "Iteration 99, loss = 0.00793230\n",
      "Iteration 100, loss = 0.00791897\n",
      "Iteration 101, loss = 0.00790852\n",
      "Iteration 102, loss = 0.00790085\n",
      "Iteration 103, loss = 0.00789886\n",
      "Iteration 104, loss = 0.00788895\n",
      "Iteration 105, loss = 0.00787320\n",
      "Iteration 106, loss = 0.00787450\n",
      "Iteration 107, loss = 0.00786152\n",
      "Iteration 108, loss = 0.00785213\n",
      "Iteration 109, loss = 0.00784664\n",
      "Iteration 110, loss = 0.00783412\n",
      "Iteration 111, loss = 0.00783290\n",
      "Iteration 112, loss = 0.00782261\n",
      "Iteration 113, loss = 0.00781528\n",
      "Iteration 114, loss = 0.00781329\n",
      "Iteration 115, loss = 0.00782480\n",
      "Iteration 116, loss = 0.00780674\n",
      "Iteration 117, loss = 0.00778907\n",
      "Iteration 118, loss = 0.00778524\n",
      "Iteration 119, loss = 0.00777990\n",
      "Iteration 120, loss = 0.00778151\n",
      "Iteration 121, loss = 0.00777231\n",
      "Iteration 122, loss = 0.00776487\n",
      "Iteration 123, loss = 0.00775839\n",
      "Iteration 124, loss = 0.00775077\n",
      "Iteration 125, loss = 0.00774773\n",
      "Iteration 126, loss = 0.00774142\n",
      "Iteration 127, loss = 0.00774048\n",
      "Iteration 128, loss = 0.00773289\n",
      "Iteration 129, loss = 0.00772383\n",
      "Iteration 130, loss = 0.00771917\n",
      "Iteration 131, loss = 0.00772035\n",
      "Iteration 132, loss = 0.00771069\n",
      "Iteration 133, loss = 0.00770645\n",
      "Iteration 134, loss = 0.00770484\n",
      "Iteration 135, loss = 0.00770214\n",
      "Iteration 136, loss = 0.00770028\n",
      "Iteration 137, loss = 0.00769146\n",
      "Iteration 138, loss = 0.00768339\n",
      "Iteration 139, loss = 0.00768206\n",
      "Iteration 140, loss = 0.00767622\n",
      "Iteration 141, loss = 0.00766992\n",
      "Iteration 142, loss = 0.00766390\n",
      "Iteration 143, loss = 0.00766800\n",
      "Iteration 144, loss = 0.00765879\n",
      "Iteration 145, loss = 0.00765779\n",
      "Iteration 146, loss = 0.00765510\n",
      "Iteration 147, loss = 0.00765418\n",
      "Iteration 148, loss = 0.00765070\n",
      "Iteration 149, loss = 0.00764150\n",
      "Iteration 150, loss = 0.00764350\n",
      "Iteration 151, loss = 0.00763811\n",
      "Iteration 152, loss = 0.00763359\n",
      "Iteration 153, loss = 0.00763065\n",
      "Iteration 154, loss = 0.00763137\n",
      "Iteration 155, loss = 0.00762757\n",
      "Iteration 156, loss = 0.00762031\n",
      "Iteration 157, loss = 0.00761788\n",
      "Iteration 158, loss = 0.00761414\n",
      "Iteration 159, loss = 0.00761581\n",
      "Iteration 160, loss = 0.00761103\n",
      "Iteration 161, loss = 0.00760634\n",
      "Iteration 162, loss = 0.00760706\n",
      "Iteration 163, loss = 0.00760826\n",
      "Iteration 164, loss = 0.00760617\n",
      "Iteration 165, loss = 0.00760214\n",
      "Iteration 166, loss = 0.00760058\n",
      "Iteration 167, loss = 0.00759740\n",
      "Iteration 168, loss = 0.00759424\n",
      "Iteration 169, loss = 0.00759313\n",
      "Iteration 170, loss = 0.00759697\n",
      "Iteration 171, loss = 0.00759135\n",
      "Iteration 172, loss = 0.00759284\n",
      "Iteration 173, loss = 0.00759694\n",
      "Iteration 174, loss = 0.00758592\n",
      "Iteration 175, loss = 0.00757945\n",
      "Iteration 176, loss = 0.00757851\n",
      "Iteration 177, loss = 0.00757355\n",
      "Iteration 178, loss = 0.00757287\n",
      "Iteration 179, loss = 0.00757343\n",
      "Iteration 180, loss = 0.00758117\n",
      "Iteration 181, loss = 0.00757589\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.19937136\n",
      "Iteration 2, loss = 0.10559576\n",
      "Iteration 3, loss = 0.05409266\n",
      "Iteration 4, loss = 0.03721305\n",
      "Iteration 5, loss = 0.02906016\n",
      "Iteration 6, loss = 0.02366315\n",
      "Iteration 7, loss = 0.02069639\n",
      "Iteration 8, loss = 0.01912430\n",
      "Iteration 9, loss = 0.01806893\n",
      "Iteration 10, loss = 0.01721888\n",
      "Iteration 11, loss = 0.01648660\n",
      "Iteration 12, loss = 0.01583502\n",
      "Iteration 13, loss = 0.01522903\n",
      "Iteration 14, loss = 0.01468307\n",
      "Iteration 15, loss = 0.01417337\n",
      "Iteration 16, loss = 0.01371132\n",
      "Iteration 17, loss = 0.01328013\n",
      "Iteration 18, loss = 0.01286968\n",
      "Iteration 19, loss = 0.01249499\n",
      "Iteration 20, loss = 0.01215172\n",
      "Iteration 21, loss = 0.01183167\n",
      "Iteration 22, loss = 0.01154429\n",
      "Iteration 23, loss = 0.01126716\n",
      "Iteration 24, loss = 0.01102785\n",
      "Iteration 25, loss = 0.01078480\n",
      "Iteration 26, loss = 0.01057510\n",
      "Iteration 27, loss = 0.01037566\n",
      "Iteration 28, loss = 0.01019887\n",
      "Iteration 29, loss = 0.01002896\n",
      "Iteration 30, loss = 0.00988123\n",
      "Iteration 31, loss = 0.00974076\n",
      "Iteration 32, loss = 0.00961603\n",
      "Iteration 33, loss = 0.00948753\n",
      "Iteration 34, loss = 0.00937968\n",
      "Iteration 35, loss = 0.00928359\n",
      "Iteration 36, loss = 0.00919085\n",
      "Iteration 37, loss = 0.00910718\n",
      "Iteration 38, loss = 0.00902333\n",
      "Iteration 39, loss = 0.00895591\n",
      "Iteration 40, loss = 0.00889854\n",
      "Iteration 41, loss = 0.00883448\n",
      "Iteration 42, loss = 0.00877320\n",
      "Iteration 43, loss = 0.00871736\n",
      "Iteration 44, loss = 0.00868119\n",
      "Iteration 45, loss = 0.00863634\n",
      "Iteration 46, loss = 0.00858843\n",
      "Iteration 47, loss = 0.00854823\n",
      "Iteration 48, loss = 0.00851186\n",
      "Iteration 49, loss = 0.00848452\n",
      "Iteration 50, loss = 0.00844805\n",
      "Iteration 51, loss = 0.00842622\n",
      "Iteration 52, loss = 0.00840599\n",
      "Iteration 53, loss = 0.00838220\n",
      "Iteration 54, loss = 0.00834378\n",
      "Iteration 55, loss = 0.00832552\n",
      "Iteration 56, loss = 0.00830296\n",
      "Iteration 57, loss = 0.00828647\n",
      "Iteration 58, loss = 0.00826362\n",
      "Iteration 59, loss = 0.00824540\n",
      "Iteration 60, loss = 0.00822867\n",
      "Iteration 61, loss = 0.00821668\n",
      "Iteration 62, loss = 0.00820082\n",
      "Iteration 63, loss = 0.00818943\n",
      "Iteration 64, loss = 0.00817074\n",
      "Iteration 65, loss = 0.00816681\n",
      "Iteration 66, loss = 0.00813884\n",
      "Iteration 67, loss = 0.00812396\n",
      "Iteration 68, loss = 0.00811556\n",
      "Iteration 69, loss = 0.00810377\n",
      "Iteration 70, loss = 0.00809317\n",
      "Iteration 71, loss = 0.00807744\n",
      "Iteration 72, loss = 0.00807043\n",
      "Iteration 73, loss = 0.00806271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 74, loss = 0.00805070\n",
      "Iteration 75, loss = 0.00804662\n",
      "Iteration 76, loss = 0.00803225\n",
      "Iteration 77, loss = 0.00802292\n",
      "Iteration 78, loss = 0.00802574\n",
      "Iteration 79, loss = 0.00799930\n",
      "Iteration 80, loss = 0.00798707\n",
      "Iteration 81, loss = 0.00798085\n",
      "Iteration 82, loss = 0.00797777\n",
      "Iteration 83, loss = 0.00797355\n",
      "Iteration 84, loss = 0.00795643\n",
      "Iteration 85, loss = 0.00795129\n",
      "Iteration 86, loss = 0.00794387\n",
      "Iteration 87, loss = 0.00793379\n",
      "Iteration 88, loss = 0.00793786\n",
      "Iteration 89, loss = 0.00792244\n",
      "Iteration 90, loss = 0.00791601\n",
      "Iteration 91, loss = 0.00791253\n",
      "Iteration 92, loss = 0.00790549\n",
      "Iteration 93, loss = 0.00790150\n",
      "Iteration 94, loss = 0.00788985\n",
      "Iteration 95, loss = 0.00787912\n",
      "Iteration 96, loss = 0.00786998\n",
      "Iteration 97, loss = 0.00786296\n",
      "Iteration 98, loss = 0.00785625\n",
      "Iteration 99, loss = 0.00785131\n",
      "Iteration 100, loss = 0.00784164\n",
      "Iteration 101, loss = 0.00783989\n",
      "Iteration 102, loss = 0.00783779\n",
      "Iteration 103, loss = 0.00782562\n",
      "Iteration 104, loss = 0.00783338\n",
      "Iteration 105, loss = 0.00781596\n",
      "Iteration 106, loss = 0.00781584\n",
      "Iteration 107, loss = 0.00780864\n",
      "Iteration 108, loss = 0.00779283\n",
      "Iteration 109, loss = 0.00778870\n",
      "Iteration 110, loss = 0.00778548\n",
      "Iteration 111, loss = 0.00778817\n",
      "Iteration 112, loss = 0.00777148\n",
      "Iteration 113, loss = 0.00775733\n",
      "Iteration 114, loss = 0.00774639\n",
      "Iteration 115, loss = 0.00776200\n",
      "Iteration 116, loss = 0.00775338\n",
      "Iteration 117, loss = 0.00773769\n",
      "Iteration 118, loss = 0.00772845\n",
      "Iteration 119, loss = 0.00772538\n",
      "Iteration 120, loss = 0.00773242\n",
      "Iteration 121, loss = 0.00771574\n",
      "Iteration 122, loss = 0.00770414\n",
      "Iteration 123, loss = 0.00770715\n",
      "Iteration 124, loss = 0.00770077\n",
      "Iteration 125, loss = 0.00770000\n",
      "Iteration 126, loss = 0.00769026\n",
      "Iteration 127, loss = 0.00768090\n",
      "Iteration 128, loss = 0.00768041\n",
      "Iteration 129, loss = 0.00767351\n",
      "Iteration 130, loss = 0.00768353\n",
      "Iteration 131, loss = 0.00767207\n",
      "Iteration 132, loss = 0.00764445\n",
      "Iteration 133, loss = 0.00764185\n",
      "Iteration 134, loss = 0.00763146\n",
      "Iteration 135, loss = 0.00762831\n",
      "Iteration 136, loss = 0.00763687\n",
      "Iteration 137, loss = 0.00761806\n",
      "Iteration 138, loss = 0.00761744\n",
      "Iteration 139, loss = 0.00761200\n",
      "Iteration 140, loss = 0.00759812\n",
      "Iteration 141, loss = 0.00760105\n",
      "Iteration 142, loss = 0.00760004\n",
      "Iteration 143, loss = 0.00759355\n",
      "Iteration 144, loss = 0.00757476\n",
      "Iteration 145, loss = 0.00758418\n",
      "Iteration 146, loss = 0.00757231\n",
      "Iteration 147, loss = 0.00756083\n",
      "Iteration 148, loss = 0.00756018\n",
      "Iteration 149, loss = 0.00755442\n",
      "Iteration 150, loss = 0.00755183\n",
      "Iteration 151, loss = 0.00755150\n",
      "Iteration 152, loss = 0.00754906\n",
      "Iteration 153, loss = 0.00755348\n",
      "Iteration 154, loss = 0.00754125\n",
      "Iteration 155, loss = 0.00753052\n",
      "Iteration 156, loss = 0.00753407\n",
      "Iteration 157, loss = 0.00752166\n",
      "Iteration 158, loss = 0.00752122\n",
      "Iteration 159, loss = 0.00752167\n",
      "Iteration 160, loss = 0.00750915\n",
      "Iteration 161, loss = 0.00751716\n",
      "Iteration 162, loss = 0.00750627\n",
      "Iteration 163, loss = 0.00750726\n",
      "Iteration 164, loss = 0.00749014\n",
      "Iteration 165, loss = 0.00749075\n",
      "Iteration 166, loss = 0.00748878\n",
      "Iteration 167, loss = 0.00748424\n",
      "Iteration 168, loss = 0.00749042\n",
      "Iteration 169, loss = 0.00748471\n",
      "Iteration 170, loss = 0.00748501\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 0.23524234\n",
      "Iteration 2, loss = 0.14075973\n",
      "Iteration 3, loss = 0.07887508\n",
      "Iteration 4, loss = 0.04535845\n",
      "Iteration 5, loss = 0.03402525\n",
      "Iteration 6, loss = 0.02745932\n",
      "Iteration 7, loss = 0.02292521\n",
      "Iteration 8, loss = 0.02010785\n",
      "Iteration 9, loss = 0.01848850\n",
      "Iteration 10, loss = 0.01742881\n",
      "Iteration 11, loss = 0.01664338\n",
      "Iteration 12, loss = 0.01600675\n",
      "Iteration 13, loss = 0.01546163\n",
      "Iteration 14, loss = 0.01499105\n",
      "Iteration 15, loss = 0.01454582\n",
      "Iteration 16, loss = 0.01413805\n",
      "Iteration 17, loss = 0.01377480\n",
      "Iteration 18, loss = 0.01341186\n",
      "Iteration 19, loss = 0.01308565\n",
      "Iteration 20, loss = 0.01278550\n",
      "Iteration 21, loss = 0.01249018\n",
      "Iteration 22, loss = 0.01222011\n",
      "Iteration 23, loss = 0.01197135\n",
      "Iteration 24, loss = 0.01173281\n",
      "Iteration 25, loss = 0.01150799\n",
      "Iteration 26, loss = 0.01129486\n",
      "Iteration 27, loss = 0.01109936\n",
      "Iteration 28, loss = 0.01091306\n",
      "Iteration 29, loss = 0.01074002\n",
      "Iteration 30, loss = 0.01057895\n",
      "Iteration 31, loss = 0.01043239\n",
      "Iteration 32, loss = 0.01028850\n",
      "Iteration 33, loss = 0.01015608\n",
      "Iteration 34, loss = 0.01003322\n",
      "Iteration 35, loss = 0.00991987\n",
      "Iteration 36, loss = 0.00980826\n",
      "Iteration 37, loss = 0.00970910\n",
      "Iteration 38, loss = 0.00962848\n",
      "Iteration 39, loss = 0.00952558\n",
      "Iteration 40, loss = 0.00944857\n",
      "Iteration 41, loss = 0.00936670\n",
      "Iteration 42, loss = 0.00929292\n",
      "Iteration 43, loss = 0.00924287\n",
      "Iteration 44, loss = 0.00916869\n",
      "Iteration 45, loss = 0.00911806\n",
      "Iteration 46, loss = 0.00905675\n",
      "Iteration 47, loss = 0.00900196\n",
      "Iteration 48, loss = 0.00895365\n",
      "Iteration 49, loss = 0.00891242\n",
      "Iteration 50, loss = 0.00886500\n",
      "Iteration 51, loss = 0.00882964\n",
      "Iteration 52, loss = 0.00879756\n",
      "Iteration 53, loss = 0.00875351\n",
      "Iteration 54, loss = 0.00872359\n",
      "Iteration 55, loss = 0.00869578\n",
      "Iteration 56, loss = 0.00866560\n",
      "Iteration 57, loss = 0.00865480\n",
      "Iteration 58, loss = 0.00861257\n",
      "Iteration 59, loss = 0.00858909\n",
      "Iteration 60, loss = 0.00856090\n",
      "Iteration 61, loss = 0.00854088\n",
      "Iteration 62, loss = 0.00852266\n",
      "Iteration 63, loss = 0.00849643\n",
      "Iteration 64, loss = 0.00847933\n",
      "Iteration 65, loss = 0.00846255\n",
      "Iteration 66, loss = 0.00844510\n",
      "Iteration 67, loss = 0.00842504\n",
      "Iteration 68, loss = 0.00841568\n",
      "Iteration 69, loss = 0.00839676\n",
      "Iteration 70, loss = 0.00838679\n",
      "Iteration 71, loss = 0.00837998\n",
      "Iteration 72, loss = 0.00834688\n",
      "Iteration 73, loss = 0.00833302\n",
      "Iteration 74, loss = 0.00832943\n",
      "Iteration 75, loss = 0.00831130\n",
      "Iteration 76, loss = 0.00829945\n",
      "Iteration 77, loss = 0.00829725\n",
      "Iteration 78, loss = 0.00827810\n",
      "Iteration 79, loss = 0.00826747\n",
      "Iteration 80, loss = 0.00826505\n",
      "Iteration 81, loss = 0.00824649\n",
      "Iteration 82, loss = 0.00823430\n",
      "Iteration 83, loss = 0.00822374\n",
      "Iteration 84, loss = 0.00821424\n",
      "Iteration 85, loss = 0.00820367\n",
      "Iteration 86, loss = 0.00819676\n",
      "Iteration 87, loss = 0.00819784\n",
      "Iteration 88, loss = 0.00817976\n",
      "Iteration 89, loss = 0.00817208\n",
      "Iteration 90, loss = 0.00816109\n",
      "Iteration 91, loss = 0.00815756\n",
      "Iteration 92, loss = 0.00814595\n",
      "Iteration 93, loss = 0.00814863\n",
      "Iteration 94, loss = 0.00813008\n",
      "Iteration 95, loss = 0.00812246\n",
      "Iteration 96, loss = 0.00812512\n",
      "Iteration 97, loss = 0.00810780\n",
      "Iteration 98, loss = 0.00810651\n",
      "Iteration 99, loss = 0.00810005\n",
      "Iteration 100, loss = 0.00809797\n",
      "Iteration 101, loss = 0.00809554\n",
      "Iteration 102, loss = 0.00808651\n",
      "Iteration 103, loss = 0.00807108\n",
      "Iteration 104, loss = 0.00806734\n",
      "Iteration 105, loss = 0.00807280\n",
      "Iteration 106, loss = 0.00806634\n",
      "Iteration 107, loss = 0.00805482\n",
      "Iteration 108, loss = 0.00804989\n",
      "Iteration 109, loss = 0.00804708\n",
      "Iteration 110, loss = 0.00804321\n",
      "Iteration 111, loss = 0.00803454\n",
      "Iteration 112, loss = 0.00803146\n",
      "Iteration 113, loss = 0.00802860\n",
      "Iteration 114, loss = 0.00802358\n",
      "Iteration 115, loss = 0.00801954\n",
      "Iteration 116, loss = 0.00802246\n",
      "Iteration 117, loss = 0.00800852\n",
      "Iteration 118, loss = 0.00800188\n",
      "Iteration 119, loss = 0.00800035\n",
      "Iteration 120, loss = 0.00799695\n",
      "Iteration 121, loss = 0.00799507\n",
      "Iteration 122, loss = 0.00799728\n",
      "Iteration 123, loss = 0.00798344\n",
      "Iteration 124, loss = 0.00798670\n",
      "Iteration 125, loss = 0.00798354\n",
      "Iteration 126, loss = 0.00797400\n",
      "Iteration 127, loss = 0.00796993\n",
      "Iteration 128, loss = 0.00796615\n",
      "Iteration 129, loss = 0.00796367\n",
      "Iteration 130, loss = 0.00795927\n",
      "Iteration 131, loss = 0.00795891\n",
      "Iteration 132, loss = 0.00795401\n",
      "Iteration 133, loss = 0.00794711\n",
      "Iteration 134, loss = 0.00794440\n",
      "Iteration 135, loss = 0.00794729\n",
      "Iteration 136, loss = 0.00794411\n",
      "Iteration 137, loss = 0.00794681\n",
      "Iteration 138, loss = 0.00794149\n",
      "Iteration 139, loss = 0.00793712\n",
      "Iteration 140, loss = 0.00793403\n",
      "Iteration 141, loss = 0.00793534\n",
      "Iteration 142, loss = 0.00792810\n",
      "Iteration 143, loss = 0.00792045\n",
      "Iteration 144, loss = 0.00792122\n",
      "Iteration 145, loss = 0.00792068\n",
      "Iteration 146, loss = 0.00791927\n",
      "Iteration 147, loss = 0.00791458\n",
      "Iteration 148, loss = 0.00790350\n",
      "Iteration 149, loss = 0.00790946\n",
      "Iteration 150, loss = 0.00790172\n",
      "Iteration 151, loss = 0.00790105\n",
      "Iteration 152, loss = 0.00790095\n",
      "Iteration 153, loss = 0.00790190\n",
      "Iteration 154, loss = 0.00789811\n",
      "Iteration 155, loss = 0.00789705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 156, loss = 0.00788521\n",
      "Iteration 157, loss = 0.00789475\n",
      "Iteration 158, loss = 0.00788244\n",
      "Iteration 159, loss = 0.00787459\n",
      "Iteration 160, loss = 0.00787955\n",
      "Iteration 161, loss = 0.00787592\n",
      "Iteration 162, loss = 0.00787001\n",
      "Iteration 163, loss = 0.00787010\n",
      "Iteration 164, loss = 0.00786447\n",
      "Iteration 165, loss = 0.00787267\n",
      "Iteration 166, loss = 0.00786009\n",
      "Iteration 167, loss = 0.00786472\n",
      "Iteration 168, loss = 0.00786226\n",
      "Iteration 169, loss = 0.00785422\n",
      "Iteration 170, loss = 0.00785267\n",
      "Iteration 171, loss = 0.00784831\n",
      "Iteration 172, loss = 0.00784488\n",
      "Iteration 173, loss = 0.00783630\n",
      "Iteration 174, loss = 0.00783981\n",
      "Iteration 175, loss = 0.00783595\n",
      "Iteration 176, loss = 0.00783484\n",
      "Iteration 177, loss = 0.00783516\n",
      "Iteration 178, loss = 0.00782619\n",
      "Iteration 179, loss = 0.00781827\n",
      "Iteration 180, loss = 0.00782423\n",
      "Iteration 181, loss = 0.00781755\n",
      "Iteration 182, loss = 0.00782283\n",
      "Iteration 183, loss = 0.00781452\n",
      "Iteration 184, loss = 0.00780639\n",
      "Iteration 185, loss = 0.00780096\n",
      "Iteration 186, loss = 0.00779987\n",
      "Iteration 187, loss = 0.00780381\n",
      "Iteration 188, loss = 0.00779932\n",
      "Iteration 189, loss = 0.00779097\n",
      "Iteration 190, loss = 0.00779658\n",
      "Iteration 191, loss = 0.00779346\n",
      "Iteration 192, loss = 0.00779163\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed:  6.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.15674908\n",
      "Iteration 2, loss = 0.06054553\n",
      "Iteration 3, loss = 0.02358588\n",
      "Iteration 4, loss = 0.01229243\n",
      "Iteration 5, loss = 0.00576500\n",
      "Iteration 6, loss = 0.00276392\n",
      "Iteration 7, loss = 0.00148506\n",
      "Iteration 8, loss = 0.00090294\n",
      "Iteration 9, loss = 0.00059829\n",
      "Iteration 10, loss = 0.00041130\n",
      "Iteration 11, loss = 0.00029784\n",
      "Iteration 12, loss = 0.00023030\n",
      "Iteration 13, loss = 0.00018196\n",
      "Iteration 14, loss = 0.00015149\n",
      "Iteration 15, loss = 0.00012634\n",
      "Iteration 16, loss = 0.00010899\n",
      "Iteration 17, loss = 0.00009467\n",
      "Iteration 18, loss = 0.00008552\n",
      "Iteration 19, loss = 0.00007840\n",
      "Iteration 20, loss = 0.00006953\n",
      "Iteration 21, loss = 0.00006470\n",
      "Iteration 22, loss = 0.00006114\n",
      "Iteration 23, loss = 0.00005895\n",
      "Iteration 24, loss = 0.00005652\n",
      "Iteration 25, loss = 0.00005369\n",
      "Iteration 26, loss = 0.00005164\n",
      "Iteration 27, loss = 0.00005086\n",
      "Iteration 28, loss = 0.00004974\n",
      "Iteration 29, loss = 0.00004868\n",
      "Iteration 30, loss = 0.00004716\n",
      "Iteration 31, loss = 0.00004600\n",
      "Iteration 32, loss = 0.00004525\n",
      "Iteration 33, loss = 0.00004402\n",
      "Iteration 34, loss = 0.00004215\n",
      "Iteration 35, loss = 0.00004270\n",
      "Iteration 36, loss = 0.00004175\n",
      "Iteration 37, loss = 0.00004073\n",
      "Iteration 38, loss = 0.00004160\n",
      "Iteration 39, loss = 0.00003984\n",
      "Iteration 40, loss = 0.00003910\n",
      "Iteration 41, loss = 0.00003966\n",
      "Iteration 42, loss = 0.00004219\n",
      "Iteration 43, loss = 0.00004041\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Fit done. Score: 0.9999560780388885\n",
      "RMS error: x 0.004769189080727496, y 0.0046002278505866846\n"
     ]
    }
   ],
   "source": [
    "# Generate samples\n",
    "samples = make_circle_data(20000)\n",
    "samples.head()\n",
    "# Train the network\n",
    "fitter = tune_network(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: x vs. y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = fitter.predict(samples.u.values.reshape(-1,1))\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "plt.scatter(samples.x,samples.y, s = 0.5)\n",
    "plt.scatter(y_pred[:,0], y_pred[:,1], s = 0.5)\n",
    "plt.savefig('../pictures/circle_generator_2d.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: x, y vs. u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize = (10,5))\n",
    "plt.scatter(samples.u, samples.x, s = 0.2)\n",
    "plt.scatter(samples.u, samples.y, s = 0.2)\n",
    "plt.scatter(samples.u, y_pred[:,0], s = 0.2)\n",
    "plt.scatter(samples.u, y_pred[:,1], s = 0.2)\n",
    "plt.savefig('../pictures/circle_generator_1d.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zuza/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/zuza/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/zuza/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/zuza/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/home/zuza/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "grid_results = pd.DataFrame(fitter.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grid_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5a6f84359de0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrid_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'grid_results' is not defined"
     ]
    }
   ],
   "source": [
    "grid_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(grid_results.rank_test_score, grid_results.mean_test_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
